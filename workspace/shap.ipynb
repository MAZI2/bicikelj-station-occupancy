{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1184b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TCN Bicikelj final training + test prediction with weather ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from tqdm import tqdm\n",
    "import holidays\n",
    "import random\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "HISTORY_LEN = 48\n",
    "PRED_HORIZON = 4\n",
    "K_NEIGHBORS = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EMBED_DIM = 8\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 4\n",
    "LR = 0.0005\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 50\n",
    "PATIENCE = 8\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(\"../data/bicikelj_train.csv\")\n",
    "meta = pd.read_csv(\"../data/bicikelj_metadata.csv\")\n",
    "station_cols = df.columns[1:]\n",
    "\n",
    "# Clean and fill\n",
    "for col in station_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "df[station_cols] = df[station_cols].ffill().bfill()\n",
    "df = df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
    "\n",
    "# --- Load weather ---\n",
    "weather_df = pd.read_csv(\"../data/weather_ljubljana.csv\", skiprows=2)\n",
    "weather_df = weather_df.rename(columns={\n",
    "    'temperature_2m (°C)': 'temperature_2m',\n",
    "    'precipitation (mm)': 'precipitation',\n",
    "    'windspeed_10m (km/h)': 'windspeed_10m',\n",
    "    'cloudcover (%)': 'cloudcover'\n",
    "})\n",
    "weather_df['time'] = pd.to_datetime(weather_df['time'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(None)\n",
    "df_merged = pd.merge(df, weather_df, left_on='timestamp', right_on='time', how='left')\n",
    "\n",
    "weather_features = ['temperature_2m', 'precipitation', 'windspeed_10m', 'cloudcover']\n",
    "df_merged[weather_features] = df_merged[weather_features].ffill().bfill()\n",
    "\n",
    "# --- Normalize ---\n",
    "station_means = df_merged[station_cols].mean()\n",
    "station_stds = df_merged[station_cols].std().replace(0, 1)\n",
    "df_norm = df_merged.copy()\n",
    "df_norm[station_cols] = (df_merged[station_cols] - station_means) / station_stds\n",
    "\n",
    "weather_means = df_merged[weather_features].mean()\n",
    "weather_stds = df_merged[weather_features].std().replace(0, 1)\n",
    "df_norm[weather_features] = (df_merged[weather_features] - weather_means) / weather_stds\n",
    "\n",
    "# --- Neighbors ---\n",
    "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
    "station_names = meta['name'].tolist()\n",
    "dists = haversine_distances(coords, coords) * 6371\n",
    "neighbors = {}\n",
    "for i, name in enumerate(station_names):\n",
    "    order = np.argsort(dists[i])\n",
    "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
    "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
    "\n",
    "# --- Dataset ---\n",
    "class SharedTCNDataset(Dataset):\n",
    "    def __init__(self, df, station_cols, neighbors, history_len, pred_horizon, weather_features):\n",
    "        self.samples = []\n",
    "        self.station_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
    "        timestamps = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "        hour_sin = np.sin(2 * np.pi * timestamps.dt.hour / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * timestamps.dt.hour / 24)\n",
    "        dow_sin = np.sin(2 * np.pi * timestamps.dt.dayofweek / 7)\n",
    "        dow_cos = np.cos(2 * np.pi * timestamps.dt.dayofweek / 7)\n",
    "        month_sin = np.sin(2 * np.pi * timestamps.dt.month / 12)\n",
    "        month_cos = np.sin(2 * np.pi * timestamps.dt.month / 12)\n",
    "        is_weekend = (timestamps.dt.dayofweek >= 5).astype(float)\n",
    "        slo_holidays = holidays.Slovenia()\n",
    "        is_holiday = timestamps.dt.date.astype(str).isin([str(d) for d in slo_holidays]).astype(float)\n",
    "\n",
    "        weather_array = df[weather_features].values\n",
    "\n",
    "        time_feats = np.concatenate([\n",
    "            np.stack([hour_sin, hour_cos, dow_sin, dow_cos,\n",
    "                      month_sin, month_cos, is_weekend, is_holiday], axis=1),\n",
    "            weather_array\n",
    "        ], axis=1)\n",
    "\n",
    "        bikes = df[station_cols].values.astype(np.float32)\n",
    "        N = len(df)\n",
    "\n",
    "        for s_name in station_cols:\n",
    "            s_idx = self.station_to_idx[s_name]\n",
    "            nn_idx = [self.station_to_idx[nn] for nn in neighbors[s_name]]\n",
    "            series = bikes[:, [s_idx] + nn_idx]\n",
    "            full_feats = np.concatenate([series, time_feats], axis=1)\n",
    "\n",
    "            for i in range(history_len, N - pred_horizon + 1):\n",
    "                x = full_feats[i - history_len:i]\n",
    "                y = bikes[i:i + pred_horizon, s_idx]\n",
    "                self.samples.append((x, y, s_idx))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, sid = self.samples[idx]\n",
    "        return (torch.tensor(x, dtype=torch.float32),\n",
    "                torch.tensor(y, dtype=torch.float32),\n",
    "                torch.tensor(sid, dtype=torch.long))\n",
    "\n",
    "# --- TCN Block ---\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               padding=self.padding, dilation=dilation)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               padding=self.padding, dilation=dilation)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = out[:, :, :-self.padding]\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = out[:, :, :-self.padding]\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return out + res\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout, num_stations, embed_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_ch = input_size if i == 0 else num_channels[i - 1]\n",
    "            out_ch = num_channels[i]\n",
    "            layers += [TemporalBlock(in_ch, out_ch, kernel_size, dilation_size, dropout)]\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.embedding = nn.Embedding(num_stations, embed_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_channels[-1] + embed_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, station_id):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        tcn_out = self.tcn(x)[:, :, -1]\n",
    "        emb = self.embedding(station_id)\n",
    "        combined = torch.cat([tcn_out, emb], dim=1)\n",
    "        return self.head(combined)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2272e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset and weather loaded, normalized, and ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import holidays\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "# --- Hyperparameters (should match those from your model!) ---\n",
    "HISTORY_LEN = 48\n",
    "PRED_HORIZON = 4\n",
    "K_NEIGHBORS = 2\n",
    "\n",
    "# --- Load metadata ---\n",
    "meta = pd.read_csv(\"../data/bicikelj_metadata.csv\")\n",
    "station_cols = pd.read_csv(\"../data/bicikelj_train.csv\").columns[1:]\n",
    "station_names = meta['name'].tolist()\n",
    "\n",
    "# --- Load weather (train, test) ---\n",
    "weather_df = pd.read_csv(\"../data/weather_ljubljana.csv\", skiprows=2)\n",
    "weather_df = weather_df.rename(columns={\n",
    "    'temperature_2m (°C)': 'temperature_2m',\n",
    "    'precipitation (mm)': 'precipitation',\n",
    "    'windspeed_10m (km/h)': 'windspeed_10m',\n",
    "    'cloudcover (%)': 'cloudcover'\n",
    "})\n",
    "weather_df['time'] = pd.to_datetime(weather_df['time'])\n",
    "weather_features = ['temperature_2m', 'precipitation', 'windspeed_10m', 'cloudcover']\n",
    "\n",
    "weather_test_df = pd.read_csv(\"../data/weather_ljubljana_test.csv\", skiprows=2)\n",
    "weather_test_df = weather_test_df.rename(columns={\n",
    "    'temperature_2m (°C)': 'temperature_2m',\n",
    "    'precipitation (mm)': 'precipitation',\n",
    "    'windspeed_10m (km/h)': 'windspeed_10m',\n",
    "    'cloudcover (%)': 'cloudcover'\n",
    "})\n",
    "weather_test_df['time'] = pd.to_datetime(weather_test_df['time'])\n",
    "\n",
    "# --- Load train for mean/std (needed for normalization) ---\n",
    "df_train = pd.read_csv(\"../data/bicikelj_train.csv\")\n",
    "for col in station_cols:\n",
    "    df_train[col] = pd.to_numeric(df_train[col], errors=\"coerce\")\n",
    "df_train[station_cols] = df_train[station_cols].ffill().bfill()\n",
    "df_train = df_train.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
    "df_train['timestamp'] = pd.to_datetime(df_train['timestamp']).dt.tz_localize(None)\n",
    "\n",
    "df_train_merged = pd.merge(df_train, weather_df, left_on='timestamp', right_on='time', how='left')\n",
    "df_train_merged[weather_features] = df_train_merged[weather_features].ffill().bfill()\n",
    "\n",
    "station_means = df_train_merged[station_cols].mean()\n",
    "station_stds = df_train_merged[station_cols].std().replace(0, 1)\n",
    "weather_means = df_train_merged[weather_features].mean()\n",
    "weather_stds = df_train_merged[weather_features].std().replace(0, 1)\n",
    "\n",
    "# --- Load and prep test set ---\n",
    "df_test = pd.read_csv(\"../data/bicikelj_test.csv\")\n",
    "for col in station_cols:\n",
    "    df_test[col] = pd.to_numeric(df_test[col], errors=\"coerce\")\n",
    "df_test[station_cols] = df_test[station_cols].ffill().bfill()\n",
    "df_test = df_test.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
    "df_test['timestamp'] = pd.to_datetime(df_test['timestamp']).dt.tz_localize(None)\n",
    "\n",
    "df_test_merged = pd.merge(df_test, weather_test_df, left_on='timestamp', right_on='time', how='left')\n",
    "df_test_merged[weather_features] = df_test_merged[weather_features].ffill().bfill()\n",
    "\n",
    "# --- Normalize test set using TRAIN stats ---\n",
    "df_test_norm = df_test_merged.copy()\n",
    "df_test_norm[station_cols] = (df_test_merged[station_cols] - station_means) / station_stds\n",
    "df_test_norm[weather_features] = (df_test_merged[weather_features] - weather_means) / weather_stds\n",
    "\n",
    "# --- Prepare neighbors dict (for input features) ---\n",
    "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
    "dists = haversine_distances(coords, coords) * 6371\n",
    "neighbors = {}\n",
    "for i, name in enumerate(station_names):\n",
    "    order = np.argsort(dists[i])\n",
    "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
    "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
    "\n",
    "print(\"Test dataset and weather loaded, normalized, and ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36539b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCN(\n",
       "  (tcn): Sequential(\n",
       "    (0): TemporalBlock(\n",
       "      (conv1): Conv1d(15, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (downsample): Conv1d(15, 64, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): TemporalBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (2): TemporalBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (3): TemporalBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Embedding(84, 8)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=72, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TCN(\n",
    "    input_size=(1 + K_NEIGHBORS + 8 + len(weather_features)),  # features per timestep\n",
    "    output_size=PRED_HORIZON,\n",
    "    num_channels=[HIDDEN_DIM] * N_LAYERS,\n",
    "    kernel_size=3,\n",
    "    dropout=DROPOUT,\n",
    "    num_stations=len(station_cols),\n",
    "    embed_dim=EMBED_DIM\n",
    ")\n",
    "model.load_state_dict(torch.load(\"tcn_model_final_weather.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of explanation samples per station:\n",
      "  Station  0: 15 explanation samples from 1509 total samples\n",
      "  Station  1: 15 explanation samples from 1509 total samples\n",
      "  Station  2: 15 explanation samples from 1509 total samples\n",
      "  Station  3: 15 explanation samples from 1509 total samples\n",
      "  Station  4: 15 explanation samples from 1509 total samples\n",
      "  Station  5: 15 explanation samples from 1509 total samples\n",
      "  Station  6: 15 explanation samples from 1509 total samples\n",
      "  Station  7: 15 explanation samples from 1509 total samples\n",
      "  Station  8: 15 explanation samples from 1509 total samples\n",
      "  Station  9: 15 explanation samples from 1509 total samples\n",
      "  Station 10: 15 explanation samples from 1509 total samples\n",
      "  Station 11: 15 explanation samples from 1509 total samples\n",
      "  Station 12: 15 explanation samples from 1509 total samples\n",
      "  Station 13: 15 explanation samples from 1509 total samples\n",
      "  Station 14: 15 explanation samples from 1509 total samples\n",
      "  Station 15: 15 explanation samples from 1509 total samples\n",
      "  Station 16: 15 explanation samples from 1509 total samples\n",
      "  Station 17: 15 explanation samples from 1509 total samples\n",
      "  Station 18: 15 explanation samples from 1509 total samples\n",
      "  Station 19: 15 explanation samples from 1509 total samples\n",
      "  Station 20: 15 explanation samples from 1509 total samples\n",
      "  Station 21: 15 explanation samples from 1509 total samples\n",
      "  Station 22: 15 explanation samples from 1509 total samples\n",
      "  Station 23: 15 explanation samples from 1509 total samples\n",
      "  Station 24: 15 explanation samples from 1509 total samples\n",
      "  Station 25: 15 explanation samples from 1509 total samples\n",
      "  Station 26: 15 explanation samples from 1509 total samples\n",
      "  Station 27: 15 explanation samples from 1509 total samples\n",
      "  Station 28: 15 explanation samples from 1509 total samples\n",
      "  Station 29: 15 explanation samples from 1509 total samples\n",
      "  Station 30: 15 explanation samples from 1509 total samples\n",
      "  Station 31: 15 explanation samples from 1509 total samples\n",
      "  Station 32: 15 explanation samples from 1509 total samples\n",
      "  Station 33: 15 explanation samples from 1509 total samples\n",
      "  Station 34: 15 explanation samples from 1509 total samples\n",
      "  Station 35: 15 explanation samples from 1509 total samples\n",
      "  Station 36: 15 explanation samples from 1509 total samples\n",
      "  Station 37: 15 explanation samples from 1509 total samples\n",
      "  Station 38: 15 explanation samples from 1509 total samples\n",
      "  Station 39: 15 explanation samples from 1509 total samples\n",
      "  Station 40: 15 explanation samples from 1509 total samples\n",
      "  Station 41: 15 explanation samples from 1509 total samples\n",
      "  Station 42: 15 explanation samples from 1509 total samples\n",
      "  Station 43: 15 explanation samples from 1509 total samples\n",
      "  Station 44: 15 explanation samples from 1509 total samples\n",
      "  Station 45: 15 explanation samples from 1509 total samples\n",
      "  Station 46: 15 explanation samples from 1509 total samples\n",
      "  Station 47: 15 explanation samples from 1509 total samples\n",
      "  Station 48: 15 explanation samples from 1509 total samples\n",
      "  Station 49: 15 explanation samples from 1509 total samples\n",
      "  Station 50: 15 explanation samples from 1509 total samples\n",
      "  Station 51: 15 explanation samples from 1509 total samples\n",
      "  Station 52: 15 explanation samples from 1509 total samples\n",
      "  Station 53: 15 explanation samples from 1509 total samples\n",
      "  Station 54: 15 explanation samples from 1509 total samples\n",
      "  Station 55: 15 explanation samples from 1509 total samples\n",
      "  Station 56: 15 explanation samples from 1509 total samples\n",
      "  Station 57: 15 explanation samples from 1509 total samples\n",
      "  Station 58: 15 explanation samples from 1509 total samples\n",
      "  Station 59: 15 explanation samples from 1509 total samples\n",
      "  Station 60: 15 explanation samples from 1509 total samples\n",
      "  Station 61: 15 explanation samples from 1509 total samples\n",
      "  Station 62: 15 explanation samples from 1509 total samples\n",
      "  Station 63: 15 explanation samples from 1509 total samples\n",
      "  Station 64: 15 explanation samples from 1509 total samples\n",
      "  Station 65: 15 explanation samples from 1509 total samples\n",
      "  Station 66: 15 explanation samples from 1509 total samples\n",
      "  Station 67: 15 explanation samples from 1509 total samples\n",
      "  Station 68: 15 explanation samples from 1509 total samples\n",
      "  Station 69: 15 explanation samples from 1509 total samples\n",
      "  Station 70: 15 explanation samples from 1509 total samples\n",
      "  Station 71: 15 explanation samples from 1509 total samples\n",
      "  Station 72: 15 explanation samples from 1509 total samples\n",
      "  Station 73: 15 explanation samples from 1509 total samples\n",
      "  Station 74: 15 explanation samples from 1509 total samples\n",
      "  Station 75: 15 explanation samples from 1509 total samples\n",
      "  Station 76: 15 explanation samples from 1509 total samples\n",
      "  Station 77: 15 explanation samples from 1509 total samples\n",
      "  Station 78: 15 explanation samples from 1509 total samples\n",
      "  Station 79: 15 explanation samples from 1509 total samples\n",
      "  Station 80: 15 explanation samples from 1509 total samples\n",
      "  Station 81: 15 explanation samples from 1509 total samples\n",
      "  Station 82: 15 explanation samples from 1509 total samples\n",
      "  Station 83: 15 explanation samples from 1509 total samples\n",
      "Total explanation samples: 1260\n",
      "Total background samples:   252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1260 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(model_predict_flat, background_kmeans)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#explainer = shap.KernelExplainer(model_predict_flat, full_background)\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_explain\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# list: (N_EXPLAIN, N_FEATURES) for each output horizon\u001b[39;00m\n\u001b[1;32m    124\u001b[0m shap_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(shap_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)        \u001b[38;5;66;03m# (N_EXPLAIN, N_FEATURES, N_HORIZONS)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshap_array shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshap_array\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/shap/explainers/_kernel.py:275\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[1;32m    274\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[0;32m--> 275\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/shap/explainers/_kernel.py:479\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[1;32m    482\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/shap/explainers/_kernel.py:624\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered:\n\u001b[1;32m    623\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m--> 624\u001b[0m modelOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries)):\n\u001b[1;32m    626\u001b[0m     modelOut \u001b[38;5;241m=\u001b[39m modelOut\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[0;32mIn[6], line 116\u001b[0m, in \u001b[0;36mmodel_predict_flat\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    114\u001b[0m sid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sids, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 167\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x, station_id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, station_id):\n\u001b[1;32m    166\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 167\u001b[0m     tcn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    168\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(station_id)\n\u001b[1;32m    169\u001b[0m     combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([tcn_out, emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 134\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 134\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding]\n\u001b[1;32m    136\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/IS/lib/python3.12/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- SETTINGS ---------\n",
    "N_BACKGROUND_PER_STATION = 3  # Number of background samples per station for SHAP\n",
    "PERCENT_PER_STATION = 0.001     # Percentage of test samples per station to use for SHAP explanation (e.g. 0.2 means 20%)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --------- PREPARE X_samples, SID_samples FROM TEST SET ---------\n",
    "# (Assumes you loaded and normalized df_test_norm, have station_cols, weather_features, neighbors, etc.)\n",
    "\n",
    "timestamps = pd.to_datetime(df_test_norm['timestamp'])\n",
    "hour_sin = np.sin(2 * np.pi * timestamps.dt.hour / 24)\n",
    "hour_cos = np.cos(2 * np.pi * timestamps.dt.hour / 24)\n",
    "dow_sin = np.sin(2 * np.pi * timestamps.dt.dayofweek / 7)\n",
    "dow_cos = np.cos(2 * np.pi * timestamps.dt.dayofweek / 7)\n",
    "month_sin = np.sin(2 * np.pi * timestamps.dt.month / 12)\n",
    "month_cos = np.cos(2 * np.pi * timestamps.dt.month / 12)\n",
    "is_weekend = (timestamps.dt.dayofweek >= 5).astype(float)\n",
    "slo_holidays = holidays.Slovenia()\n",
    "is_holiday = timestamps.dt.date.astype(str).isin([str(d) for d in slo_holidays]).astype(float)\n",
    "\n",
    "time_feats_arr = np.stack([hour_sin, hour_cos, dow_sin, dow_cos,\n",
    "                           month_sin, month_cos, is_weekend, is_holiday], axis=1)\n",
    "bikes = df_test_norm[station_cols].values.astype(np.float32)\n",
    "weather_arr = df_test_norm[weather_features].values.astype(np.float32)\n",
    "\n",
    "X_samples = []\n",
    "SID_samples = []\n",
    "\n",
    "N = len(df_test_norm)\n",
    "station_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
    "for s_name in station_cols:\n",
    "    s_idx = station_to_idx[s_name]\n",
    "    nn_idx = [station_to_idx[nn] for nn in neighbors[s_name]]\n",
    "    series = bikes[:, [s_idx] + nn_idx]\n",
    "    full_feats = np.concatenate([series, time_feats_arr, weather_arr], axis=1)\n",
    "    for i in range(HISTORY_LEN, N - PRED_HORIZON + 1):\n",
    "        x_seq = full_feats[i - HISTORY_LEN:i]\n",
    "        X_samples.append(x_seq)\n",
    "        SID_samples.append(s_idx)\n",
    "\n",
    "X_samples = torch.tensor(np.stack(X_samples), dtype=torch.float32)\n",
    "SID_samples = torch.tensor(SID_samples, dtype=torch.long)\n",
    "\n",
    "# --------- FEATURE NAMES ---------\n",
    "n_neighbors = K_NEIGHBORS\n",
    "feature_names_per_step = (\n",
    "    ['station_value'] +\n",
    "    [f'neighbor_{i+1}' for i in range(n_neighbors)] +\n",
    "    ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos', 'is_weekend', 'is_holiday'] +\n",
    "    weather_features\n",
    ")\n",
    "NUM_STEP_FEATURES = len(feature_names_per_step)\n",
    "\n",
    "flat_feature_names = []\n",
    "for t in range(HISTORY_LEN):\n",
    "    for fname in feature_names_per_step:\n",
    "        flat_feature_names.append(f\"{fname}_t-{HISTORY_LEN-t}\")\n",
    "flat_feature_names.append(\"station_id\")\n",
    "\n",
    "def flatten_sid(xs, sids):\n",
    "    flat_xs = xs.reshape(xs.shape[0], -1)\n",
    "    flat = np.concatenate([flat_xs, sids.reshape(-1, 1)], axis=1)\n",
    "    return flat\n",
    "\n",
    "# --------- STATION-BALANCED SAMPLING ---------\n",
    "station_indices = {sid: [] for sid in range(len(station_cols))}\n",
    "for i, sid in enumerate(SID_samples):\n",
    "    station_indices[int(sid.item() if torch.is_tensor(sid) else sid)].append(i)\n",
    "\n",
    "explain_idxs = []\n",
    "background_idxs = []\n",
    "for sid, idxs in station_indices.items():\n",
    "    n = len(idxs)\n",
    "    if n == 0:\n",
    "        continue\n",
    "    # Explanation: randomly sample PERCENT_PER_STATION for SHAP explanations\n",
    "    n_explain = max(1, int(n * PERCENT_PER_STATION))\n",
    "    chosen_explain = np.random.choice(idxs, size=n_explain, replace=False)\n",
    "    explain_idxs.extend(chosen_explain)\n",
    "    # Background: randomly sample N_BACKGROUND_PER_STATION for SHAP background\n",
    "    chosen_background = np.random.choice(idxs, size=min(N_BACKGROUND_PER_STATION, n), replace=False)\n",
    "    background_idxs.extend(chosen_background)\n",
    "\n",
    "print(\"\\nNumber of explanation samples per station:\")\n",
    "for sid, idxs in station_indices.items():\n",
    "    n = len(idxs)\n",
    "    n_explain = max(1, int(n * PERCENT_PER_STATION)) if n > 0 else 0\n",
    "    print(f\"  Station {sid:>2}: {n_explain} explanation samples from {n} total samples\")\n",
    "print(f\"Total explanation samples: {len(explain_idxs)}\")\n",
    "print(f\"Total background samples:   {len(background_idxs)}\")\n",
    "\n",
    "\n",
    "# --------- PREPARE EXPLAIN/BACKGROUND MATRICES ---------\n",
    "explain_x = X_samples[explain_idxs].cpu().numpy()\n",
    "explain_sid = SID_samples[explain_idxs].cpu().numpy()\n",
    "full_explain = flatten_sid(explain_x, explain_sid)\n",
    "\n",
    "background_x = X_samples[background_idxs].cpu().numpy()\n",
    "background_sid = SID_samples[background_idxs].cpu().numpy()\n",
    "full_background = flatten_sid(background_x, background_sid)\n",
    "\n",
    "# --------- MODEL WRAPPER FOR SHAP ---------\n",
    "def model_predict_flat(x):\n",
    "    n_feat = NUM_STEP_FEATURES\n",
    "    seq_len = HISTORY_LEN\n",
    "    features = x[:, :seq_len * n_feat].reshape(-1, seq_len, n_feat)\n",
    "    sids = x[:, -1].astype(np.int64)\n",
    "    t = torch.tensor(features, dtype=torch.float32).to(DEVICE)\n",
    "    sid = torch.tensor(sids, dtype=torch.long).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        return model(t, sid).cpu().numpy()  # (batch, PRED_HORIZON)\n",
    "\n",
    "# --------- SHAP EXPLAINER ---------\n",
    "background_kmeans = shap.kmeans(full_background, 3)\n",
    "explainer = shap.KernelExplainer(model_predict_flat, background_kmeans)\n",
    "\n",
    "#explainer = shap.KernelExplainer(model_predict_flat, full_background)\n",
    "shap_values = explainer.shap_values(full_explain)  # list: (N_EXPLAIN, N_FEATURES) for each output horizon\n",
    "shap_array = np.stack(shap_values, axis=-1)        # (N_EXPLAIN, N_FEATURES, N_HORIZONS)\n",
    "\n",
    "print(f\"shap_array shape: {shap_array.shape}\")\n",
    "\n",
    "# --------- PLOT TOP 10 FEATURES (mean SHAP for horizon 0) ---------\n",
    "mean_abs_shap = np.mean(np.abs(shap_array[:, :, 0]), axis=0)\n",
    "top10_idx = np.argsort(mean_abs_shap)[-10:][::-1]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(10), mean_abs_shap[top10_idx])\n",
    "plt.xticks(range(10), [flat_feature_names[i] for i in top10_idx], rotation=45, ha='right')\n",
    "plt.title(\"Top 10 features by mean |SHAP| (horizon 0, station-balanced, test set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
