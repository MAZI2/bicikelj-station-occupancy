{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo9f8qlb5fzh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "\n",
        "# Load metadata\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)  # radians for haversine\n",
        "\n",
        "# Compute pairwise distances (in km)\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "\n",
        "# Use exponential decay for adjacency weights\n",
        "sigma = 1.0  # adjust this to control the \"spread\" (try 0.5-2.0)\n",
        "adj = np.exp(-dists / sigma)\n",
        "\n",
        "# Optionally zero-out self-edges (or set to 1)\n",
        "np.fill_diagonal(adj, 1.0)\n",
        "\n",
        "# Optionally threshold to k-nearest\n",
        "k = 4\n",
        "for i in range(adj.shape[0]):\n",
        "    adj[i, np.argsort(adj[i])[:-k-1]] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbgDhAmPMDv2"
      },
      "outputs": [],
      "source": [
        "adj = adj / (adj.sum(axis=1, keepdims=True) + 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJyK698UUhHu"
      },
      "outputs": [],
      "source": [
        "# For a hard threshold:\n",
        "adj = (dists < 1.0).astype(float)\n",
        "np.fill_diagonal(adj, 1.0)\n",
        "adj = adj / (adj.sum(axis=1, keepdims=True) + 1e-8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STGCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtEBniAC5g6q",
        "outputId": "9ec6280b-9e17-4405-8839-e2ae3ca3d415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Any NaN in X_train? False\n",
            "Any NaN in y_train? False\n",
            "X_train min/max 0.0 1.0\n",
            "y_train min/max 0.0 0.9999999996153845\n",
            "Epoch 1/100 Train Loss: 0.0323 Val Loss: 0.0202\n",
            "Epoch 2/100 Train Loss: 0.0178 Val Loss: 0.0179\n",
            "Epoch 3/100 Train Loss: 0.0168 Val Loss: 0.0174\n",
            "Epoch 4/100 Train Loss: 0.0165 Val Loss: 0.0172\n",
            "Epoch 5/100 Train Loss: 0.0164 Val Loss: 0.0171\n",
            "Epoch 6/100 Train Loss: 0.0163 Val Loss: 0.0171\n",
            "Epoch 7/100 Train Loss: 0.0162 Val Loss: 0.0170\n",
            "Epoch 8/100 Train Loss: 0.0162 Val Loss: 0.0169\n",
            "Epoch 9/100 Train Loss: 0.0161 Val Loss: 0.0168\n",
            "Epoch 10/100 Train Loss: 0.0161 Val Loss: 0.0168\n",
            "Epoch 11/100 Train Loss: 0.0160 Val Loss: 0.0170\n",
            "Epoch 12/100 Train Loss: 0.0160 Val Loss: 0.0167\n",
            "Epoch 13/100 Train Loss: 0.0160 Val Loss: 0.0169\n",
            "Epoch 14/100 Train Loss: 0.0159 Val Loss: 0.0170\n",
            "Epoch 15/100 Train Loss: 0.0159 Val Loss: 0.0166\n",
            "Epoch 16/100 Train Loss: 0.0159 Val Loss: 0.0166\n",
            "Epoch 17/100 Train Loss: 0.0159 Val Loss: 0.0167\n",
            "Epoch 18/100 Train Loss: 0.0159 Val Loss: 0.0166\n",
            "Epoch 19/100 Train Loss: 0.0158 Val Loss: 0.0166\n",
            "Epoch 20/100 Train Loss: 0.0158 Val Loss: 0.0165\n",
            "Epoch 21/100 Train Loss: 0.0158 Val Loss: 0.0166\n",
            "Epoch 22/100 Train Loss: 0.0158 Val Loss: 0.0165\n",
            "Epoch 23/100 Train Loss: 0.0158 Val Loss: 0.0166\n",
            "Epoch 24/100 Train Loss: 0.0158 Val Loss: 0.0165\n",
            "Epoch 25/100 Train Loss: 0.0158 Val Loss: 0.0165\n",
            "Epoch 26/100 Train Loss: 0.0158 Val Loss: 0.0165\n",
            "Epoch 27/100 Train Loss: 0.0157 Val Loss: 0.0165\n",
            "Epoch 28/100 Train Loss: 0.0157 Val Loss: 0.0165\n",
            "Epoch 29/100 Train Loss: 0.0157 Val Loss: 0.0169\n",
            "Epoch 30/100 Train Loss: 0.0157 Val Loss: 0.0164\n",
            "Epoch 31/100 Train Loss: 0.0157 Val Loss: 0.0164\n",
            "Epoch 32/100 Train Loss: 0.0157 Val Loss: 0.0165\n",
            "Epoch 33/100 Train Loss: 0.0157 Val Loss: 0.0166\n",
            "Epoch 34/100 Train Loss: 0.0157 Val Loss: 0.0165\n",
            "Epoch 35/100 Train Loss: 0.0157 Val Loss: 0.0166\n",
            "Epoch 36/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 37/100 Train Loss: 0.0157 Val Loss: 0.0166\n",
            "Epoch 38/100 Train Loss: 0.0156 Val Loss: 0.0163\n",
            "Epoch 39/100 Train Loss: 0.0157 Val Loss: 0.0164\n",
            "Epoch 40/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 41/100 Train Loss: 0.0156 Val Loss: 0.0163\n",
            "Epoch 42/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 43/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 44/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 45/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 46/100 Train Loss: 0.0156 Val Loss: 0.0163\n",
            "Epoch 47/100 Train Loss: 0.0156 Val Loss: 0.0163\n",
            "Epoch 48/100 Train Loss: 0.0156 Val Loss: 0.0165\n",
            "Epoch 49/100 Train Loss: 0.0156 Val Loss: 0.0165\n",
            "Epoch 50/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 51/100 Train Loss: 0.0156 Val Loss: 0.0163\n",
            "Epoch 52/100 Train Loss: 0.0156 Val Loss: 0.0165\n",
            "Epoch 53/100 Train Loss: 0.0156 Val Loss: 0.0164\n",
            "Epoch 54/100 Train Loss: 0.0155 Val Loss: 0.0164\n",
            "Epoch 55/100 Train Loss: 0.0156 Val Loss: 0.0163\n",
            "Epoch 56/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 57/100 Train Loss: 0.0156 Val Loss: 0.0162\n",
            "Epoch 58/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 59/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 60/100 Train Loss: 0.0155 Val Loss: 0.0165\n",
            "Epoch 61/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 62/100 Train Loss: 0.0155 Val Loss: 0.0162\n",
            "Epoch 63/100 Train Loss: 0.0155 Val Loss: 0.0162\n",
            "Epoch 64/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 65/100 Train Loss: 0.0155 Val Loss: 0.0165\n",
            "Epoch 66/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 67/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 68/100 Train Loss: 0.0155 Val Loss: 0.0162\n",
            "Epoch 69/100 Train Loss: 0.0155 Val Loss: 0.0163\n",
            "Epoch 70/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 71/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 72/100 Train Loss: 0.0154 Val Loss: 0.0163\n",
            "Epoch 73/100 Train Loss: 0.0154 Val Loss: 0.0163\n",
            "Epoch 74/100 Train Loss: 0.0154 Val Loss: 0.0164\n",
            "Epoch 75/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 76/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 77/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 78/100 Train Loss: 0.0154 Val Loss: 0.0163\n",
            "Epoch 79/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 80/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 81/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 82/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 83/100 Train Loss: 0.0154 Val Loss: 0.0165\n",
            "Epoch 84/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 85/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 86/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 87/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 88/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 89/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 90/100 Train Loss: 0.0154 Val Loss: 0.0162\n",
            "Epoch 91/100 Train Loss: 0.0154 Val Loss: 0.0163\n",
            "Epoch 92/100 Train Loss: 0.0154 Val Loss: 0.0163\n",
            "Epoch 93/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 94/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 95/100 Train Loss: 0.0153 Val Loss: 0.0163\n",
            "Epoch 96/100 Train Loss: 0.0153 Val Loss: 0.0161\n",
            "Epoch 97/100 Train Loss: 0.0153 Val Loss: 0.0161\n",
            "Epoch 98/100 Train Loss: 0.0153 Val Loss: 0.0161\n",
            "Epoch 99/100 Train Loss: 0.0154 Val Loss: 0.0161\n",
            "Epoch 100/100 Train Loss: 0.0153 Val Loss: 0.0161\n",
            "Validation MSE: 0.0162\n",
            "Saved napovedi.csv with legend and only rows containing predictions.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ------------------------- MODEL DEFINITIONS -------------------------\n",
        "\n",
        "class GraphConv(nn.Module):\n",
        "    def __init__(self, in_features, out_features, adj):\n",
        "        super().__init__()\n",
        "        self.register_buffer('adj', torch.FloatTensor(adj))\n",
        "        self.lin = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        adj = self.adj.to(x.device)\n",
        "        x = torch.einsum('ij,bjf->bif', adj, x)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "class STGCNBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, adj):\n",
        "        super().__init__()\n",
        "        self.temp_conv = nn.Conv2d(\n",
        "            in_channels=in_features, out_channels=out_features,\n",
        "            kernel_size=(3, 1), padding=(1, 0)\n",
        "        )\n",
        "        self.gconv = GraphConv(out_features, out_features, adj)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)  # [batch, in_features, seq_len, nodes]\n",
        "        x = self.temp_conv(x)       # [batch, out_features, seq_len, nodes]\n",
        "        x = self.relu(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # [batch, seq_len, nodes, out_features]\n",
        "\n",
        "        out = []\n",
        "        for t in range(x.shape[1]):\n",
        "            xt = x[:, t, :, :]\n",
        "            xt = self.gconv(xt)\n",
        "            out.append(xt)\n",
        "        x = torch.stack(out, dim=1)\n",
        "        return self.relu(x)\n",
        "\n",
        "class STGCN(nn.Module):\n",
        "    def __init__(self, adj, num_nodes, input_len=48, output_len=4, in_features=3, hidden=8):\n",
        "        super().__init__()\n",
        "        self.block1 = STGCNBlock(in_features, hidden, adj)\n",
        "        # Comment out block2 for a much smaller model\n",
        "        # self.block2 = STGCNBlock(hidden, hidden, adj)\n",
        "        self.fc = nn.Linear(input_len * hidden, output_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        # x = self.block2(x)  # REMOVE THIS LINE\n",
        "        B, T, N, H = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).reshape(B, N, T * H)\n",
        "        out = self.fc(x)\n",
        "        return out.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "# ------------------------- DATASET & UTILS -------------------------\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def train_val_split(X, y, val_ratio=0.2):\n",
        "    N = X.shape[0]\n",
        "    split = int(N * (1 - val_ratio))\n",
        "    X_train, X_val = X[:split], X[split:]\n",
        "    y_train, y_val = y[:split], y[split:]\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "# ------------------------- FEATURE ENGINEERING -------------------------\n",
        "\n",
        "def add_time_features(df, window_size, station_cols):\n",
        "    n_samples = len(df) - window_size - 4 + 1  # \"-4+1\" ensures we have room for pred window\n",
        "    n_stations = len(station_cols)\n",
        "    X, y = [], []\n",
        "    for i in range(n_samples):\n",
        "        window = df.iloc[i:i+window_size]\n",
        "        bikes = window[station_cols].astype(float).values\n",
        "        timestamps = pd.to_datetime(window['timestamp'])\n",
        "        hour_of_day = timestamps.dt.hour.values[:, None] / 23.0\n",
        "        day_of_week = timestamps.dt.dayofweek.values[:, None] / 6.0\n",
        "        hour_feat = np.tile(hour_of_day, (1, n_stations))\n",
        "        dow_feat = np.tile(day_of_week, (1, n_stations))\n",
        "        features = np.stack([bikes, hour_feat, dow_feat], axis=-1)  # [win, stations, 3]\n",
        "        X.append(features)\n",
        "        y_window = df.iloc[i+window_size:i+window_size+4][station_cols].astype(float).values\n",
        "        y.append(y_window)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ------------------------- TRAINING LOOP -------------------------\n",
        "\n",
        "def train_stgcn_with_val(X_train, y_train, X_val, y_val, adj,\n",
        "                         num_epochs=100, lr=0.001, patience=10, batch_size=32):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_nodes = X_train.shape[2]\n",
        "    in_features = X_train.shape[3]\n",
        "    input_len = X_train.shape[1]\n",
        "    output_len = y_train.shape[1]\n",
        "    model = STGCN(adj, num_nodes, input_len, output_len, in_features).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                out = model(xb)\n",
        "                val_loss = loss_fn(out, yb)\n",
        "                val_losses.append(val_loss.item())\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "def predict_batches(model, X, batch_size=32, device='cpu'):\n",
        "    model.eval()\n",
        "    dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X))\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for xb, in loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            preds.append(out.cpu().numpy())\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "# ------------------------- TEST SET PREDICTION & EXPORT -------------------------\n",
        "\n",
        "def predict_and_fill(model, test_df, adj, station_cols, device='cpu'):\n",
        "    filled_test = test_df.copy()\n",
        "    mask_missing = test_df[station_cols].isnull() | (test_df[station_cols] == '')\n",
        "\n",
        "    # Find all indices where a new hole starts\n",
        "    rows_with_holes = mask_missing.any(axis=1)\n",
        "    idx_missing = np.where(rows_with_holes)[0]\n",
        "    header = list(test_df.columns)\n",
        "    legend_row = pd.DataFrame([header], columns=header)\n",
        "\n",
        "    i = 0\n",
        "    while i < len(test_df):\n",
        "        if rows_with_holes.iloc[i]:\n",
        "            # Start of a gap\n",
        "            obs_idx = i - 48\n",
        "            if obs_idx < 0:\n",
        "                i += 1\n",
        "                continue  # not enough history\n",
        "            window = filled_test.iloc[obs_idx:i]\n",
        "            timestamps = pd.to_datetime(window['timestamp'])\n",
        "            hour_of_day = timestamps.dt.hour.values[:, None] / 23.0\n",
        "            day_of_week = timestamps.dt.dayofweek.values[:, None] / 6.0\n",
        "            bikes = window[station_cols].astype(float).values\n",
        "            hour_feat = np.tile(hour_of_day, (1, len(station_cols)))\n",
        "            dow_feat = np.tile(day_of_week, (1, len(station_cols)))\n",
        "            features = np.stack([bikes, hour_feat, dow_feat], axis=-1)\n",
        "            model_input = np.expand_dims(features, axis=0)  # [1, 48, num_stations, 3]\n",
        "\n",
        "            # Predict next 4 hours\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                inp = torch.FloatTensor(model_input).to(device)\n",
        "                pred = model(inp).cpu().numpy()  # [1, 4, num_stations]\n",
        "            pred = pred[0]\n",
        "            for j in range(4):\n",
        "                row_idx = i + j\n",
        "                if row_idx >= len(filled_test):\n",
        "                    break\n",
        "                for k, station in enumerate(station_cols):\n",
        "                    if pd.isnull(filled_test.loc[row_idx, station]) or filled_test.loc[row_idx, station] == '':\n",
        "                        filled_test.loc[row_idx, station] = float(pred[j, k])\n",
        "            i += 4\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # Output only rows where there were missing predictions\n",
        "    rows_with_preds = rows_with_holes\n",
        "    final_df = filled_test[rows_with_preds]\n",
        "    final_with_legend = pd.concat([legend_row, final_df], ignore_index=True)\n",
        "    final_with_legend.to_csv('napovedi.csv', index=False, header=False)\n",
        "    print(\"Saved napovedi.csv with legend and only rows containing predictions.\")\n",
        "\n",
        "# ------------------------- MAIN PIPELINE -------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Load your training data ---\n",
        "    train_df = pd.read_csv('bicikelj_train.csv')\n",
        "    station_cols = list(train_df.columns)[1:]  # skip timestamp\n",
        "\n",
        "    # --- Simple adjacency: identity (replace with real graph for best results) ---\n",
        "    adj = np.eye(len(station_cols))\n",
        "\n",
        "    # --- Feature engineering for train ---\n",
        "    # Normalize bike counts for all data columns\n",
        "    bike_min = train_df[station_cols].astype(float).min().min()\n",
        "    bike_max = train_df[station_cols].astype(float).max().max()\n",
        "    for col in station_cols:\n",
        "        train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "        train_df[station_cols] = train_df[station_cols].fillna(0)\n",
        "\n",
        "    train_df[station_cols] = (train_df[station_cols] - bike_min) / (bike_max - bike_min + 1e-8)\n",
        "\n",
        "    X, y = add_time_features(train_df, window_size=48, station_cols=station_cols)\n",
        "\n",
        "    # --- Chronological split ---\n",
        "    X_train, X_val, y_train, y_val = train_val_split(X, y, val_ratio=0.2)\n",
        "    print(\"Any NaN in X_train?\", np.isnan(X_train).any())\n",
        "    print(\"Any NaN in y_train?\", np.isnan(y_train).any())\n",
        "    print(\"X_train min/max\", X_train.min(), X_train.max())\n",
        "    print(\"y_train min/max\", y_train.min(), y_train.max())\n",
        "\n",
        "    # --- Train with early stopping ---\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = train_stgcn_with_val(\n",
        "        X_train, y_train, X_val, y_val, adj,\n",
        "        num_epochs=100, patience=10, batch_size=64)\n",
        "\n",
        "    # --- Validation MSE ---\n",
        "    val_pred = predict_batches(model, X_val, batch_size=64, device=device)\n",
        "    val_mse = mse(y_val, val_pred)\n",
        "    print(f'Validation MSE: {val_mse:.4f}')\n",
        "\n",
        "    # --- Test set prediction and export ---\n",
        "    test_df = pd.read_csv('bicikelj_test.csv', dtype=str)\n",
        "    predict_and_fill(model, test_df, adj, station_cols, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImDl8LyXe4w"
      },
      "source": [
        "# MLP Per station"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UymH-eMDXh4k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden=64, out_features=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_features)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YZVkCXIXj9H",
        "outputId": "8759ff2c-f609-4a8f-fe88-c46aff8f3e20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-f26298bf4ea3>:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Parameters ---\n",
        "history_len = 48\n",
        "pred_horizon = 4\n",
        "k_neighbors = 2\n",
        "hidden_dim = 64\n",
        "epochs = 25\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "# --- Load data ---\n",
        "train_df = pd.read_csv('bicikelj_train.csv')\n",
        "test_df = pd.read_csv('bicikelj_test.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "\n",
        "station_cols = train_df.columns[1:]\n",
        "for col in station_cols:\n",
        "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "train_df = train_df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- Neighbor detection ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371  # km\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:k_neighbors]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbxDGD0NXm3u"
      },
      "outputs": [],
      "source": [
        "class BikeDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.X = torch.tensor(features, dtype=torch.float32)\n",
        "        self.y = torch.tensor(targets, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "F2RJmSXCXoXK",
        "outputId": "8aae7e9c-c163-441e-af81-4715a6e59766"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-596409142e66>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnn_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_hist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnn_hist\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mhour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m23.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mown_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: avoid this kludge.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_guess_datetime_format_for_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_guess_datetime_format_for_array\u001b[0;34m(arr, dayfirst)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_non_nan_element\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_non_null\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E721\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# GH#32264 np.str_ object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             guessed_format = guess_datetime_format(\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mfirst_non_nan_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             )\n",
            "\u001b[0;32mparsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.guess_datetime_format\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing._fill_token\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    175\u001b[0m     a Match object, or None if no match was found.\"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "features_dict = {}\n",
        "targets_dict = {}\n",
        "\n",
        "for station in station_cols:\n",
        "    features = []\n",
        "    targets = []\n",
        "    nn_stations = neighbors[station]\n",
        "    for i in range(history_len, len(train_df) - pred_horizon + 1):\n",
        "        own_hist = train_df[station].iloc[i-history_len:i].values.astype(float)\n",
        "        nn_hist = []\n",
        "        for nn in nn_stations:\n",
        "            nn_hist.append(train_df[nn].iloc[i-history_len:i].values.astype(float))\n",
        "        nn_hist = np.concatenate(nn_hist) if nn_hist else np.zeros(0)\n",
        "        hour = pd.to_datetime(train_df['timestamp'].iloc[i]).hour / 23.0\n",
        "        dow = pd.to_datetime(train_df['timestamp'].iloc[i]).dayofweek / 6.0\n",
        "        f = np.concatenate([own_hist, nn_hist, [hour, dow]])\n",
        "        features.append(f)\n",
        "        target = train_df[station].iloc[i:i+pred_horizon].values.astype(float)\n",
        "        targets.append(target)\n",
        "    features_dict[station] = np.array(features)\n",
        "    targets_dict[station] = np.array(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11w7v--Laeil"
      },
      "outputs": [],
      "source": [
        "timestamps = pd.to_datetime(train_df['timestamp'])\n",
        "hours = (timestamps.dt.hour / 23.0).values\n",
        "dows = (timestamps.dt.dayofweek / 6.0).values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SkN4kpJaf8s"
      },
      "outputs": [],
      "source": [
        "bikes = train_df[station_cols].values.astype(float)\n",
        "timestamps = pd.to_datetime(train_df['timestamp'])\n",
        "hours = (timestamps.dt.hour / 23.0).values\n",
        "dows = (timestamps.dt.dayofweek / 6.0).values\n",
        "features_dict = {}\n",
        "targets_dict = {}\n",
        "\n",
        "neighbor_indices = {station: [station_cols.get_loc(nn) for nn in neighbors[station]] for station in station_cols}\n",
        "\n",
        "for s_idx, station in enumerate(station_cols):\n",
        "    features = []\n",
        "    targets = []\n",
        "    nn_idx = neighbor_indices[station]\n",
        "    for i in range(history_len, len(bikes) - pred_horizon + 1):\n",
        "        own_hist = bikes[i-history_len:i, s_idx]\n",
        "        nn_hist = bikes[i-history_len:i][:, nn_idx].flatten()  # shape: history_len * k_neighbors\n",
        "        hour = hours[i]\n",
        "        dow = dows[i]\n",
        "        f = np.concatenate([own_hist, nn_hist, [hour, dow]])\n",
        "        features.append(f)\n",
        "        targets.append(bikes[i:i+pred_horizon, s_idx])\n",
        "    features_dict[station] = np.array(features)\n",
        "    targets_dict[station] = np.array(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVZ0Z4lhXqH7",
        "outputId": "7c5c4bb2-e489-4cc5-b7ba-9fa541099de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Station: LIDL BEŽIGRAD | Epoch 1/50 | Loss: 17.4032\n",
            "Station: LIDL BEŽIGRAD | Epoch 6/50 | Loss: 10.9166\n",
            "Station: LIDL BEŽIGRAD | Epoch 11/50 | Loss: 10.0934\n",
            "Station: LIDL BEŽIGRAD | Epoch 16/50 | Loss: 9.2770\n",
            "Station: LIDL BEŽIGRAD | Epoch 21/50 | Loss: 8.5763\n",
            "Station: LIDL BEŽIGRAD | Epoch 26/50 | Loss: 8.0435\n",
            "Station: LIDL BEŽIGRAD | Epoch 31/50 | Loss: 7.5349\n",
            "Station: LIDL BEŽIGRAD | Epoch 36/50 | Loss: 7.1985\n",
            "Station: LIDL BEŽIGRAD | Epoch 41/50 | Loss: 6.8938\n",
            "Station: LIDL BEŽIGRAD | Epoch 46/50 | Loss: 6.5745\n",
            "Station: LIDL BEŽIGRAD | Epoch 50/50 | Loss: 6.5241\n",
            "Station: ŠMARTINSKI PARK | Epoch 1/50 | Loss: 15.9760\n",
            "Station: ŠMARTINSKI PARK | Epoch 6/50 | Loss: 7.6463\n",
            "Station: ŠMARTINSKI PARK | Epoch 11/50 | Loss: 7.2727\n",
            "Station: ŠMARTINSKI PARK | Epoch 16/50 | Loss: 7.1046\n",
            "Station: ŠMARTINSKI PARK | Epoch 21/50 | Loss: 6.8524\n",
            "Station: ŠMARTINSKI PARK | Epoch 26/50 | Loss: 6.7038\n",
            "Station: ŠMARTINSKI PARK | Epoch 31/50 | Loss: 6.4399\n",
            "Station: ŠMARTINSKI PARK | Epoch 36/50 | Loss: 6.1399\n",
            "Station: ŠMARTINSKI PARK | Epoch 41/50 | Loss: 5.9722\n",
            "Station: ŠMARTINSKI PARK | Epoch 46/50 | Loss: 5.8349\n",
            "Station: ŠMARTINSKI PARK | Epoch 50/50 | Loss: 5.6662\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 1/50 | Loss: 14.8541\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 6/50 | Loss: 6.5023\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 11/50 | Loss: 6.1807\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 16/50 | Loss: 5.8968\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 21/50 | Loss: 5.6435\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 26/50 | Loss: 5.4478\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 31/50 | Loss: 5.2454\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 36/50 | Loss: 5.0657\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 41/50 | Loss: 4.8906\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 46/50 | Loss: 4.8274\n",
            "Station: SAVSKO NASELJE 1-ŠMARTINSKA CESTA | Epoch 50/50 | Loss: 4.5930\n",
            "Station: ČRNUČE | Epoch 1/50 | Loss: 16.0220\n",
            "Station: ČRNUČE | Epoch 6/50 | Loss: 6.1663\n",
            "Station: ČRNUČE | Epoch 11/50 | Loss: 5.6668\n",
            "Station: ČRNUČE | Epoch 16/50 | Loss: 5.5117\n",
            "Station: ČRNUČE | Epoch 21/50 | Loss: 5.2337\n",
            "Station: ČRNUČE | Epoch 26/50 | Loss: 5.0694\n",
            "Station: ČRNUČE | Epoch 31/50 | Loss: 4.9368\n",
            "Station: ČRNUČE | Epoch 36/50 | Loss: 4.8725\n",
            "Station: ČRNUČE | Epoch 41/50 | Loss: 4.6977\n",
            "Station: ČRNUČE | Epoch 46/50 | Loss: 4.5031\n",
            "Station: ČRNUČE | Epoch 50/50 | Loss: 4.4012\n",
            "Station: VILHARJEVA CESTA | Epoch 1/50 | Loss: 27.9217\n",
            "Station: VILHARJEVA CESTA | Epoch 6/50 | Loss: 13.8507\n",
            "Station: VILHARJEVA CESTA | Epoch 11/50 | Loss: 12.8052\n",
            "Station: VILHARJEVA CESTA | Epoch 16/50 | Loss: 11.9793\n",
            "Station: VILHARJEVA CESTA | Epoch 21/50 | Loss: 11.2863\n",
            "Station: VILHARJEVA CESTA | Epoch 26/50 | Loss: 10.6831\n",
            "Station: VILHARJEVA CESTA | Epoch 31/50 | Loss: 10.2767\n",
            "Station: VILHARJEVA CESTA | Epoch 36/50 | Loss: 9.7145\n",
            "Station: VILHARJEVA CESTA | Epoch 41/50 | Loss: 9.4528\n",
            "Station: VILHARJEVA CESTA | Epoch 46/50 | Loss: 9.0402\n",
            "Station: VILHARJEVA CESTA | Epoch 50/50 | Loss: 8.8457\n",
            "Station: MASARYKOVA DDC | Epoch 1/50 | Loss: 20.6137\n",
            "Station: MASARYKOVA DDC | Epoch 6/50 | Loss: 11.8990\n",
            "Station: MASARYKOVA DDC | Epoch 11/50 | Loss: 10.9560\n",
            "Station: MASARYKOVA DDC | Epoch 16/50 | Loss: 10.4813\n",
            "Station: MASARYKOVA DDC | Epoch 21/50 | Loss: 9.7764\n",
            "Station: MASARYKOVA DDC | Epoch 26/50 | Loss: 9.2660\n",
            "Station: MASARYKOVA DDC | Epoch 31/50 | Loss: 8.8016\n",
            "Station: MASARYKOVA DDC | Epoch 36/50 | Loss: 8.4563\n",
            "Station: MASARYKOVA DDC | Epoch 41/50 | Loss: 8.0430\n",
            "Station: MASARYKOVA DDC | Epoch 46/50 | Loss: 7.7282\n",
            "Station: MASARYKOVA DDC | Epoch 50/50 | Loss: 7.6343\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 1/50 | Loss: 24.0103\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 6/50 | Loss: 12.7570\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 11/50 | Loss: 11.7956\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 16/50 | Loss: 11.1126\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 21/50 | Loss: 10.5388\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 26/50 | Loss: 10.1430\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 31/50 | Loss: 9.7085\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 36/50 | Loss: 9.3907\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 41/50 | Loss: 9.0848\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 46/50 | Loss: 8.7586\n",
            "Station: POGAČARJEV TRG-TRŽNICA | Epoch 50/50 | Loss: 8.5327\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 1/50 | Loss: 46.7480\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 6/50 | Loss: 30.2485\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 11/50 | Loss: 28.2145\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 16/50 | Loss: 26.7593\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 21/50 | Loss: 25.4952\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 26/50 | Loss: 24.5040\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 31/50 | Loss: 23.4172\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 36/50 | Loss: 22.6650\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 41/50 | Loss: 22.0675\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 46/50 | Loss: 21.4086\n",
            "Station: CANKARJEVA UL.-NAMA | Epoch 50/50 | Loss: 20.9377\n",
            "Station: ANTONOV TRG | Epoch 1/50 | Loss: 31.4773\n",
            "Station: ANTONOV TRG | Epoch 6/50 | Loss: 13.3484\n",
            "Station: ANTONOV TRG | Epoch 11/50 | Loss: 12.6285\n",
            "Station: ANTONOV TRG | Epoch 16/50 | Loss: 12.2318\n",
            "Station: ANTONOV TRG | Epoch 21/50 | Loss: 11.9098\n",
            "Station: ANTONOV TRG | Epoch 26/50 | Loss: 11.7055\n",
            "Station: ANTONOV TRG | Epoch 31/50 | Loss: 11.3916\n",
            "Station: ANTONOV TRG | Epoch 36/50 | Loss: 11.2302\n",
            "Station: ANTONOV TRG | Epoch 41/50 | Loss: 11.1298\n",
            "Station: ANTONOV TRG | Epoch 46/50 | Loss: 10.8477\n",
            "Station: ANTONOV TRG | Epoch 50/50 | Loss: 10.7826\n",
            "Station: PRUŠNIKOVA | Epoch 1/50 | Loss: 14.9845\n",
            "Station: PRUŠNIKOVA | Epoch 6/50 | Loss: 5.6050\n",
            "Station: PRUŠNIKOVA | Epoch 11/50 | Loss: 5.3930\n",
            "Station: PRUŠNIKOVA | Epoch 16/50 | Loss: 5.1718\n",
            "Station: PRUŠNIKOVA | Epoch 21/50 | Loss: 5.0345\n",
            "Station: PRUŠNIKOVA | Epoch 26/50 | Loss: 4.8554\n",
            "Station: PRUŠNIKOVA | Epoch 31/50 | Loss: 4.6787\n",
            "Station: PRUŠNIKOVA | Epoch 36/50 | Loss: 4.5268\n",
            "Station: PRUŠNIKOVA | Epoch 41/50 | Loss: 4.4016\n",
            "Station: PRUŠNIKOVA | Epoch 46/50 | Loss: 4.2303\n",
            "Station: PRUŠNIKOVA | Epoch 50/50 | Loss: 4.1374\n",
            "Station: TEHNOLOŠKI PARK | Epoch 1/50 | Loss: 11.3267\n",
            "Station: TEHNOLOŠKI PARK | Epoch 6/50 | Loss: 3.5921\n",
            "Station: TEHNOLOŠKI PARK | Epoch 11/50 | Loss: 3.3055\n",
            "Station: TEHNOLOŠKI PARK | Epoch 16/50 | Loss: 3.0656\n",
            "Station: TEHNOLOŠKI PARK | Epoch 21/50 | Loss: 3.1021\n",
            "Station: TEHNOLOŠKI PARK | Epoch 26/50 | Loss: 2.8615\n",
            "Station: TEHNOLOŠKI PARK | Epoch 31/50 | Loss: 2.7550\n",
            "Station: TEHNOLOŠKI PARK | Epoch 36/50 | Loss: 2.6323\n",
            "Station: TEHNOLOŠKI PARK | Epoch 41/50 | Loss: 2.6259\n",
            "Station: TEHNOLOŠKI PARK | Epoch 46/50 | Loss: 2.5474\n",
            "Station: TEHNOLOŠKI PARK | Epoch 50/50 | Loss: 2.4827\n",
            "Station: KOSEŠKI BAJER | Epoch 1/50 | Loss: 17.1663\n",
            "Station: KOSEŠKI BAJER | Epoch 6/50 | Loss: 6.0487\n",
            "Station: KOSEŠKI BAJER | Epoch 11/50 | Loss: 5.7935\n",
            "Station: KOSEŠKI BAJER | Epoch 16/50 | Loss: 5.6718\n",
            "Station: KOSEŠKI BAJER | Epoch 21/50 | Loss: 5.6587\n",
            "Station: KOSEŠKI BAJER | Epoch 26/50 | Loss: 5.4521\n",
            "Station: KOSEŠKI BAJER | Epoch 31/50 | Loss: 5.4265\n",
            "Station: KOSEŠKI BAJER | Epoch 36/50 | Loss: 5.2198\n",
            "Station: KOSEŠKI BAJER | Epoch 41/50 | Loss: 5.0276\n",
            "Station: KOSEŠKI BAJER | Epoch 46/50 | Loss: 4.9174\n",
            "Station: KOSEŠKI BAJER | Epoch 50/50 | Loss: 4.8294\n",
            "Station: TIVOLI | Epoch 1/50 | Loss: 24.6074\n",
            "Station: TIVOLI | Epoch 6/50 | Loss: 16.3989\n",
            "Station: TIVOLI | Epoch 11/50 | Loss: 15.1963\n",
            "Station: TIVOLI | Epoch 16/50 | Loss: 14.3198\n",
            "Station: TIVOLI | Epoch 21/50 | Loss: 13.3477\n",
            "Station: TIVOLI | Epoch 26/50 | Loss: 12.6213\n",
            "Station: TIVOLI | Epoch 31/50 | Loss: 12.0383\n",
            "Station: TIVOLI | Epoch 36/50 | Loss: 11.4287\n",
            "Station: TIVOLI | Epoch 41/50 | Loss: 10.8858\n",
            "Station: TIVOLI | Epoch 46/50 | Loss: 10.6703\n",
            "Station: TIVOLI | Epoch 50/50 | Loss: 10.3091\n",
            "Station: TRŽNICA MOSTE | Epoch 1/50 | Loss: 24.6407\n",
            "Station: TRŽNICA MOSTE | Epoch 6/50 | Loss: 10.6808\n",
            "Station: TRŽNICA MOSTE | Epoch 11/50 | Loss: 10.1517\n",
            "Station: TRŽNICA MOSTE | Epoch 16/50 | Loss: 9.9340\n",
            "Station: TRŽNICA MOSTE | Epoch 21/50 | Loss: 9.7474\n",
            "Station: TRŽNICA MOSTE | Epoch 26/50 | Loss: 9.5595\n",
            "Station: TRŽNICA MOSTE | Epoch 31/50 | Loss: 9.4818\n",
            "Station: TRŽNICA MOSTE | Epoch 36/50 | Loss: 9.1246\n",
            "Station: TRŽNICA MOSTE | Epoch 41/50 | Loss: 9.1213\n",
            "Station: TRŽNICA MOSTE | Epoch 46/50 | Loss: 8.7971\n",
            "Station: TRŽNICA MOSTE | Epoch 50/50 | Loss: 8.7673\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 1/50 | Loss: 18.0282\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 6/50 | Loss: 10.3809\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 11/50 | Loss: 9.7264\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 16/50 | Loss: 9.1542\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 21/50 | Loss: 8.4834\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 26/50 | Loss: 7.9796\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 31/50 | Loss: 7.6320\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 36/50 | Loss: 7.2624\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 41/50 | Loss: 6.9151\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 46/50 | Loss: 6.6311\n",
            "Station: GRUDNOVO NABREŽJE-KARLOVŠKA C. | Epoch 50/50 | Loss: 6.4320\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 1/50 | Loss: 17.9703\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 6/50 | Loss: 7.3405\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 11/50 | Loss: 6.8421\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 16/50 | Loss: 6.5879\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 21/50 | Loss: 6.4818\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 26/50 | Loss: 6.3155\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 31/50 | Loss: 6.2415\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 36/50 | Loss: 6.1580\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 41/50 | Loss: 5.9928\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 46/50 | Loss: 5.9414\n",
            "Station: LIDL-LITIJSKA CESTA | Epoch 50/50 | Loss: 5.8548\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 1/50 | Loss: 26.9982\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 6/50 | Loss: 11.2671\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 11/50 | Loss: 10.6742\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 16/50 | Loss: 10.3389\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 21/50 | Loss: 9.9748\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 26/50 | Loss: 9.6859\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 31/50 | Loss: 9.4719\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 36/50 | Loss: 9.2630\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 41/50 | Loss: 8.9756\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 46/50 | Loss: 8.7804\n",
            "Station: ŠPORTNI CENTER STOŽICE | Epoch 50/50 | Loss: 8.5259\n",
            "Station: ŠPICA | Epoch 1/50 | Loss: 22.2147\n",
            "Station: ŠPICA | Epoch 6/50 | Loss: 11.2900\n",
            "Station: ŠPICA | Epoch 11/50 | Loss: 10.8356\n",
            "Station: ŠPICA | Epoch 16/50 | Loss: 10.4485\n",
            "Station: ŠPICA | Epoch 21/50 | Loss: 10.0508\n",
            "Station: ŠPICA | Epoch 26/50 | Loss: 9.7564\n",
            "Station: ŠPICA | Epoch 31/50 | Loss: 9.6231\n",
            "Station: ŠPICA | Epoch 36/50 | Loss: 9.3014\n",
            "Station: ŠPICA | Epoch 41/50 | Loss: 9.0517\n",
            "Station: ŠPICA | Epoch 46/50 | Loss: 8.8565\n",
            "Station: ŠPICA | Epoch 50/50 | Loss: 8.6911\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 1/50 | Loss: 11.9465\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 6/50 | Loss: 5.6222\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 11/50 | Loss: 5.3777\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 16/50 | Loss: 5.0142\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 21/50 | Loss: 4.7530\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 26/50 | Loss: 4.4427\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 31/50 | Loss: 4.2397\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 36/50 | Loss: 4.1265\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 41/50 | Loss: 3.9282\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 46/50 | Loss: 3.8361\n",
            "Station: ROŠKA - STRELIŠKA | Epoch 50/50 | Loss: 3.6961\n",
            "Station: BAVARSKI DVOR | Epoch 1/50 | Loss: 24.6297\n",
            "Station: BAVARSKI DVOR | Epoch 6/50 | Loss: 16.7852\n",
            "Station: BAVARSKI DVOR | Epoch 11/50 | Loss: 15.7059\n",
            "Station: BAVARSKI DVOR | Epoch 16/50 | Loss: 14.7604\n",
            "Station: BAVARSKI DVOR | Epoch 21/50 | Loss: 14.1265\n",
            "Station: BAVARSKI DVOR | Epoch 26/50 | Loss: 13.4104\n",
            "Station: BAVARSKI DVOR | Epoch 31/50 | Loss: 12.9569\n",
            "Station: BAVARSKI DVOR | Epoch 36/50 | Loss: 12.4567\n",
            "Station: BAVARSKI DVOR | Epoch 41/50 | Loss: 12.0464\n",
            "Station: BAVARSKI DVOR | Epoch 46/50 | Loss: 11.8133\n",
            "Station: BAVARSKI DVOR | Epoch 50/50 | Loss: 11.5120\n",
            "Station: STARA CERKEV | Epoch 1/50 | Loss: 14.3738\n",
            "Station: STARA CERKEV | Epoch 6/50 | Loss: 8.1159\n",
            "Station: STARA CERKEV | Epoch 11/50 | Loss: 7.6279\n",
            "Station: STARA CERKEV | Epoch 16/50 | Loss: 7.3169\n",
            "Station: STARA CERKEV | Epoch 21/50 | Loss: 6.9389\n",
            "Station: STARA CERKEV | Epoch 26/50 | Loss: 6.5754\n",
            "Station: STARA CERKEV | Epoch 31/50 | Loss: 6.2772\n",
            "Station: STARA CERKEV | Epoch 36/50 | Loss: 6.0053\n",
            "Station: STARA CERKEV | Epoch 41/50 | Loss: 5.7859\n",
            "Station: STARA CERKEV | Epoch 46/50 | Loss: 5.5903\n",
            "Station: STARA CERKEV | Epoch 50/50 | Loss: 5.4383\n",
            "Station: SITULA | Epoch 1/50 | Loss: 21.6163\n",
            "Station: SITULA | Epoch 6/50 | Loss: 11.8589\n",
            "Station: SITULA | Epoch 11/50 | Loss: 11.2988\n",
            "Station: SITULA | Epoch 16/50 | Loss: 10.9043\n",
            "Station: SITULA | Epoch 21/50 | Loss: 10.3123\n",
            "Station: SITULA | Epoch 26/50 | Loss: 10.0132\n",
            "Station: SITULA | Epoch 31/50 | Loss: 9.6522\n",
            "Station: SITULA | Epoch 36/50 | Loss: 9.3097\n",
            "Station: SITULA | Epoch 41/50 | Loss: 8.9529\n",
            "Station: SITULA | Epoch 46/50 | Loss: 8.7148\n",
            "Station: SITULA | Epoch 50/50 | Loss: 8.6128\n",
            "Station: ILIRSKA ULICA | Epoch 1/50 | Loss: 17.8066\n",
            "Station: ILIRSKA ULICA | Epoch 6/50 | Loss: 11.4930\n",
            "Station: ILIRSKA ULICA | Epoch 11/50 | Loss: 10.7866\n",
            "Station: ILIRSKA ULICA | Epoch 16/50 | Loss: 10.0745\n",
            "Station: ILIRSKA ULICA | Epoch 21/50 | Loss: 9.3924\n",
            "Station: ILIRSKA ULICA | Epoch 26/50 | Loss: 8.8015\n",
            "Station: ILIRSKA ULICA | Epoch 31/50 | Loss: 8.3037\n",
            "Station: ILIRSKA ULICA | Epoch 36/50 | Loss: 8.0050\n",
            "Station: ILIRSKA ULICA | Epoch 41/50 | Loss: 7.5532\n",
            "Station: ILIRSKA ULICA | Epoch 46/50 | Loss: 7.3370\n",
            "Station: ILIRSKA ULICA | Epoch 50/50 | Loss: 7.0934\n",
            "Station: LIDL - RUDNIK | Epoch 1/50 | Loss: 9.6024\n",
            "Station: LIDL - RUDNIK | Epoch 6/50 | Loss: 2.3465\n",
            "Station: LIDL - RUDNIK | Epoch 11/50 | Loss: 2.1486\n",
            "Station: LIDL - RUDNIK | Epoch 16/50 | Loss: 2.0895\n",
            "Station: LIDL - RUDNIK | Epoch 21/50 | Loss: 2.0392\n",
            "Station: LIDL - RUDNIK | Epoch 26/50 | Loss: 2.0637\n",
            "Station: LIDL - RUDNIK | Epoch 31/50 | Loss: 1.9334\n",
            "Station: LIDL - RUDNIK | Epoch 36/50 | Loss: 1.9009\n",
            "Station: LIDL - RUDNIK | Epoch 41/50 | Loss: 1.9051\n",
            "Station: LIDL - RUDNIK | Epoch 46/50 | Loss: 1.8548\n",
            "Station: LIDL - RUDNIK | Epoch 50/50 | Loss: 1.8714\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 1/50 | Loss: 17.6333\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 6/50 | Loss: 8.6473\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 11/50 | Loss: 7.9715\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 16/50 | Loss: 7.4618\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 21/50 | Loss: 7.1960\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 26/50 | Loss: 6.9402\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 31/50 | Loss: 6.5329\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 36/50 | Loss: 6.3424\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 41/50 | Loss: 6.0354\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 46/50 | Loss: 5.8448\n",
            "Station: KOPALIŠČE KOLEZIJA | Epoch 50/50 | Loss: 5.7616\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 1/50 | Loss: 21.8053\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 6/50 | Loss: 7.5368\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 11/50 | Loss: 6.9666\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 16/50 | Loss: 6.7790\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 21/50 | Loss: 6.6146\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 26/50 | Loss: 6.5400\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 31/50 | Loss: 6.3266\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 36/50 | Loss: 6.2549\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 41/50 | Loss: 6.1052\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 46/50 | Loss: 6.0889\n",
            "Station: POVŠETOVA - KAJUHOVA | Epoch 50/50 | Loss: 5.9461\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 1/50 | Loss: 19.6700\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 6/50 | Loss: 11.8478\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 11/50 | Loss: 11.2925\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 16/50 | Loss: 10.8052\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 21/50 | Loss: 10.3995\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 26/50 | Loss: 9.9474\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 31/50 | Loss: 9.4892\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 36/50 | Loss: 9.1752\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 41/50 | Loss: 8.8390\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 46/50 | Loss: 8.4937\n",
            "Station: DUNAJSKA C.-PS MERCATOR | Epoch 50/50 | Loss: 8.3096\n",
            "Station: CITYPARK | Epoch 1/50 | Loss: 19.1027\n",
            "Station: CITYPARK | Epoch 6/50 | Loss: 8.2173\n",
            "Station: CITYPARK | Epoch 11/50 | Loss: 7.6424\n",
            "Station: CITYPARK | Epoch 16/50 | Loss: 7.4752\n",
            "Station: CITYPARK | Epoch 21/50 | Loss: 7.1981\n",
            "Station: CITYPARK | Epoch 26/50 | Loss: 6.7959\n",
            "Station: CITYPARK | Epoch 31/50 | Loss: 6.6082\n",
            "Station: CITYPARK | Epoch 36/50 | Loss: 6.2132\n",
            "Station: CITYPARK | Epoch 41/50 | Loss: 5.9434\n",
            "Station: CITYPARK | Epoch 46/50 | Loss: 5.8254\n",
            "Station: CITYPARK | Epoch 50/50 | Loss: 5.5758\n",
            "Station: KOPRSKA ULICA | Epoch 1/50 | Loss: 3.0086\n",
            "Station: KOPRSKA ULICA | Epoch 6/50 | Loss: 1.3522\n",
            "Station: KOPRSKA ULICA | Epoch 11/50 | Loss: 1.2823\n",
            "Station: KOPRSKA ULICA | Epoch 16/50 | Loss: 1.2307\n",
            "Station: KOPRSKA ULICA | Epoch 21/50 | Loss: 1.1795\n",
            "Station: KOPRSKA ULICA | Epoch 26/50 | Loss: 1.1420\n",
            "Station: KOPRSKA ULICA | Epoch 31/50 | Loss: 1.0928\n",
            "Station: KOPRSKA ULICA | Epoch 36/50 | Loss: 1.0756\n",
            "Station: KOPRSKA ULICA | Epoch 41/50 | Loss: 1.0459\n",
            "Station: KOPRSKA ULICA | Epoch 46/50 | Loss: 1.0079\n",
            "Station: KOPRSKA ULICA | Epoch 50/50 | Loss: 0.9948\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 1/50 | Loss: 26.9818\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 6/50 | Loss: 13.8620\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 11/50 | Loss: 13.3721\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 16/50 | Loss: 13.1000\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 21/50 | Loss: 12.7726\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 26/50 | Loss: 12.2784\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 31/50 | Loss: 12.2633\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 36/50 | Loss: 11.5325\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 41/50 | Loss: 11.3323\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 46/50 | Loss: 10.9509\n",
            "Station: LIDL - VOJKOVA CESTA | Epoch 50/50 | Loss: 10.7731\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 1/50 | Loss: 21.7016\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 6/50 | Loss: 10.6135\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 11/50 | Loss: 9.7442\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 16/50 | Loss: 9.2755\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 21/50 | Loss: 8.8200\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 26/50 | Loss: 8.2969\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 31/50 | Loss: 7.9221\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 36/50 | Loss: 7.5093\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 41/50 | Loss: 7.2794\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 46/50 | Loss: 6.9936\n",
            "Station: POLJANSKA-POTOČNIKOVA | Epoch 50/50 | Loss: 6.7894\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 1/50 | Loss: 26.8930\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 6/50 | Loss: 12.6789\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 11/50 | Loss: 12.0830\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 16/50 | Loss: 11.9145\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 21/50 | Loss: 11.4941\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 26/50 | Loss: 11.2325\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 31/50 | Loss: 10.9283\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 36/50 | Loss: 10.7425\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 41/50 | Loss: 10.3797\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 46/50 | Loss: 10.2369\n",
            "Station: POVŠETOVA-GRABLOVIČEVA | Epoch 50/50 | Loss: 9.9416\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 1/50 | Loss: 16.2680\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 6/50 | Loss: 9.3739\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 11/50 | Loss: 8.7566\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 16/50 | Loss: 8.2517\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 21/50 | Loss: 7.8325\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 26/50 | Loss: 7.3133\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 31/50 | Loss: 6.9016\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 36/50 | Loss: 6.6341\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 41/50 | Loss: 6.2627\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 46/50 | Loss: 6.0661\n",
            "Station: PARK NAVJE-ŽELEZNA CESTA | Epoch 50/50 | Loss: 5.8749\n",
            "Station: ZALOG | Epoch 1/50 | Loss: 9.5565\n",
            "Station: ZALOG | Epoch 6/50 | Loss: 2.3234\n",
            "Station: ZALOG | Epoch 11/50 | Loss: 2.2372\n",
            "Station: ZALOG | Epoch 16/50 | Loss: 2.1535\n",
            "Station: ZALOG | Epoch 21/50 | Loss: 2.0954\n",
            "Station: ZALOG | Epoch 26/50 | Loss: 2.0732\n",
            "Station: ZALOG | Epoch 31/50 | Loss: 2.0656\n",
            "Station: ZALOG | Epoch 36/50 | Loss: 1.9706\n",
            "Station: ZALOG | Epoch 41/50 | Loss: 1.9720\n",
            "Station: ZALOG | Epoch 46/50 | Loss: 1.9418\n",
            "Station: ZALOG | Epoch 50/50 | Loss: 1.8874\n",
            "Station: CESTA NA ROŽNIK | Epoch 1/50 | Loss: 18.1301\n",
            "Station: CESTA NA ROŽNIK | Epoch 6/50 | Loss: 9.8610\n",
            "Station: CESTA NA ROŽNIK | Epoch 11/50 | Loss: 9.4023\n",
            "Station: CESTA NA ROŽNIK | Epoch 16/50 | Loss: 8.9771\n",
            "Station: CESTA NA ROŽNIK | Epoch 21/50 | Loss: 8.6111\n",
            "Station: CESTA NA ROŽNIK | Epoch 26/50 | Loss: 8.2708\n",
            "Station: CESTA NA ROŽNIK | Epoch 31/50 | Loss: 7.8021\n",
            "Station: CESTA NA ROŽNIK | Epoch 36/50 | Loss: 7.5228\n",
            "Station: CESTA NA ROŽNIK | Epoch 41/50 | Loss: 7.1831\n",
            "Station: CESTA NA ROŽNIK | Epoch 46/50 | Loss: 6.9203\n",
            "Station: CESTA NA ROŽNIK | Epoch 50/50 | Loss: 6.5324\n",
            "Station: HOFER-KAJUHOVA | Epoch 1/50 | Loss: 21.8977\n",
            "Station: HOFER-KAJUHOVA | Epoch 6/50 | Loss: 7.5538\n",
            "Station: HOFER-KAJUHOVA | Epoch 11/50 | Loss: 7.3243\n",
            "Station: HOFER-KAJUHOVA | Epoch 16/50 | Loss: 7.1920\n",
            "Station: HOFER-KAJUHOVA | Epoch 21/50 | Loss: 6.9955\n",
            "Station: HOFER-KAJUHOVA | Epoch 26/50 | Loss: 7.0228\n",
            "Station: HOFER-KAJUHOVA | Epoch 31/50 | Loss: 6.7698\n",
            "Station: HOFER-KAJUHOVA | Epoch 36/50 | Loss: 6.6540\n",
            "Station: HOFER-KAJUHOVA | Epoch 41/50 | Loss: 6.6131\n",
            "Station: HOFER-KAJUHOVA | Epoch 46/50 | Loss: 6.4378\n",
            "Station: HOFER-KAJUHOVA | Epoch 50/50 | Loss: 6.3326\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 1/50 | Loss: 17.6292\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 6/50 | Loss: 11.4945\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 11/50 | Loss: 10.6695\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 16/50 | Loss: 9.7572\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 21/50 | Loss: 8.9241\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 26/50 | Loss: 8.2266\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 31/50 | Loss: 7.8064\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 36/50 | Loss: 7.4070\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 41/50 | Loss: 7.0951\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 46/50 | Loss: 6.7766\n",
            "Station: DUNAJSKA C.-PS PETROL | Epoch 50/50 | Loss: 6.6460\n",
            "Station: STUDENEC | Epoch 1/50 | Loss: 16.0285\n",
            "Station: STUDENEC | Epoch 6/50 | Loss: 4.4136\n",
            "Station: STUDENEC | Epoch 11/50 | Loss: 4.0647\n",
            "Station: STUDENEC | Epoch 16/50 | Loss: 4.1229\n",
            "Station: STUDENEC | Epoch 21/50 | Loss: 3.9340\n",
            "Station: STUDENEC | Epoch 26/50 | Loss: 3.8118\n",
            "Station: STUDENEC | Epoch 31/50 | Loss: 3.8300\n",
            "Station: STUDENEC | Epoch 36/50 | Loss: 3.6795\n",
            "Station: STUDENEC | Epoch 41/50 | Loss: 3.6363\n",
            "Station: STUDENEC | Epoch 46/50 | Loss: 3.5783\n",
            "Station: STUDENEC | Epoch 50/50 | Loss: 3.6356\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 1/50 | Loss: 23.5777\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 6/50 | Loss: 16.0838\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 11/50 | Loss: 14.6845\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 16/50 | Loss: 13.7749\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 21/50 | Loss: 12.8351\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 26/50 | Loss: 12.1289\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 31/50 | Loss: 11.4141\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 36/50 | Loss: 11.0454\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 41/50 | Loss: 10.6204\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 46/50 | Loss: 10.2566\n",
            "Station: PARKIRIŠČE NUK 2-FF | Epoch 50/50 | Loss: 10.1303\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 1/50 | Loss: 25.2421\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 6/50 | Loss: 8.4045\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 11/50 | Loss: 7.9657\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 16/50 | Loss: 7.9216\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 21/50 | Loss: 7.7733\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 26/50 | Loss: 7.7050\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 31/50 | Loss: 7.3876\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 36/50 | Loss: 7.2193\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 41/50 | Loss: 7.1458\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 46/50 | Loss: 6.9832\n",
            "Station: BRATOVŠEVA PLOŠČAD | Epoch 50/50 | Loss: 6.9338\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 1/50 | Loss: 29.4555\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 6/50 | Loss: 18.6687\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 11/50 | Loss: 17.3189\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 16/50 | Loss: 16.2547\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 21/50 | Loss: 15.4528\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 26/50 | Loss: 14.8756\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 31/50 | Loss: 14.2043\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 36/50 | Loss: 13.6465\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 41/50 | Loss: 13.1995\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 46/50 | Loss: 12.7963\n",
            "Station: KONGRESNI TRG-ŠUBIČEVA ULICA | Epoch 50/50 | Loss: 12.4467\n",
            "Station: BS4-STOŽICE | Epoch 1/50 | Loss: 19.5415\n",
            "Station: BS4-STOŽICE | Epoch 6/50 | Loss: 5.7540\n",
            "Station: BS4-STOŽICE | Epoch 11/50 | Loss: 5.5286\n",
            "Station: BS4-STOŽICE | Epoch 16/50 | Loss: 5.3270\n",
            "Station: BS4-STOŽICE | Epoch 21/50 | Loss: 5.1216\n",
            "Station: BS4-STOŽICE | Epoch 26/50 | Loss: 5.0892\n",
            "Station: BS4-STOŽICE | Epoch 31/50 | Loss: 4.9524\n",
            "Station: BS4-STOŽICE | Epoch 36/50 | Loss: 4.9098\n",
            "Station: BS4-STOŽICE | Epoch 41/50 | Loss: 4.8029\n",
            "Station: BS4-STOŽICE | Epoch 46/50 | Loss: 4.7405\n",
            "Station: BS4-STOŽICE | Epoch 50/50 | Loss: 4.6716\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 1/50 | Loss: 26.3296\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 6/50 | Loss: 12.7323\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 11/50 | Loss: 11.9501\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 16/50 | Loss: 11.6396\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 21/50 | Loss: 11.4289\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 26/50 | Loss: 11.0948\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 31/50 | Loss: 10.8177\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 36/50 | Loss: 10.5277\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 41/50 | Loss: 10.4118\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 46/50 | Loss: 10.1017\n",
            "Station: GERBIČEVA - ŠPORTNI PARK SVOBODA | Epoch 50/50 | Loss: 9.9378\n",
            "Station: ŽIVALSKI VRT | Epoch 1/50 | Loss: 15.2029\n",
            "Station: ŽIVALSKI VRT | Epoch 6/50 | Loss: 7.3954\n",
            "Station: ŽIVALSKI VRT | Epoch 11/50 | Loss: 6.9725\n",
            "Station: ŽIVALSKI VRT | Epoch 16/50 | Loss: 6.6394\n",
            "Station: ŽIVALSKI VRT | Epoch 21/50 | Loss: 6.3571\n",
            "Station: ŽIVALSKI VRT | Epoch 26/50 | Loss: 6.1318\n",
            "Station: ŽIVALSKI VRT | Epoch 31/50 | Loss: 5.9675\n",
            "Station: ŽIVALSKI VRT | Epoch 36/50 | Loss: 5.6639\n",
            "Station: ŽIVALSKI VRT | Epoch 41/50 | Loss: 5.5522\n",
            "Station: ŽIVALSKI VRT | Epoch 46/50 | Loss: 5.3470\n",
            "Station: ŽIVALSKI VRT | Epoch 50/50 | Loss: 5.2409\n",
            "Station: VOKA - SLOVENČEVA | Epoch 1/50 | Loss: 15.5588\n",
            "Station: VOKA - SLOVENČEVA | Epoch 6/50 | Loss: 7.0629\n",
            "Station: VOKA - SLOVENČEVA | Epoch 11/50 | Loss: 6.6228\n",
            "Station: VOKA - SLOVENČEVA | Epoch 16/50 | Loss: 6.3008\n",
            "Station: VOKA - SLOVENČEVA | Epoch 21/50 | Loss: 5.9406\n",
            "Station: VOKA - SLOVENČEVA | Epoch 26/50 | Loss: 5.6422\n",
            "Station: VOKA - SLOVENČEVA | Epoch 31/50 | Loss: 5.4441\n",
            "Station: VOKA - SLOVENČEVA | Epoch 36/50 | Loss: 5.1877\n",
            "Station: VOKA - SLOVENČEVA | Epoch 41/50 | Loss: 5.0990\n",
            "Station: VOKA - SLOVENČEVA | Epoch 46/50 | Loss: 4.8910\n",
            "Station: VOKA - SLOVENČEVA | Epoch 50/50 | Loss: 4.7980\n",
            "Station: BTC CITY/DVORANA A | Epoch 1/50 | Loss: 20.7477\n",
            "Station: BTC CITY/DVORANA A | Epoch 6/50 | Loss: 8.0864\n",
            "Station: BTC CITY/DVORANA A | Epoch 11/50 | Loss: 7.7941\n",
            "Station: BTC CITY/DVORANA A | Epoch 16/50 | Loss: 7.3953\n",
            "Station: BTC CITY/DVORANA A | Epoch 21/50 | Loss: 7.1883\n",
            "Station: BTC CITY/DVORANA A | Epoch 26/50 | Loss: 6.9163\n",
            "Station: BTC CITY/DVORANA A | Epoch 31/50 | Loss: 6.6832\n",
            "Station: BTC CITY/DVORANA A | Epoch 36/50 | Loss: 6.4576\n",
            "Station: BTC CITY/DVORANA A | Epoch 41/50 | Loss: 6.1913\n",
            "Station: BTC CITY/DVORANA A | Epoch 46/50 | Loss: 6.1165\n",
            "Station: BTC CITY/DVORANA A | Epoch 50/50 | Loss: 5.9226\n",
            "Station: TRNOVO | Epoch 1/50 | Loss: 19.6236\n",
            "Station: TRNOVO | Epoch 6/50 | Loss: 7.0819\n",
            "Station: TRNOVO | Epoch 11/50 | Loss: 6.8923\n",
            "Station: TRNOVO | Epoch 16/50 | Loss: 6.4723\n",
            "Station: TRNOVO | Epoch 21/50 | Loss: 6.3273\n",
            "Station: TRNOVO | Epoch 26/50 | Loss: 6.0845\n",
            "Station: TRNOVO | Epoch 31/50 | Loss: 5.9123\n",
            "Station: TRNOVO | Epoch 36/50 | Loss: 5.7876\n",
            "Station: TRNOVO | Epoch 41/50 | Loss: 5.5629\n",
            "Station: TRNOVO | Epoch 46/50 | Loss: 5.4346\n",
            "Station: TRNOVO | Epoch 50/50 | Loss: 5.2858\n",
            "Station: P+R BARJE | Epoch 1/50 | Loss: 17.9818\n",
            "Station: P+R BARJE | Epoch 6/50 | Loss: 5.3865\n",
            "Station: P+R BARJE | Epoch 11/50 | Loss: 5.0938\n",
            "Station: P+R BARJE | Epoch 16/50 | Loss: 4.9227\n",
            "Station: P+R BARJE | Epoch 21/50 | Loss: 4.8661\n",
            "Station: P+R BARJE | Epoch 26/50 | Loss: 4.6812\n",
            "Station: P+R BARJE | Epoch 31/50 | Loss: 4.5733\n",
            "Station: P+R BARJE | Epoch 36/50 | Loss: 4.4490\n",
            "Station: P+R BARJE | Epoch 41/50 | Loss: 4.2795\n",
            "Station: P+R BARJE | Epoch 46/50 | Loss: 4.1333\n",
            "Station: P+R BARJE | Epoch 50/50 | Loss: 4.1391\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 1/50 | Loss: 26.5443\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 6/50 | Loss: 15.9361\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 11/50 | Loss: 14.9498\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 16/50 | Loss: 14.0960\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 21/50 | Loss: 13.4124\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 26/50 | Loss: 12.8828\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 31/50 | Loss: 12.4211\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 36/50 | Loss: 11.9771\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 41/50 | Loss: 11.4932\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 46/50 | Loss: 11.1604\n",
            "Station: ROŽNA DOLINA-ŠKRABČEVA UL. | Epoch 50/50 | Loss: 11.0032\n",
            "Station: KINO ŠIŠKA | Epoch 1/50 | Loss: 43.5783\n",
            "Station: KINO ŠIŠKA | Epoch 6/50 | Loss: 21.7368\n",
            "Station: KINO ŠIŠKA | Epoch 11/50 | Loss: 20.6491\n",
            "Station: KINO ŠIŠKA | Epoch 16/50 | Loss: 19.8406\n",
            "Station: KINO ŠIŠKA | Epoch 21/50 | Loss: 19.0423\n",
            "Station: KINO ŠIŠKA | Epoch 26/50 | Loss: 18.6744\n",
            "Station: KINO ŠIŠKA | Epoch 31/50 | Loss: 18.0205\n",
            "Station: KINO ŠIŠKA | Epoch 36/50 | Loss: 17.3435\n",
            "Station: KINO ŠIŠKA | Epoch 41/50 | Loss: 16.9914\n",
            "Station: KINO ŠIŠKA | Epoch 46/50 | Loss: 16.2834\n",
            "Station: KINO ŠIŠKA | Epoch 50/50 | Loss: 16.0096\n",
            "Station: BRODARJEV TRG | Epoch 1/50 | Loss: 23.5833\n",
            "Station: BRODARJEV TRG | Epoch 6/50 | Loss: 7.8404\n",
            "Station: BRODARJEV TRG | Epoch 11/50 | Loss: 7.4409\n",
            "Station: BRODARJEV TRG | Epoch 16/50 | Loss: 7.1514\n",
            "Station: BRODARJEV TRG | Epoch 21/50 | Loss: 6.9989\n",
            "Station: BRODARJEV TRG | Epoch 26/50 | Loss: 6.8395\n",
            "Station: BRODARJEV TRG | Epoch 31/50 | Loss: 6.6915\n",
            "Station: BRODARJEV TRG | Epoch 36/50 | Loss: 6.5032\n",
            "Station: BRODARJEV TRG | Epoch 41/50 | Loss: 6.4185\n",
            "Station: BRODARJEV TRG | Epoch 46/50 | Loss: 6.2955\n",
            "Station: BRODARJEV TRG | Epoch 50/50 | Loss: 6.0926\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 1/50 | Loss: 18.4620\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 6/50 | Loss: 7.5315\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 11/50 | Loss: 7.2318\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 16/50 | Loss: 6.9722\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 21/50 | Loss: 6.7708\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 26/50 | Loss: 6.5120\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 31/50 | Loss: 6.3970\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 36/50 | Loss: 6.2248\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 41/50 | Loss: 6.1004\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 46/50 | Loss: 5.8305\n",
            "Station: ZALOŠKA C.-GRABLOVIČEVA C. | Epoch 50/50 | Loss: 5.7564\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 1/50 | Loss: 18.4766\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 6/50 | Loss: 7.2287\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 11/50 | Loss: 6.5326\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 16/50 | Loss: 6.3652\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 21/50 | Loss: 6.0151\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 26/50 | Loss: 5.7582\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 31/50 | Loss: 5.5180\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 36/50 | Loss: 5.3742\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 41/50 | Loss: 5.2608\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 46/50 | Loss: 5.0462\n",
            "Station: DOLENJSKA C. - STRELIŠČE | Epoch 50/50 | Loss: 4.9743\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 1/50 | Loss: 21.5918\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 6/50 | Loss: 6.9693\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 11/50 | Loss: 6.5470\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 16/50 | Loss: 6.2804\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 21/50 | Loss: 6.1143\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 26/50 | Loss: 5.9024\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 31/50 | Loss: 5.7970\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 36/50 | Loss: 5.6071\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 41/50 | Loss: 5.5895\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 46/50 | Loss: 5.3259\n",
            "Station: ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA | Epoch 50/50 | Loss: 5.3377\n",
            "Station: SOSESKA NOVO BRDO | Epoch 1/50 | Loss: 17.4587\n",
            "Station: SOSESKA NOVO BRDO | Epoch 6/50 | Loss: 5.4833\n",
            "Station: SOSESKA NOVO BRDO | Epoch 11/50 | Loss: 5.2012\n",
            "Station: SOSESKA NOVO BRDO | Epoch 16/50 | Loss: 4.9760\n",
            "Station: SOSESKA NOVO BRDO | Epoch 21/50 | Loss: 4.8314\n",
            "Station: SOSESKA NOVO BRDO | Epoch 26/50 | Loss: 4.8002\n",
            "Station: SOSESKA NOVO BRDO | Epoch 31/50 | Loss: 4.7369\n",
            "Station: SOSESKA NOVO BRDO | Epoch 36/50 | Loss: 4.6511\n",
            "Station: SOSESKA NOVO BRDO | Epoch 41/50 | Loss: 4.5305\n",
            "Station: SOSESKA NOVO BRDO | Epoch 46/50 | Loss: 4.3874\n",
            "Station: SOSESKA NOVO BRDO | Epoch 50/50 | Loss: 4.3225\n",
            "Station: TRŽNICA KOSEZE | Epoch 1/50 | Loss: 25.9577\n",
            "Station: TRŽNICA KOSEZE | Epoch 6/50 | Loss: 10.2311\n",
            "Station: TRŽNICA KOSEZE | Epoch 11/50 | Loss: 9.5896\n",
            "Station: TRŽNICA KOSEZE | Epoch 16/50 | Loss: 9.0052\n",
            "Station: TRŽNICA KOSEZE | Epoch 21/50 | Loss: 8.5988\n",
            "Station: TRŽNICA KOSEZE | Epoch 26/50 | Loss: 8.4544\n",
            "Station: TRŽNICA KOSEZE | Epoch 31/50 | Loss: 8.1464\n",
            "Station: TRŽNICA KOSEZE | Epoch 36/50 | Loss: 7.9720\n",
            "Station: TRŽNICA KOSEZE | Epoch 41/50 | Loss: 7.6357\n",
            "Station: TRŽNICA KOSEZE | Epoch 46/50 | Loss: 7.4424\n",
            "Station: TRŽNICA KOSEZE | Epoch 50/50 | Loss: 7.3129\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 1/50 | Loss: 22.3238\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 6/50 | Loss: 11.5103\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 11/50 | Loss: 11.0519\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 16/50 | Loss: 10.5601\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 21/50 | Loss: 10.2668\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 26/50 | Loss: 9.9380\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 31/50 | Loss: 9.4602\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 36/50 | Loss: 9.2627\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 41/50 | Loss: 8.8953\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 46/50 | Loss: 8.4877\n",
            "Station: ALEJA - CELOVŠKA CESTA | Epoch 50/50 | Loss: 8.3161\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 1/50 | Loss: 19.2579\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 6/50 | Loss: 6.1899\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 11/50 | Loss: 5.9407\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 16/50 | Loss: 5.7240\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 21/50 | Loss: 5.6359\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 26/50 | Loss: 5.6044\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 31/50 | Loss: 5.4405\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 36/50 | Loss: 5.3175\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 41/50 | Loss: 5.2191\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 46/50 | Loss: 5.1525\n",
            "Station: MERCATOR CENTER ŠIŠKA | Epoch 50/50 | Loss: 5.0416\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 1/50 | Loss: 25.9644\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 6/50 | Loss: 17.0029\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 11/50 | Loss: 15.4758\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 16/50 | Loss: 14.4459\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 21/50 | Loss: 13.4998\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 26/50 | Loss: 12.8340\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 31/50 | Loss: 12.2867\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 36/50 | Loss: 11.8375\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 41/50 | Loss: 11.3199\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 46/50 | Loss: 11.1407\n",
            "Station: GH ŠENTPETER-NJEGOŠEVA C. | Epoch 50/50 | Loss: 10.9120\n",
            "Station: HOFER - POLJE | Epoch 1/50 | Loss: 14.3267\n",
            "Station: HOFER - POLJE | Epoch 6/50 | Loss: 3.5649\n",
            "Station: HOFER - POLJE | Epoch 11/50 | Loss: 3.3479\n",
            "Station: HOFER - POLJE | Epoch 16/50 | Loss: 3.2330\n",
            "Station: HOFER - POLJE | Epoch 21/50 | Loss: 3.1299\n",
            "Station: HOFER - POLJE | Epoch 26/50 | Loss: 3.0678\n",
            "Station: HOFER - POLJE | Epoch 31/50 | Loss: 3.0687\n",
            "Station: HOFER - POLJE | Epoch 36/50 | Loss: 2.9689\n",
            "Station: HOFER - POLJE | Epoch 41/50 | Loss: 2.9393\n",
            "Station: HOFER - POLJE | Epoch 46/50 | Loss: 2.9574\n",
            "Station: HOFER - POLJE | Epoch 50/50 | Loss: 2.8289\n",
            "Station: VIŠKO POLJE | Epoch 1/50 | Loss: 17.1709\n",
            "Station: VIŠKO POLJE | Epoch 6/50 | Loss: 6.6759\n",
            "Station: VIŠKO POLJE | Epoch 11/50 | Loss: 6.3463\n",
            "Station: VIŠKO POLJE | Epoch 16/50 | Loss: 6.1768\n",
            "Station: VIŠKO POLJE | Epoch 21/50 | Loss: 6.0556\n",
            "Station: VIŠKO POLJE | Epoch 26/50 | Loss: 5.9043\n",
            "Station: VIŠKO POLJE | Epoch 31/50 | Loss: 5.7196\n",
            "Station: VIŠKO POLJE | Epoch 36/50 | Loss: 5.5483\n",
            "Station: VIŠKO POLJE | Epoch 41/50 | Loss: 5.4144\n",
            "Station: VIŠKO POLJE | Epoch 46/50 | Loss: 5.2789\n",
            "Station: VIŠKO POLJE | Epoch 50/50 | Loss: 5.1437\n",
            "Station: BONIFACIJA | Epoch 1/50 | Loss: 21.1157\n",
            "Station: BONIFACIJA | Epoch 6/50 | Loss: 8.4645\n",
            "Station: BONIFACIJA | Epoch 11/50 | Loss: 7.9856\n",
            "Station: BONIFACIJA | Epoch 16/50 | Loss: 7.7363\n",
            "Station: BONIFACIJA | Epoch 21/50 | Loss: 7.5975\n",
            "Station: BONIFACIJA | Epoch 26/50 | Loss: 7.4214\n",
            "Station: BONIFACIJA | Epoch 31/50 | Loss: 7.2293\n",
            "Station: BONIFACIJA | Epoch 36/50 | Loss: 7.0228\n",
            "Station: BONIFACIJA | Epoch 41/50 | Loss: 6.8309\n",
            "Station: BONIFACIJA | Epoch 46/50 | Loss: 6.6323\n",
            "Station: BONIFACIJA | Epoch 50/50 | Loss: 6.5805\n",
            "Station: P + R DOLGI MOST | Epoch 1/50 | Loss: 18.2077\n",
            "Station: P + R DOLGI MOST | Epoch 6/50 | Loss: 7.8540\n",
            "Station: P + R DOLGI MOST | Epoch 11/50 | Loss: 7.6311\n",
            "Station: P + R DOLGI MOST | Epoch 16/50 | Loss: 7.4555\n",
            "Station: P + R DOLGI MOST | Epoch 21/50 | Loss: 7.1559\n",
            "Station: P + R DOLGI MOST | Epoch 26/50 | Loss: 7.0568\n",
            "Station: P + R DOLGI MOST | Epoch 31/50 | Loss: 6.7897\n",
            "Station: P + R DOLGI MOST | Epoch 36/50 | Loss: 6.6369\n",
            "Station: P + R DOLGI MOST | Epoch 41/50 | Loss: 6.5024\n",
            "Station: P + R DOLGI MOST | Epoch 46/50 | Loss: 6.3512\n",
            "Station: P + R DOLGI MOST | Epoch 50/50 | Loss: 6.2528\n",
            "Station: DRAVLJE | Epoch 1/50 | Loss: 11.8757\n",
            "Station: DRAVLJE | Epoch 6/50 | Loss: 4.9116\n",
            "Station: DRAVLJE | Epoch 11/50 | Loss: 4.6676\n",
            "Station: DRAVLJE | Epoch 16/50 | Loss: 4.5052\n",
            "Station: DRAVLJE | Epoch 21/50 | Loss: 4.3254\n",
            "Station: DRAVLJE | Epoch 26/50 | Loss: 4.2533\n",
            "Station: DRAVLJE | Epoch 31/50 | Loss: 4.1462\n",
            "Station: DRAVLJE | Epoch 36/50 | Loss: 4.0751\n",
            "Station: DRAVLJE | Epoch 41/50 | Loss: 3.9548\n",
            "Station: DRAVLJE | Epoch 46/50 | Loss: 3.8767\n",
            "Station: DRAVLJE | Epoch 50/50 | Loss: 3.8134\n",
            "Station: POLJE | Epoch 1/50 | Loss: 11.1478\n",
            "Station: POLJE | Epoch 6/50 | Loss: 2.6276\n",
            "Station: POLJE | Epoch 11/50 | Loss: 2.4210\n",
            "Station: POLJE | Epoch 16/50 | Loss: 2.3572\n",
            "Station: POLJE | Epoch 21/50 | Loss: 2.3167\n",
            "Station: POLJE | Epoch 26/50 | Loss: 2.3026\n",
            "Station: POLJE | Epoch 31/50 | Loss: 2.2621\n",
            "Station: POLJE | Epoch 36/50 | Loss: 2.2641\n",
            "Station: POLJE | Epoch 41/50 | Loss: 2.2164\n",
            "Station: POLJE | Epoch 46/50 | Loss: 2.2012\n",
            "Station: POLJE | Epoch 50/50 | Loss: 2.2102\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 1/50 | Loss: 12.6489\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 6/50 | Loss: 3.1836\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 11/50 | Loss: 2.9667\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 16/50 | Loss: 2.8634\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 21/50 | Loss: 2.8253\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 26/50 | Loss: 2.7407\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 31/50 | Loss: 2.6732\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 36/50 | Loss: 2.6127\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 41/50 | Loss: 2.5589\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 46/50 | Loss: 2.6109\n",
            "Station: SUPERNOVA LJUBLJANA - RUDNIK | Epoch 50/50 | Loss: 2.5042\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 1/50 | Loss: 14.7259\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 6/50 | Loss: 5.4025\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 11/50 | Loss: 5.0026\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 16/50 | Loss: 5.0073\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 21/50 | Loss: 4.7044\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 26/50 | Loss: 4.5711\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 31/50 | Loss: 4.3937\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 36/50 | Loss: 4.2622\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 41/50 | Loss: 4.0905\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 46/50 | Loss: 3.9673\n",
            "Station: SREDNJA FRIZERSKA ŠOLA | Epoch 50/50 | Loss: 3.8696\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 1/50 | Loss: 56.9493\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 6/50 | Loss: 25.8566\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 11/50 | Loss: 23.9233\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 16/50 | Loss: 22.3080\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 21/50 | Loss: 21.1976\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 26/50 | Loss: 20.2917\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 31/50 | Loss: 19.2881\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 36/50 | Loss: 18.7094\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 41/50 | Loss: 17.9592\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 46/50 | Loss: 17.3404\n",
            "Station: TRG OF-KOLODVORSKA UL. | Epoch 50/50 | Loss: 16.8113\n",
            "Station: TRG MDB | Epoch 1/50 | Loss: 23.3452\n",
            "Station: TRG MDB | Epoch 6/50 | Loss: 14.7592\n",
            "Station: TRG MDB | Epoch 11/50 | Loss: 13.7440\n",
            "Station: TRG MDB | Epoch 16/50 | Loss: 12.9817\n",
            "Station: TRG MDB | Epoch 21/50 | Loss: 12.2412\n",
            "Station: TRG MDB | Epoch 26/50 | Loss: 11.6705\n",
            "Station: TRG MDB | Epoch 31/50 | Loss: 11.2597\n",
            "Station: TRG MDB | Epoch 36/50 | Loss: 10.7695\n",
            "Station: TRG MDB | Epoch 41/50 | Loss: 10.3805\n",
            "Station: TRG MDB | Epoch 46/50 | Loss: 9.9962\n",
            "Station: TRG MDB | Epoch 50/50 | Loss: 9.8714\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 1/50 | Loss: 25.6363\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 6/50 | Loss: 13.5277\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 11/50 | Loss: 12.7484\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 16/50 | Loss: 12.2731\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 21/50 | Loss: 11.7440\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 26/50 | Loss: 11.4667\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 31/50 | Loss: 11.0436\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 36/50 | Loss: 10.6379\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 41/50 | Loss: 10.3422\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 46/50 | Loss: 10.0325\n",
            "Station: TRŽAŠKA C.-ILIRIJA | Epoch 50/50 | Loss: 9.7945\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 1/50 | Loss: 29.9282\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 6/50 | Loss: 18.1566\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 11/50 | Loss: 17.0588\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 16/50 | Loss: 16.4768\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 21/50 | Loss: 15.7021\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 26/50 | Loss: 14.9952\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 31/50 | Loss: 14.3760\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 36/50 | Loss: 13.8383\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 41/50 | Loss: 13.3975\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 46/50 | Loss: 12.9818\n",
            "Station: PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE | Epoch 50/50 | Loss: 12.7843\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 1/50 | Loss: 21.6511\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 6/50 | Loss: 12.6368\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 11/50 | Loss: 12.3087\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 16/50 | Loss: 11.9268\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 21/50 | Loss: 11.5079\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 26/50 | Loss: 11.2746\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 31/50 | Loss: 10.8211\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 36/50 | Loss: 10.5785\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 41/50 | Loss: 10.1332\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 46/50 | Loss: 9.9084\n",
            "Station: MERCATOR MARKET - CELOVŠKA C. 163 | Epoch 50/50 | Loss: 9.5880\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 1/50 | Loss: 23.3257\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 6/50 | Loss: 11.3808\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 11/50 | Loss: 10.5297\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 16/50 | Loss: 9.8638\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 21/50 | Loss: 9.3917\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 26/50 | Loss: 8.8715\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 31/50 | Loss: 8.4767\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 36/50 | Loss: 8.1790\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 41/50 | Loss: 7.9144\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 46/50 | Loss: 7.6619\n",
            "Station: SAVSKO NASELJE 2-LINHARTOVA CESTA | Epoch 50/50 | Loss: 7.3456\n",
            "Station: BREG | Epoch 1/50 | Loss: 22.8443\n",
            "Station: BREG | Epoch 6/50 | Loss: 12.1859\n",
            "Station: BREG | Epoch 11/50 | Loss: 11.2146\n",
            "Station: BREG | Epoch 16/50 | Loss: 10.5160\n",
            "Station: BREG | Epoch 21/50 | Loss: 10.0709\n",
            "Station: BREG | Epoch 26/50 | Loss: 9.5998\n",
            "Station: BREG | Epoch 31/50 | Loss: 9.0889\n",
            "Station: BREG | Epoch 36/50 | Loss: 8.5870\n",
            "Station: BREG | Epoch 41/50 | Loss: 8.2672\n",
            "Station: BREG | Epoch 46/50 | Loss: 7.8677\n",
            "Station: BREG | Epoch 50/50 | Loss: 7.6318\n",
            "Station: BTC CITY ATLANTIS | Epoch 1/50 | Loss: 14.3502\n",
            "Station: BTC CITY ATLANTIS | Epoch 6/50 | Loss: 4.9989\n",
            "Station: BTC CITY ATLANTIS | Epoch 11/50 | Loss: 4.6449\n",
            "Station: BTC CITY ATLANTIS | Epoch 16/50 | Loss: 4.5137\n",
            "Station: BTC CITY ATLANTIS | Epoch 21/50 | Loss: 4.2829\n",
            "Station: BTC CITY ATLANTIS | Epoch 26/50 | Loss: 4.1468\n",
            "Station: BTC CITY ATLANTIS | Epoch 31/50 | Loss: 4.0632\n",
            "Station: BTC CITY ATLANTIS | Epoch 36/50 | Loss: 3.8485\n",
            "Station: BTC CITY ATLANTIS | Epoch 41/50 | Loss: 3.8500\n",
            "Station: BTC CITY ATLANTIS | Epoch 46/50 | Loss: 3.7309\n",
            "Station: BTC CITY ATLANTIS | Epoch 50/50 | Loss: 3.6153\n",
            "Station: IKEA | Epoch 1/50 | Loss: 16.4288\n",
            "Station: IKEA | Epoch 6/50 | Loss: 5.9107\n",
            "Station: IKEA | Epoch 11/50 | Loss: 5.5819\n",
            "Station: IKEA | Epoch 16/50 | Loss: 5.2705\n",
            "Station: IKEA | Epoch 21/50 | Loss: 5.0229\n",
            "Station: IKEA | Epoch 26/50 | Loss: 4.8894\n",
            "Station: IKEA | Epoch 31/50 | Loss: 4.7496\n",
            "Station: IKEA | Epoch 36/50 | Loss: 4.5671\n",
            "Station: IKEA | Epoch 41/50 | Loss: 4.3991\n",
            "Station: IKEA | Epoch 46/50 | Loss: 4.2420\n",
            "Station: IKEA | Epoch 50/50 | Loss: 4.1637\n",
            "Station: MIKLOŠIČEV PARK | Epoch 1/50 | Loss: 18.5039\n",
            "Station: MIKLOŠIČEV PARK | Epoch 6/50 | Loss: 12.8364\n",
            "Station: MIKLOŠIČEV PARK | Epoch 11/50 | Loss: 11.8462\n",
            "Station: MIKLOŠIČEV PARK | Epoch 16/50 | Loss: 11.0440\n",
            "Station: MIKLOŠIČEV PARK | Epoch 21/50 | Loss: 10.4079\n",
            "Station: MIKLOŠIČEV PARK | Epoch 26/50 | Loss: 9.7296\n",
            "Station: MIKLOŠIČEV PARK | Epoch 31/50 | Loss: 9.2191\n",
            "Station: MIKLOŠIČEV PARK | Epoch 36/50 | Loss: 8.7753\n",
            "Station: MIKLOŠIČEV PARK | Epoch 41/50 | Loss: 8.4602\n",
            "Station: MIKLOŠIČEV PARK | Epoch 46/50 | Loss: 8.0912\n",
            "Station: MIKLOŠIČEV PARK | Epoch 50/50 | Loss: 7.9490\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 1/50 | Loss: 25.4085\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 6/50 | Loss: 10.5545\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 11/50 | Loss: 9.7980\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 16/50 | Loss: 9.3730\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 21/50 | Loss: 9.0594\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 26/50 | Loss: 8.7696\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 31/50 | Loss: 8.4576\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 36/50 | Loss: 8.1678\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 41/50 | Loss: 7.9460\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 46/50 | Loss: 7.6467\n",
            "Station: BARJANSKA C.-CENTER STAREJŠIH TRNOVO | Epoch 50/50 | Loss: 7.4747\n",
            "Station: LEK - VEROVŠKOVA | Epoch 1/50 | Loss: 16.3499\n",
            "Station: LEK - VEROVŠKOVA | Epoch 6/50 | Loss: 5.8514\n",
            "Station: LEK - VEROVŠKOVA | Epoch 11/50 | Loss: 5.2847\n",
            "Station: LEK - VEROVŠKOVA | Epoch 16/50 | Loss: 4.9801\n",
            "Station: LEK - VEROVŠKOVA | Epoch 21/50 | Loss: 4.7810\n",
            "Station: LEK - VEROVŠKOVA | Epoch 26/50 | Loss: 4.6640\n",
            "Station: LEK - VEROVŠKOVA | Epoch 31/50 | Loss: 4.5452\n",
            "Station: LEK - VEROVŠKOVA | Epoch 36/50 | Loss: 4.3286\n",
            "Station: LEK - VEROVŠKOVA | Epoch 41/50 | Loss: 4.1433\n",
            "Station: LEK - VEROVŠKOVA | Epoch 46/50 | Loss: 4.0397\n",
            "Station: LEK - VEROVŠKOVA | Epoch 50/50 | Loss: 3.9211\n",
            "Station: AMBROŽEV TRG | Epoch 1/50 | Loss: 18.4431\n",
            "Station: AMBROŽEV TRG | Epoch 6/50 | Loss: 11.1507\n",
            "Station: AMBROŽEV TRG | Epoch 11/50 | Loss: 10.3746\n",
            "Station: AMBROŽEV TRG | Epoch 16/50 | Loss: 9.8315\n",
            "Station: AMBROŽEV TRG | Epoch 21/50 | Loss: 9.1536\n",
            "Station: AMBROŽEV TRG | Epoch 26/50 | Loss: 8.6003\n",
            "Station: AMBROŽEV TRG | Epoch 31/50 | Loss: 8.0336\n",
            "Station: AMBROŽEV TRG | Epoch 36/50 | Loss: 7.6314\n",
            "Station: AMBROŽEV TRG | Epoch 41/50 | Loss: 7.3699\n",
            "Station: AMBROŽEV TRG | Epoch 46/50 | Loss: 7.0826\n",
            "Station: AMBROŽEV TRG | Epoch 50/50 | Loss: 6.9162\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 1/50 | Loss: 20.3038\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 6/50 | Loss: 10.1832\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 11/50 | Loss: 9.7844\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 16/50 | Loss: 9.3114\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 21/50 | Loss: 8.9579\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 26/50 | Loss: 8.5161\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 31/50 | Loss: 8.1431\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 36/50 | Loss: 7.8026\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 41/50 | Loss: 7.5873\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 46/50 | Loss: 7.3794\n",
            "Station: VOJKOVA - GASILSKA BRIGADA | Epoch 50/50 | Loss: 7.2121\n",
            "Station: RAKOVNIK | Epoch 1/50 | Loss: 28.0576\n",
            "Station: RAKOVNIK | Epoch 6/50 | Loss: 8.4505\n",
            "Station: RAKOVNIK | Epoch 11/50 | Loss: 7.7736\n",
            "Station: RAKOVNIK | Epoch 16/50 | Loss: 7.4736\n",
            "Station: RAKOVNIK | Epoch 21/50 | Loss: 7.2469\n",
            "Station: RAKOVNIK | Epoch 26/50 | Loss: 7.0332\n",
            "Station: RAKOVNIK | Epoch 31/50 | Loss: 6.7328\n",
            "Station: RAKOVNIK | Epoch 36/50 | Loss: 6.5931\n",
            "Station: RAKOVNIK | Epoch 41/50 | Loss: 6.3845\n",
            "Station: RAKOVNIK | Epoch 46/50 | Loss: 6.2906\n",
            "Station: RAKOVNIK | Epoch 50/50 | Loss: 6.0945\n",
            "Station: PREGLOV TRG | Epoch 1/50 | Loss: 20.6303\n",
            "Station: PREGLOV TRG | Epoch 6/50 | Loss: 6.7002\n",
            "Station: PREGLOV TRG | Epoch 11/50 | Loss: 6.4146\n",
            "Station: PREGLOV TRG | Epoch 16/50 | Loss: 6.2174\n",
            "Station: PREGLOV TRG | Epoch 21/50 | Loss: 6.0901\n",
            "Station: PREGLOV TRG | Epoch 26/50 | Loss: 5.9975\n",
            "Station: PREGLOV TRG | Epoch 31/50 | Loss: 6.0519\n",
            "Station: PREGLOV TRG | Epoch 36/50 | Loss: 5.8531\n",
            "Station: PREGLOV TRG | Epoch 41/50 | Loss: 5.7751\n",
            "Station: PREGLOV TRG | Epoch 46/50 | Loss: 5.7803\n",
            "Station: PREGLOV TRG | Epoch 50/50 | Loss: 5.6282\n",
            "Station: PLEČNIKOV STADION | Epoch 1/50 | Loss: 17.6278\n",
            "Station: PLEČNIKOV STADION | Epoch 6/50 | Loss: 8.9562\n",
            "Station: PLEČNIKOV STADION | Epoch 11/50 | Loss: 8.4160\n",
            "Station: PLEČNIKOV STADION | Epoch 16/50 | Loss: 7.9375\n",
            "Station: PLEČNIKOV STADION | Epoch 21/50 | Loss: 7.4511\n",
            "Station: PLEČNIKOV STADION | Epoch 26/50 | Loss: 7.0437\n",
            "Station: PLEČNIKOV STADION | Epoch 31/50 | Loss: 6.6953\n",
            "Station: PLEČNIKOV STADION | Epoch 36/50 | Loss: 6.4011\n",
            "Station: PLEČNIKOV STADION | Epoch 41/50 | Loss: 6.1050\n",
            "Station: PLEČNIKOV STADION | Epoch 46/50 | Loss: 5.9123\n",
            "Station: PLEČNIKOV STADION | Epoch 50/50 | Loss: 5.7748\n"
          ]
        }
      ],
      "source": [
        "models = {}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "epochs = 50\n",
        "\n",
        "for station in station_cols:\n",
        "    X = features_dict[station]\n",
        "    y = targets_dict[station]\n",
        "    dataset = BikeDataset(X, y)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    in_features = X.shape[1]\n",
        "    model = MLP(in_features=in_features, hidden=hidden_dim, out_features=pred_horizon).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        losses = []\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        if epoch % 5 == 0 or epoch == epochs-1:\n",
        "            print(f\"Station: {station} | Epoch {epoch+1}/{epochs} | Loss: {np.mean(losses):.4f}\")\n",
        "    models[station] = model.cpu()  # Save to CPU for later use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYVyLOwIXsOy"
      },
      "outputs": [],
      "source": [
        "test_pred = test_df.copy()\n",
        "test_pred[station_cols] = test_pred[station_cols].astype(str)\n",
        "i = 0\n",
        "while i < len(test_df):\n",
        "    window = test_df.iloc[i:i+history_len]\n",
        "    pred_start = i + history_len\n",
        "    if pred_start + pred_horizon > len(test_df):\n",
        "        break\n",
        "    to_pred = test_df.iloc[pred_start:pred_start+pred_horizon]\n",
        "    mask = to_pred[station_cols].isnull() | (to_pred[station_cols] == '')\n",
        "    if mask.values.any():\n",
        "        for sidx, station in enumerate(station_cols):\n",
        "            if mask[station].any():\n",
        "                own_hist = window[station].values.astype(float)\n",
        "                nn_hist = []\n",
        "                for nn in neighbors[station]:\n",
        "                    nn_hist.append(window[nn].values.astype(float))\n",
        "                nn_hist = np.concatenate(nn_hist) if nn_hist else np.zeros(0)\n",
        "                hour = pd.to_datetime(to_pred['timestamp'].iloc[0]).hour / 23.0\n",
        "                dow = pd.to_datetime(to_pred['timestamp'].iloc[0]).dayofweek / 6.0\n",
        "                f = np.concatenate([own_hist, nn_hist, [hour, dow]])[None, :]\n",
        "                # Predict using PyTorch model\n",
        "                model = models[station]\n",
        "                with torch.no_grad():\n",
        "                    pred = model(torch.tensor(f, dtype=torch.float32)).numpy().flatten()\n",
        "                for h in range(pred_horizon):\n",
        "                    if mask[station].iloc[h]:\n",
        "                        test_pred.loc[pred_start + h, station] = pred[h]\n",
        "    i += history_len + pred_horizon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHh9WXy_Xttl",
        "outputId": "b4d8da52-e46c-48c2-8728-d597df090740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved as pytorch_per_station.csv\n"
          ]
        }
      ],
      "source": [
        "rows_with_preds = test_df[station_cols].isnull() | (test_df[station_cols] == '')\n",
        "rows_with_preds = rows_with_preds.any(axis=1)\n",
        "header = pd.DataFrame([test_df.columns], columns=test_df.columns)\n",
        "final = pd.concat([header, test_pred[rows_with_preds]], ignore_index=True)\n",
        "final.to_csv(\"pytorch_per_station.csv\", index=False, header=False)\n",
        "print(\"Predictions saved as pytorch_per_station.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6DHBXOWjG6U",
        "outputId": "5cd10fd2-a757-4768-ba4c-192f207d0737"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-39-fb7755594c7d>:28: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "\n",
        "# --- PARAMETERS ---\n",
        "history_len = 48\n",
        "pred_horizon = 4\n",
        "k_neighbors = 2\n",
        "val_ratio = 0.15\n",
        "holdout_ratio = 0.15\n",
        "epochs = 80\n",
        "batch_size = 32\n",
        "hidden_dim = 64\n",
        "dropout_p = 0.3\n",
        "learning_rate = 0.001\n",
        "patience = 10\n",
        "\n",
        "# --- LOAD AND CLEAN DATA ---\n",
        "train_df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = train_df.columns[1:]\n",
        "\n",
        "for col in station_cols:\n",
        "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "train_df = train_df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- NEIGHBOR DETECTION ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:k_neighbors]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- FEATURE & TARGET GENERATION ---\n",
        "def make_features_targets(df, station_cols, neighbors, history_len, pred_horizon):\n",
        "    T = len(df)\n",
        "    features_dict = {}\n",
        "    targets_dict = {}\n",
        "    timestamps = pd.to_datetime(df['timestamp'])\n",
        "    hours = timestamps.dt.hour.values / 23.0\n",
        "    dows = timestamps.dt.dayofweek.values / 6.0\n",
        "    for station in station_cols:\n",
        "        own_vals = df[station].values.astype(float)\n",
        "        nn_arrays = [df[nn].values.astype(float) for nn in neighbors[station]]\n",
        "        all_feat = []\n",
        "        all_tgt = []\n",
        "        for i in range(history_len, T - pred_horizon + 1):\n",
        "            main_hist = own_vals[i-history_len:i]\n",
        "            nn_hist = np.concatenate([arr[i-history_len:i] for arr in nn_arrays]) if nn_arrays else np.zeros(0)\n",
        "            f = np.concatenate([main_hist, nn_hist, [hours[i], dows[i]]])\n",
        "            t = own_vals[i:i+pred_horizon]\n",
        "            all_feat.append(f)\n",
        "            all_tgt.append(t)\n",
        "        features_dict[station] = np.stack(all_feat)\n",
        "        targets_dict[station] = np.stack(all_tgt)\n",
        "    return features_dict, targets_dict\n",
        "\n",
        "features_dict, targets_dict = make_features_targets(train_df, station_cols, neighbors, history_len, pred_horizon)\n",
        "\n",
        "# --- FIXED SPLITTING FUNCTION ---\n",
        "def get_splits(n, val_ratio, holdout_ratio):\n",
        "    train_end = int(n * (1 - val_ratio - holdout_ratio))\n",
        "    val_end = int(n * (1 - holdout_ratio))\n",
        "    return slice(0, train_end), slice(train_end, val_end), slice(val_end, n)\n",
        "\n",
        "splits = {}\n",
        "for station in station_cols:\n",
        "    n = features_dict[station].shape[0]\n",
        "    train_idx, val_idx, holdout_idx = get_splits(n, val_ratio, holdout_ratio)\n",
        "    splits[station] = {\n",
        "        \"train\": train_idx,\n",
        "        \"val\": val_idx,\n",
        "        \"holdout\": holdout_idx\n",
        "    }\n",
        "\n",
        "# --- DATASET CLASS ---\n",
        "class BikeDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --- MODEL ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden=64, out_features=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, out_features)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "def train_mlp(X_train, y_train, X_val, y_val, in_features, out_features,\n",
        "              epochs=80, batch_size=32, hidden_dim=64, dropout=0.3,\n",
        "              lr=1e-3, patience=10, device='cpu', weight_decay=0.0):\n",
        "    model = MLP(in_features, hidden_dim, out_features, dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    train_set = BikeDataset(X_train, y_train)\n",
        "    val_set = BikeDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                val_loss = loss_fn(preds, yb)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "        avg_train = np.mean(train_losses)\n",
        "        avg_val = np.mean(val_losses) if val_losses else float('nan')\n",
        "        print(f\"Epoch {epoch+1:03d} | Train: {avg_train:.4f} | Val: {avg_val:.4f}\")\n",
        "        if avg_val < best_val_loss:\n",
        "            best_val_loss = avg_val\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# --- TRAINING PER STATION ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mlp_models = {}\n",
        "holdout_mse = {}\n",
        "\n",
        "# for station in station_cols:\n",
        "#     feat = features_dict[station]\n",
        "#     targ = targets_dict[station]\n",
        "#     train_idx, val_idx, holdout_idx = splits[station][\"train\"], splits[station][\"val\"], splits[station][\"holdout\"]\n",
        "#     X_train, y_train = feat[train_idx], targ[train_idx]\n",
        "#     X_val, y_val = feat[val_idx], targ[val_idx]\n",
        "#     X_hold, y_hold = feat[holdout_idx], targ[holdout_idx]\n",
        "\n",
        "#     print(f\"\\n--- Training station {station} ---\")\n",
        "#     print(\"Train samples:\", len(X_train), \"| Val samples:\", len(X_val), \"| Holdout samples:\", len(X_hold))\n",
        "\n",
        "#     model = train_mlp(X_train, y_train, X_val, y_val, in_features=X_train.shape[1], out_features=pred_horizon,\n",
        "#                       epochs=epochs, batch_size=batch_size, hidden_dim=hidden_dim, dropout=dropout_p,\n",
        "#                       lr=learning_rate, patience=patience, device=device)\n",
        "#     mlp_models[station] = model.cpu()\n",
        "\n",
        "#     # Evaluate on holdout\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         hold_pred = model(torch.from_numpy(X_hold).float())\n",
        "#         mse = ((hold_pred.numpy() - y_hold) ** 2).mean()\n",
        "#         holdout_mse[station] = mse\n",
        "#     print(f\"Holdout MSE for {station}: {mse:.4f}\")\n",
        "\n",
        "# print(\"Mean Holdout MSE across stations:\", np.mean(list(holdout_mse.values())))\n",
        "\n",
        "# Use a small fixed subset for faster iteration\n",
        "# selected_stations = [\n",
        "#     'LIDL BEŽIGRAD',\n",
        "#     'CITYPARK',\n",
        "# ]\n",
        "\n",
        "\n",
        "# for station in station_cols:\n",
        "#     if station not in selected_stations:\n",
        "#         continue  # Skip stations not in the selected subset\n",
        "\n",
        "#     feat = features_dict[station]\n",
        "#     targ = targets_dict[station]\n",
        "#     train_idx, val_idx, holdout_idx = splits[station][\"train\"], splits[station][\"val\"], splits[station][\"holdout\"]\n",
        "#     X_train, y_train = feat[train_idx], targ[train_idx]\n",
        "#     X_val, y_val = feat[val_idx], targ[val_idx]\n",
        "#     X_hold, y_hold = feat[holdout_idx], targ[holdout_idx]\n",
        "\n",
        "#     print(f\"\\n--- Training station {station} ---\")\n",
        "#     print(\"Train samples:\", len(X_train), \"| Val samples:\", len(X_val), \"| Holdout samples:\", len(X_hold))\n",
        "\n",
        "#     model = train_mlp(X_train, y_train, X_val, y_val, in_features=X_train.shape[1], out_features=pred_horizon,\n",
        "#                       epochs=epochs, batch_size=batch_size, hidden_dim=hidden_dim, dropout=dropout_p,\n",
        "#                       lr=learning_rate, patience=patience, device=device)\n",
        "#     mlp_models[station] = model.cpu()\n",
        "\n",
        "#     # Evaluate on holdout\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         hold_pred = model(torch.from_numpy(X_hold).float())\n",
        "#         mse = ((hold_pred.numpy() - y_hold) ** 2).mean()\n",
        "#         holdout_mse[station] = mse\n",
        "#     print(f\"Holdout MSE for {station}: {mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osADL8E24GgX"
      },
      "source": [
        "## Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_mD__A94H-K",
        "outputId": "80ba1f22-5c65-42a2-da5d-61d816a43a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Grid search for LIDL BEŽIGRAD ###\n",
            "\n",
            "Trying hidden=64, dropout=0.15, lr=0.0007, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 22.9553 | Val: 13.8325\n",
            "Epoch 002 | Train: 11.9611 | Val: 12.7888\n",
            "Epoch 003 | Train: 11.5383 | Val: 12.9310\n",
            "Epoch 004 | Train: 11.2091 | Val: 12.9310\n",
            "Epoch 005 | Train: 11.0979 | Val: 12.6823\n",
            "Epoch 006 | Train: 11.0463 | Val: 12.7624\n",
            "Epoch 007 | Train: 10.7517 | Val: 12.9262\n",
            "Epoch 008 | Train: 10.5677 | Val: 12.7931\n",
            "Epoch 009 | Train: 10.5151 | Val: 13.1522\n",
            "Epoch 010 | Train: 10.4811 | Val: 13.1094\n",
            "Epoch 011 | Train: 10.3638 | Val: 12.9281\n",
            "Epoch 012 | Train: 10.3393 | Val: 13.4025\n",
            "Epoch 013 | Train: 10.1763 | Val: 13.0831\n",
            "Epoch 014 | Train: 10.2087 | Val: 13.0782\n",
            "Epoch 015 | Train: 9.9839 | Val: 12.9553\n",
            "Early stopping.\n",
            "Holdout MSE: 11.7827\n",
            "\n",
            "Trying hidden=48, dropout=0.15, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 21.7702 | Val: 13.5399\n",
            "Epoch 002 | Train: 12.0815 | Val: 13.1615\n",
            "Epoch 003 | Train: 11.6832 | Val: 12.9949\n",
            "Epoch 004 | Train: 11.4535 | Val: 12.8785\n",
            "Epoch 005 | Train: 11.1603 | Val: 13.0129\n",
            "Epoch 006 | Train: 11.1006 | Val: 12.7898\n",
            "Epoch 007 | Train: 10.8952 | Val: 13.0966\n",
            "Epoch 008 | Train: 10.8684 | Val: 12.8931\n",
            "Epoch 009 | Train: 10.7737 | Val: 12.9199\n",
            "Epoch 010 | Train: 10.8135 | Val: 12.9102\n",
            "Epoch 011 | Train: 10.6545 | Val: 12.6590\n",
            "Epoch 012 | Train: 10.6172 | Val: 13.0066\n",
            "Epoch 013 | Train: 10.5453 | Val: 12.7935\n",
            "Epoch 014 | Train: 10.4893 | Val: 12.9457\n",
            "Epoch 015 | Train: 10.3596 | Val: 12.9261\n",
            "Epoch 016 | Train: 10.3668 | Val: 12.9699\n",
            "Epoch 017 | Train: 10.2983 | Val: 13.1394\n",
            "Epoch 018 | Train: 10.3057 | Val: 13.4884\n",
            "Epoch 019 | Train: 10.3231 | Val: 13.6987\n",
            "Epoch 020 | Train: 10.2374 | Val: 13.0230\n",
            "Epoch 021 | Train: 10.1467 | Val: 13.2377\n",
            "Early stopping.\n",
            "Holdout MSE: 11.7442\n",
            "\n",
            "Trying hidden=48, dropout=0.15, lr=0.0007, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 24.5804 | Val: 14.8813\n",
            "Epoch 002 | Train: 12.2423 | Val: 13.2467\n",
            "Epoch 003 | Train: 11.7989 | Val: 13.1187\n",
            "Epoch 004 | Train: 11.7665 | Val: 12.7553\n",
            "Epoch 005 | Train: 11.2728 | Val: 12.7144\n",
            "Epoch 006 | Train: 11.2001 | Val: 12.8140\n",
            "Epoch 007 | Train: 11.0913 | Val: 12.7833\n",
            "Epoch 008 | Train: 10.9802 | Val: 12.8460\n",
            "Epoch 009 | Train: 10.9235 | Val: 12.7767\n",
            "Epoch 010 | Train: 10.8009 | Val: 12.8329\n",
            "Epoch 011 | Train: 10.7292 | Val: 12.8128\n",
            "Epoch 012 | Train: 10.6719 | Val: 12.9735\n",
            "Epoch 013 | Train: 10.6150 | Val: 13.2495\n",
            "Epoch 014 | Train: 10.5550 | Val: 12.8637\n",
            "Epoch 015 | Train: 10.5513 | Val: 13.0362\n",
            "Early stopping.\n",
            "Holdout MSE: 11.7518\n",
            "\n",
            "Trying hidden=64, dropout=0.2, lr=0.0007, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 23.9683 | Val: 14.3838\n",
            "Epoch 002 | Train: 12.4387 | Val: 13.2617\n",
            "Epoch 003 | Train: 11.8588 | Val: 12.9582\n",
            "Epoch 004 | Train: 11.5982 | Val: 13.4751\n",
            "Epoch 005 | Train: 11.3502 | Val: 12.7457\n",
            "Epoch 006 | Train: 11.2055 | Val: 12.8277\n",
            "Epoch 007 | Train: 11.1316 | Val: 12.7300\n",
            "Epoch 008 | Train: 11.1772 | Val: 12.9147\n",
            "Epoch 009 | Train: 10.9176 | Val: 13.1514\n",
            "Epoch 010 | Train: 10.7917 | Val: 12.6401\n",
            "Epoch 011 | Train: 10.7880 | Val: 12.8220\n",
            "Epoch 012 | Train: 10.6795 | Val: 12.9469\n",
            "Epoch 013 | Train: 10.7121 | Val: 13.0908\n",
            "Epoch 014 | Train: 10.5862 | Val: 12.8859\n",
            "Epoch 015 | Train: 10.5260 | Val: 13.2772\n",
            "Epoch 016 | Train: 10.3787 | Val: 13.0973\n",
            "Epoch 017 | Train: 10.3799 | Val: 13.6806\n",
            "Epoch 018 | Train: 10.5088 | Val: 13.3510\n",
            "Epoch 019 | Train: 10.3994 | Val: 12.8590\n",
            "Epoch 020 | Train: 10.2137 | Val: 12.9169\n",
            "Early stopping.\n",
            "Holdout MSE: 12.0057\n",
            "\n",
            "Trying hidden=64, dropout=0.25, lr=0.0007, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 23.9250 | Val: 14.0823\n",
            "Epoch 002 | Train: 12.4897 | Val: 13.2775\n",
            "Epoch 003 | Train: 11.9346 | Val: 13.5350\n",
            "Epoch 004 | Train: 11.6324 | Val: 13.4597\n",
            "Epoch 005 | Train: 11.5796 | Val: 13.4269\n",
            "Epoch 006 | Train: 11.4751 | Val: 12.8678\n",
            "Epoch 007 | Train: 11.2885 | Val: 12.7602\n",
            "Epoch 008 | Train: 11.2440 | Val: 13.0885\n",
            "Epoch 009 | Train: 11.1521 | Val: 13.0851\n",
            "Epoch 010 | Train: 11.0702 | Val: 13.0144\n",
            "Epoch 011 | Train: 10.9642 | Val: 12.7664\n",
            "Epoch 012 | Train: 11.0587 | Val: 12.9428\n",
            "Epoch 013 | Train: 10.8637 | Val: 13.3610\n",
            "Epoch 014 | Train: 10.8771 | Val: 13.2259\n",
            "Epoch 015 | Train: 10.7111 | Val: 12.8457\n",
            "Epoch 016 | Train: 10.7186 | Val: 13.3106\n",
            "Epoch 017 | Train: 10.6793 | Val: 12.8261\n",
            "Early stopping.\n",
            "Holdout MSE: 11.7720\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.0007, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 29.4431 | Val: 17.7046\n",
            "Epoch 002 | Train: 13.6391 | Val: 13.2152\n",
            "Epoch 003 | Train: 12.8650 | Val: 13.6670\n",
            "Epoch 004 | Train: 12.6565 | Val: 13.3602\n",
            "Epoch 005 | Train: 12.3992 | Val: 12.7976\n",
            "Epoch 006 | Train: 12.3680 | Val: 12.9895\n",
            "Epoch 007 | Train: 12.1310 | Val: 12.6197\n",
            "Epoch 008 | Train: 12.1377 | Val: 12.8213\n",
            "Epoch 009 | Train: 11.9104 | Val: 13.0721\n",
            "Epoch 010 | Train: 11.9230 | Val: 12.8493\n",
            "Epoch 011 | Train: 11.8492 | Val: 12.7841\n",
            "Epoch 012 | Train: 11.7186 | Val: 12.6288\n",
            "Epoch 013 | Train: 11.8006 | Val: 12.6367\n",
            "Epoch 014 | Train: 11.6614 | Val: 13.0012\n",
            "Epoch 015 | Train: 11.6490 | Val: 12.6392\n",
            "Epoch 016 | Train: 11.6501 | Val: 12.8102\n",
            "Epoch 017 | Train: 11.6470 | Val: 13.1502\n",
            "Early stopping.\n",
            "Holdout MSE: 12.1307\n",
            "\n",
            "Trying hidden=48, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 21.7152 | Val: 14.1374\n",
            "Epoch 002 | Train: 12.3320 | Val: 13.1229\n",
            "Epoch 003 | Train: 11.9356 | Val: 12.9096\n",
            "Epoch 004 | Train: 11.6769 | Val: 13.1695\n",
            "Epoch 005 | Train: 11.5809 | Val: 13.0321\n",
            "Epoch 006 | Train: 11.3793 | Val: 13.3998\n",
            "Epoch 007 | Train: 11.3431 | Val: 12.6174\n",
            "Epoch 008 | Train: 11.2486 | Val: 13.4381\n",
            "Epoch 009 | Train: 11.1278 | Val: 13.1648\n",
            "Epoch 010 | Train: 11.1602 | Val: 12.8006\n",
            "Epoch 011 | Train: 10.9277 | Val: 13.5501\n",
            "Epoch 012 | Train: 10.9931 | Val: 12.7823\n",
            "Epoch 013 | Train: 10.7646 | Val: 12.7919\n",
            "Epoch 014 | Train: 10.8409 | Val: 13.2417\n",
            "Epoch 015 | Train: 10.8894 | Val: 13.0605\n",
            "Epoch 016 | Train: 10.6824 | Val: 12.7436\n",
            "Epoch 017 | Train: 10.7030 | Val: 12.6175\n",
            "Early stopping.\n",
            "Holdout MSE: 11.6380\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.001, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 24.4584 | Val: 14.5772\n",
            "Epoch 002 | Train: 12.8168 | Val: 13.3389\n",
            "Epoch 003 | Train: 12.2918 | Val: 13.1948\n",
            "Epoch 004 | Train: 12.0534 | Val: 13.4752\n",
            "Epoch 005 | Train: 11.9830 | Val: 12.7757\n",
            "Epoch 006 | Train: 11.8907 | Val: 13.4430\n",
            "Epoch 007 | Train: 11.6715 | Val: 13.4973\n",
            "Epoch 008 | Train: 11.4967 | Val: 12.8999\n",
            "Epoch 009 | Train: 11.3672 | Val: 12.6736\n",
            "Epoch 010 | Train: 11.3762 | Val: 12.6957\n",
            "Epoch 011 | Train: 11.3003 | Val: 13.3068\n",
            "Epoch 012 | Train: 11.4069 | Val: 12.6358\n",
            "Epoch 013 | Train: 11.2254 | Val: 12.9397\n",
            "Epoch 014 | Train: 11.2739 | Val: 12.8941\n",
            "Epoch 015 | Train: 11.1251 | Val: 13.1827\n",
            "Epoch 016 | Train: 11.0805 | Val: 12.8345\n",
            "Epoch 017 | Train: 11.1164 | Val: 12.5090\n",
            "Epoch 018 | Train: 10.9653 | Val: 12.6243\n",
            "Epoch 019 | Train: 11.0186 | Val: 12.6894\n",
            "Epoch 020 | Train: 11.0074 | Val: 12.6844\n",
            "Epoch 021 | Train: 10.9050 | Val: 12.7861\n",
            "Epoch 022 | Train: 10.8466 | Val: 12.8790\n",
            "Epoch 023 | Train: 10.8935 | Val: 12.8585\n",
            "Epoch 024 | Train: 10.8708 | Val: 13.0612\n",
            "Epoch 025 | Train: 10.8883 | Val: 12.6023\n",
            "Epoch 026 | Train: 10.6823 | Val: 12.8372\n",
            "Epoch 027 | Train: 10.8095 | Val: 12.8642\n",
            "Early stopping.\n",
            "Holdout MSE: 11.7397\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.0007, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 30.9461 | Val: 14.9177\n",
            "Epoch 002 | Train: 13.9664 | Val: 13.8766\n",
            "Epoch 003 | Train: 13.0478 | Val: 12.8937\n",
            "Epoch 004 | Train: 12.6153 | Val: 13.0880\n",
            "Epoch 005 | Train: 12.4719 | Val: 12.5749\n",
            "Epoch 006 | Train: 12.2613 | Val: 12.8036\n",
            "Epoch 007 | Train: 12.1314 | Val: 12.4777\n",
            "Epoch 008 | Train: 12.0789 | Val: 12.3424\n",
            "Epoch 009 | Train: 12.0627 | Val: 13.0933\n",
            "Epoch 010 | Train: 11.9759 | Val: 12.2884\n",
            "Epoch 011 | Train: 11.9045 | Val: 12.5561\n",
            "Epoch 012 | Train: 11.8293 | Val: 12.9779\n",
            "Epoch 013 | Train: 11.7706 | Val: 12.7810\n",
            "Epoch 014 | Train: 11.5761 | Val: 12.7933\n",
            "Epoch 015 | Train: 11.6615 | Val: 12.4298\n",
            "Epoch 016 | Train: 11.6458 | Val: 12.4153\n",
            "Epoch 017 | Train: 11.5814 | Val: 12.4474\n",
            "Epoch 018 | Train: 11.4808 | Val: 12.3447\n",
            "Epoch 019 | Train: 11.5439 | Val: 12.7072\n",
            "Epoch 020 | Train: 11.4993 | Val: 12.5639\n",
            "Early stopping.\n",
            "Holdout MSE: 11.5403\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 26.1867 | Val: 13.9165\n",
            "Epoch 002 | Train: 13.2831 | Val: 13.0802\n",
            "Epoch 003 | Train: 12.7219 | Val: 13.5153\n",
            "Epoch 004 | Train: 12.4746 | Val: 12.7077\n",
            "Epoch 005 | Train: 12.3241 | Val: 13.0192\n",
            "Epoch 006 | Train: 12.0885 | Val: 13.5284\n",
            "Epoch 007 | Train: 11.8891 | Val: 12.7384\n",
            "Epoch 008 | Train: 11.9289 | Val: 12.9143\n",
            "Epoch 009 | Train: 11.8817 | Val: 12.8283\n",
            "Epoch 010 | Train: 11.8093 | Val: 12.5331\n",
            "Epoch 011 | Train: 11.6703 | Val: 12.8140\n",
            "Epoch 012 | Train: 11.5434 | Val: 12.6298\n",
            "Epoch 013 | Train: 11.7531 | Val: 12.9587\n",
            "Epoch 014 | Train: 11.4326 | Val: 12.7691\n",
            "Epoch 015 | Train: 11.6127 | Val: 12.8035\n",
            "Epoch 016 | Train: 11.5524 | Val: 12.4951\n",
            "Epoch 017 | Train: 11.4448 | Val: 13.1399\n",
            "Epoch 018 | Train: 11.4951 | Val: 12.8281\n",
            "Epoch 019 | Train: 11.5977 | Val: 12.7058\n",
            "Epoch 020 | Train: 11.3170 | Val: 12.4535\n",
            "Epoch 021 | Train: 11.4453 | Val: 12.5958\n",
            "Epoch 022 | Train: 11.2610 | Val: 12.7078\n",
            "Epoch 023 | Train: 11.2110 | Val: 13.1345\n",
            "Epoch 024 | Train: 11.1990 | Val: 12.7012\n",
            "Epoch 025 | Train: 11.1447 | Val: 12.6131\n",
            "Epoch 026 | Train: 11.2931 | Val: 12.8576\n",
            "Epoch 027 | Train: 11.2563 | Val: 12.5291\n",
            "Epoch 028 | Train: 11.1752 | Val: 12.6987\n",
            "Epoch 029 | Train: 11.0860 | Val: 12.7100\n",
            "Epoch 030 | Train: 11.3631 | Val: 12.5844\n",
            "Early stopping.\n",
            "Holdout MSE: 11.4380\n",
            "\n",
            "Trying hidden=64, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 21.0490 | Val: 13.8042\n",
            "Epoch 002 | Train: 12.1895 | Val: 13.0754\n",
            "Epoch 003 | Train: 11.8116 | Val: 13.4756\n",
            "Epoch 004 | Train: 11.5996 | Val: 13.4204\n",
            "Epoch 005 | Train: 11.1830 | Val: 13.4421\n",
            "Epoch 006 | Train: 11.2686 | Val: 13.2513\n",
            "Epoch 007 | Train: 11.0472 | Val: 12.9427\n",
            "Epoch 008 | Train: 10.9476 | Val: 13.0736\n",
            "Epoch 009 | Train: 10.9139 | Val: 12.9700\n",
            "Epoch 010 | Train: 10.9057 | Val: 12.9788\n",
            "Epoch 011 | Train: 10.6938 | Val: 13.0446\n",
            "Epoch 012 | Train: 10.7300 | Val: 12.9235\n",
            "Epoch 013 | Train: 10.6720 | Val: 13.0736\n",
            "Epoch 014 | Train: 10.4575 | Val: 13.4394\n",
            "Epoch 015 | Train: 10.3945 | Val: 12.8747\n",
            "Epoch 016 | Train: 10.2046 | Val: 13.4049\n",
            "Epoch 017 | Train: 10.3584 | Val: 12.9757\n",
            "Epoch 018 | Train: 10.3699 | Val: 12.8447\n",
            "Epoch 019 | Train: 10.1840 | Val: 13.1278\n",
            "Epoch 020 | Train: 10.2423 | Val: 13.1963\n",
            "Epoch 021 | Train: 10.1206 | Val: 13.4692\n",
            "Epoch 022 | Train: 10.0428 | Val: 13.0794\n",
            "Epoch 023 | Train: 9.9603 | Val: 13.3677\n",
            "Epoch 024 | Train: 9.9857 | Val: 13.2130\n",
            "Epoch 025 | Train: 9.8312 | Val: 13.2699\n",
            "Epoch 026 | Train: 9.9151 | Val: 13.2896\n",
            "Epoch 027 | Train: 10.0002 | Val: 13.3001\n",
            "Epoch 028 | Train: 9.9038 | Val: 13.2594\n",
            "Early stopping.\n",
            "Holdout MSE: 11.9757\n",
            "\n",
            "Trying hidden=32, dropout=0.15, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 26.1863 | Val: 14.0812\n",
            "Epoch 002 | Train: 12.4624 | Val: 13.0784\n",
            "Epoch 003 | Train: 12.0026 | Val: 13.7470\n",
            "Epoch 004 | Train: 11.8405 | Val: 12.9049\n",
            "Epoch 005 | Train: 11.5358 | Val: 12.8445\n",
            "Epoch 006 | Train: 11.4024 | Val: 13.0772\n",
            "Epoch 007 | Train: 11.3897 | Val: 13.0937\n",
            "Epoch 008 | Train: 11.3224 | Val: 12.6820\n",
            "Epoch 009 | Train: 11.2011 | Val: 13.0030\n",
            "Epoch 010 | Train: 11.1934 | Val: 13.0188\n",
            "Epoch 011 | Train: 11.1534 | Val: 12.8071\n",
            "Epoch 012 | Train: 11.0376 | Val: 12.8523\n",
            "Epoch 013 | Train: 11.0221 | Val: 12.8992\n",
            "Epoch 014 | Train: 10.9860 | Val: 12.7094\n",
            "Epoch 015 | Train: 10.7994 | Val: 12.7939\n",
            "Epoch 016 | Train: 10.8561 | Val: 12.8472\n",
            "Epoch 017 | Train: 10.8601 | Val: 12.9282\n",
            "Epoch 018 | Train: 10.8522 | Val: 12.6471\n",
            "Epoch 019 | Train: 10.8088 | Val: 12.6458\n",
            "Epoch 020 | Train: 10.7009 | Val: 13.0243\n",
            "Epoch 021 | Train: 10.7411 | Val: 12.7570\n",
            "Epoch 022 | Train: 10.6864 | Val: 13.0463\n",
            "Epoch 023 | Train: 10.6544 | Val: 12.6844\n",
            "Epoch 024 | Train: 10.7080 | Val: 12.8846\n",
            "Epoch 025 | Train: 10.6390 | Val: 12.9456\n",
            "Epoch 026 | Train: 10.5218 | Val: 13.0560\n",
            "Epoch 027 | Train: 10.5077 | Val: 12.8745\n",
            "Epoch 028 | Train: 10.5268 | Val: 12.8664\n",
            "Epoch 029 | Train: 10.5219 | Val: 12.9541\n",
            "Early stopping.\n",
            "Holdout MSE: 11.9557\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.001, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 25.1404 | Val: 14.1637\n",
            "Epoch 002 | Train: 13.4688 | Val: 13.5445\n",
            "Epoch 003 | Train: 12.8653 | Val: 13.1779\n",
            "Epoch 004 | Train: 12.6999 | Val: 13.0727\n",
            "Epoch 005 | Train: 12.3425 | Val: 12.9467\n",
            "Epoch 006 | Train: 12.3624 | Val: 13.4565\n",
            "Epoch 007 | Train: 12.1407 | Val: 12.9100\n",
            "Epoch 008 | Train: 12.0548 | Val: 12.6343\n",
            "Epoch 009 | Train: 11.8242 | Val: 13.0398\n",
            "Epoch 010 | Train: 11.7202 | Val: 12.8369\n",
            "Epoch 011 | Train: 11.7137 | Val: 13.0942\n",
            "Epoch 012 | Train: 11.7161 | Val: 12.9769\n",
            "Epoch 013 | Train: 11.6537 | Val: 12.7791\n",
            "Epoch 014 | Train: 11.5980 | Val: 12.8357\n",
            "Epoch 015 | Train: 11.4942 | Val: 13.5314\n",
            "Epoch 016 | Train: 11.6715 | Val: 13.3468\n",
            "Epoch 017 | Train: 11.5345 | Val: 13.0078\n",
            "Epoch 018 | Train: 11.4386 | Val: 12.8383\n",
            "Early stopping.\n",
            "Holdout MSE: 11.4850\n",
            "\n",
            "Trying hidden=64, dropout=0.2, lr=0.0007, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 23.5842 | Val: 15.1781\n",
            "Epoch 002 | Train: 12.4524 | Val: 13.3521\n",
            "Epoch 003 | Train: 11.7667 | Val: 12.9222\n",
            "Epoch 004 | Train: 11.5377 | Val: 12.9604\n",
            "Epoch 005 | Train: 11.2819 | Val: 13.0057\n",
            "Epoch 006 | Train: 11.0938 | Val: 12.9648\n",
            "Epoch 007 | Train: 11.2262 | Val: 13.1415\n",
            "Epoch 008 | Train: 11.0299 | Val: 12.8858\n",
            "Epoch 009 | Train: 10.7868 | Val: 12.7020\n",
            "Epoch 010 | Train: 10.7863 | Val: 12.6899\n",
            "Epoch 011 | Train: 10.7462 | Val: 13.0738\n",
            "Epoch 012 | Train: 10.6366 | Val: 12.8384\n",
            "Epoch 013 | Train: 10.5063 | Val: 12.9289\n",
            "Epoch 014 | Train: 10.5517 | Val: 13.1106\n",
            "Epoch 015 | Train: 10.3970 | Val: 13.1292\n",
            "Epoch 016 | Train: 10.3532 | Val: 13.0398\n",
            "Epoch 017 | Train: 10.3315 | Val: 13.1218\n",
            "Epoch 018 | Train: 10.2499 | Val: 13.6167\n",
            "Epoch 019 | Train: 10.3451 | Val: 12.8199\n",
            "Epoch 020 | Train: 10.1323 | Val: 12.8438\n",
            "Early stopping.\n",
            "Holdout MSE: 11.9175\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 23.3361 | Val: 14.0126\n",
            "Epoch 002 | Train: 13.1062 | Val: 13.3257\n",
            "Epoch 003 | Train: 12.5060 | Val: 13.9481\n",
            "Epoch 004 | Train: 12.1938 | Val: 13.3768\n",
            "Epoch 005 | Train: 11.8913 | Val: 12.7532\n",
            "Epoch 006 | Train: 11.9086 | Val: 13.0463\n",
            "Epoch 007 | Train: 11.7165 | Val: 12.8489\n",
            "Epoch 008 | Train: 11.6920 | Val: 12.8348\n",
            "Epoch 009 | Train: 11.4999 | Val: 13.3390\n",
            "Epoch 010 | Train: 11.4866 | Val: 12.7526\n",
            "Epoch 011 | Train: 11.3594 | Val: 12.8198\n",
            "Epoch 012 | Train: 11.3918 | Val: 12.5186\n",
            "Epoch 013 | Train: 11.3717 | Val: 12.7452\n",
            "Epoch 014 | Train: 11.2922 | Val: 12.6542\n",
            "Epoch 015 | Train: 11.2162 | Val: 12.8235\n",
            "Epoch 016 | Train: 11.2512 | Val: 12.8100\n",
            "Epoch 017 | Train: 11.0914 | Val: 13.0720\n",
            "Epoch 018 | Train: 11.1695 | Val: 12.8514\n",
            "Epoch 019 | Train: 11.1256 | Val: 12.8215\n",
            "Epoch 020 | Train: 11.2795 | Val: 12.6415\n",
            "Epoch 021 | Train: 11.0482 | Val: 12.6854\n",
            "Epoch 022 | Train: 11.0169 | Val: 13.2129\n",
            "Early stopping.\n",
            "Holdout MSE: 12.4089\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.0007, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 27.5609 | Val: 16.3091\n",
            "Epoch 002 | Train: 13.0429 | Val: 13.1831\n",
            "Epoch 003 | Train: 12.6225 | Val: 13.0372\n",
            "Epoch 004 | Train: 12.3122 | Val: 13.0738\n",
            "Epoch 005 | Train: 12.0672 | Val: 13.1738\n",
            "Epoch 006 | Train: 11.7883 | Val: 13.3981\n",
            "Epoch 007 | Train: 11.8437 | Val: 12.8348\n",
            "Epoch 008 | Train: 11.7255 | Val: 12.6990\n",
            "Epoch 009 | Train: 11.5192 | Val: 12.7618\n",
            "Epoch 010 | Train: 11.5635 | Val: 12.9445\n",
            "Epoch 011 | Train: 11.5471 | Val: 12.7502\n",
            "Epoch 012 | Train: 11.5570 | Val: 12.7585\n",
            "Epoch 013 | Train: 11.3628 | Val: 12.5932\n",
            "Epoch 014 | Train: 11.3504 | Val: 13.4794\n",
            "Epoch 015 | Train: 11.3869 | Val: 12.7096\n",
            "Epoch 016 | Train: 11.2704 | Val: 12.9953\n",
            "Epoch 017 | Train: 11.2877 | Val: 12.5429\n",
            "Epoch 018 | Train: 11.2343 | Val: 12.4975\n",
            "Epoch 019 | Train: 11.1345 | Val: 12.9126\n",
            "Epoch 020 | Train: 11.1765 | Val: 12.7583\n",
            "Epoch 021 | Train: 10.9788 | Val: 12.8699\n",
            "Epoch 022 | Train: 11.1811 | Val: 12.8408\n",
            "Epoch 023 | Train: 11.0188 | Val: 12.6532\n",
            "Epoch 024 | Train: 11.0034 | Val: 12.6222\n",
            "Epoch 025 | Train: 10.8061 | Val: 12.7885\n",
            "Epoch 026 | Train: 10.8900 | Val: 12.5439\n",
            "Epoch 027 | Train: 10.9722 | Val: 12.8137\n",
            "Epoch 028 | Train: 10.8252 | Val: 12.6713\n",
            "Early stopping.\n",
            "Holdout MSE: 11.5473\n",
            "\n",
            "Trying hidden=48, dropout=0.15, lr=0.0007, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 26.2594 | Val: 13.9353\n",
            "Epoch 002 | Train: 12.2723 | Val: 12.7684\n",
            "Epoch 003 | Train: 11.7561 | Val: 12.6127\n",
            "Epoch 004 | Train: 11.5393 | Val: 12.9088\n",
            "Epoch 005 | Train: 11.3336 | Val: 12.9062\n",
            "Epoch 006 | Train: 11.2043 | Val: 12.5861\n",
            "Epoch 007 | Train: 11.1959 | Val: 12.6707\n",
            "Epoch 008 | Train: 11.0015 | Val: 12.8450\n",
            "Epoch 009 | Train: 10.8458 | Val: 12.5849\n",
            "Epoch 010 | Train: 10.9067 | Val: 13.0694\n",
            "Epoch 011 | Train: 10.7767 | Val: 13.4349\n",
            "Epoch 012 | Train: 10.6372 | Val: 12.9903\n",
            "Epoch 013 | Train: 10.6455 | Val: 12.8774\n",
            "Epoch 014 | Train: 10.5571 | Val: 12.8765\n",
            "Epoch 015 | Train: 10.5659 | Val: 12.7384\n",
            "Epoch 016 | Train: 10.4211 | Val: 12.7705\n",
            "Epoch 017 | Train: 10.5016 | Val: 13.0135\n",
            "Epoch 018 | Train: 10.4251 | Val: 12.8647\n",
            "Epoch 019 | Train: 10.4265 | Val: 13.1486\n",
            "Early stopping.\n",
            "Holdout MSE: 11.9889\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.001, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 25.5917 | Val: 13.8111\n",
            "Epoch 002 | Train: 13.3493 | Val: 13.2409\n",
            "Epoch 003 | Train: 12.8058 | Val: 13.7009\n",
            "Epoch 004 | Train: 12.4249 | Val: 12.9653\n",
            "Epoch 005 | Train: 12.3672 | Val: 12.8774\n",
            "Epoch 006 | Train: 12.0729 | Val: 14.1455\n",
            "Epoch 007 | Train: 12.0802 | Val: 12.8604\n",
            "Epoch 008 | Train: 12.0226 | Val: 12.9401\n",
            "Epoch 009 | Train: 11.7718 | Val: 12.6252\n",
            "Epoch 010 | Train: 11.8347 | Val: 13.1075\n",
            "Epoch 011 | Train: 11.7446 | Val: 12.9637\n",
            "Epoch 012 | Train: 11.5728 | Val: 12.8724\n",
            "Epoch 013 | Train: 11.5606 | Val: 13.2911\n",
            "Epoch 014 | Train: 11.5038 | Val: 12.5043\n",
            "Epoch 015 | Train: 11.5097 | Val: 12.3087\n",
            "Epoch 016 | Train: 11.5906 | Val: 12.5917\n",
            "Epoch 017 | Train: 11.4755 | Val: 12.7279\n",
            "Epoch 018 | Train: 11.3484 | Val: 13.1237\n",
            "Epoch 019 | Train: 11.4246 | Val: 12.5253\n",
            "Epoch 020 | Train: 11.3828 | Val: 12.7046\n",
            "Epoch 021 | Train: 11.2667 | Val: 12.5284\n",
            "Epoch 022 | Train: 11.3418 | Val: 12.9144\n",
            "Epoch 023 | Train: 11.3673 | Val: 12.4287\n",
            "Epoch 024 | Train: 11.1916 | Val: 12.3899\n",
            "Epoch 025 | Train: 11.2212 | Val: 12.7240\n",
            "Early stopping.\n",
            "Holdout MSE: 11.7939\n",
            "\n",
            "Trying hidden=32, dropout=0.15, lr=0.001, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 24.9577 | Val: 13.7644\n",
            "Epoch 002 | Train: 12.3826 | Val: 13.1888\n",
            "Epoch 003 | Train: 12.1327 | Val: 12.6737\n",
            "Epoch 004 | Train: 11.7711 | Val: 12.7818\n",
            "Epoch 005 | Train: 11.6907 | Val: 12.9023\n",
            "Epoch 006 | Train: 11.6014 | Val: 13.2785\n",
            "Epoch 007 | Train: 11.4558 | Val: 13.0583\n",
            "Epoch 008 | Train: 11.4635 | Val: 13.0787\n",
            "Epoch 009 | Train: 11.3227 | Val: 12.9156\n",
            "Epoch 010 | Train: 11.2978 | Val: 13.0447\n",
            "Epoch 011 | Train: 11.0999 | Val: 12.6601\n",
            "Epoch 012 | Train: 11.1111 | Val: 12.5971\n",
            "Epoch 013 | Train: 11.0816 | Val: 12.5569\n",
            "Epoch 014 | Train: 10.9166 | Val: 13.3093\n",
            "Epoch 015 | Train: 10.9572 | Val: 12.8381\n",
            "Epoch 016 | Train: 10.8142 | Val: 13.0693\n",
            "Epoch 017 | Train: 10.8230 | Val: 13.0199\n",
            "Epoch 018 | Train: 10.7693 | Val: 12.8671\n",
            "Epoch 019 | Train: 10.7494 | Val: 13.2248\n",
            "Epoch 020 | Train: 10.6345 | Val: 12.8824\n",
            "Epoch 021 | Train: 10.6738 | Val: 13.0298\n",
            "Epoch 022 | Train: 10.6345 | Val: 13.0438\n",
            "Epoch 023 | Train: 10.6124 | Val: 12.6705\n",
            "Early stopping.\n",
            "Holdout MSE: 11.4812\n",
            "\n",
            "Trying hidden=48, dropout=0.2, lr=0.001, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 21.8804 | Val: 14.3886\n",
            "Epoch 002 | Train: 12.4509 | Val: 13.3992\n",
            "Epoch 003 | Train: 12.0148 | Val: 13.4763\n",
            "Epoch 004 | Train: 11.7994 | Val: 12.6378\n",
            "Epoch 005 | Train: 11.6708 | Val: 13.1741\n",
            "Epoch 006 | Train: 11.5430 | Val: 12.8951\n",
            "Epoch 007 | Train: 11.3504 | Val: 13.0697\n",
            "Epoch 008 | Train: 11.3354 | Val: 12.7472\n",
            "Epoch 009 | Train: 11.1370 | Val: 12.7015\n",
            "Epoch 010 | Train: 11.1364 | Val: 13.1456\n",
            "Epoch 011 | Train: 11.1723 | Val: 12.6914\n",
            "Epoch 012 | Train: 10.8448 | Val: 12.9467\n",
            "Epoch 013 | Train: 10.8971 | Val: 12.9227\n",
            "Epoch 014 | Train: 10.9315 | Val: 13.0274\n",
            "Early stopping.\n",
            "Holdout MSE: 11.8641\n",
            "\n",
            "✅ Best config for LIDL BEŽIGRAD: {'hidden_dim': 32, 'dropout': 0.25, 'learning_rate': 0.001, 'weight_decay': 1e-05, 'batch_size': 32}\n",
            "📉 Best holdout MSE: 11.4380\n",
            "\n",
            "### Grid search for CITYPARK ###\n",
            "\n",
            "Trying hidden=64, dropout=0.15, lr=0.0007, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 36.8420 | Val: 10.4200\n",
            "Epoch 002 | Train: 10.3842 | Val: 9.6667\n",
            "Epoch 003 | Train: 9.6454 | Val: 9.5755\n",
            "Epoch 004 | Train: 9.5263 | Val: 9.5776\n",
            "Epoch 005 | Train: 9.3565 | Val: 9.4722\n",
            "Epoch 006 | Train: 9.0781 | Val: 9.3714\n",
            "Epoch 007 | Train: 8.8493 | Val: 9.3293\n",
            "Epoch 008 | Train: 8.7428 | Val: 9.5488\n",
            "Epoch 009 | Train: 8.7314 | Val: 9.3733\n",
            "Epoch 010 | Train: 8.6664 | Val: 9.3837\n",
            "Epoch 011 | Train: 8.7283 | Val: 9.8114\n",
            "Epoch 012 | Train: 8.5829 | Val: 9.8617\n",
            "Epoch 013 | Train: 8.4513 | Val: 9.9226\n",
            "Epoch 014 | Train: 8.4296 | Val: 9.3300\n",
            "Epoch 015 | Train: 8.4439 | Val: 9.3896\n",
            "Epoch 016 | Train: 8.1866 | Val: 9.3261\n",
            "Epoch 017 | Train: 8.2955 | Val: 9.3894\n",
            "Epoch 018 | Train: 8.2934 | Val: 9.6252\n",
            "Epoch 019 | Train: 8.2300 | Val: 9.3570\n",
            "Epoch 020 | Train: 8.0877 | Val: 9.2781\n",
            "Epoch 021 | Train: 8.1173 | Val: 9.3146\n",
            "Epoch 022 | Train: 8.1712 | Val: 9.7515\n",
            "Epoch 023 | Train: 8.0878 | Val: 9.2191\n",
            "Epoch 024 | Train: 8.0492 | Val: 9.5793\n",
            "Epoch 025 | Train: 8.0299 | Val: 9.4196\n",
            "Epoch 026 | Train: 7.9392 | Val: 9.1985\n",
            "Epoch 027 | Train: 7.9058 | Val: 9.4301\n",
            "Epoch 028 | Train: 7.8684 | Val: 9.4797\n",
            "Epoch 029 | Train: 7.7902 | Val: 9.3370\n",
            "Epoch 030 | Train: 7.8973 | Val: 9.6204\n",
            "Epoch 031 | Train: 7.7015 | Val: 9.5392\n",
            "Epoch 032 | Train: 7.8534 | Val: 9.3156\n",
            "Epoch 033 | Train: 7.8223 | Val: 9.7753\n",
            "Epoch 034 | Train: 7.8515 | Val: 9.4856\n",
            "Epoch 035 | Train: 7.6836 | Val: 9.7332\n",
            "Epoch 036 | Train: 7.7390 | Val: 9.3826\n",
            "Early stopping.\n",
            "Holdout MSE: 8.0725\n",
            "\n",
            "Trying hidden=48, dropout=0.15, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 33.7985 | Val: 10.6805\n",
            "Epoch 002 | Train: 10.4633 | Val: 10.1046\n",
            "Epoch 003 | Train: 9.8199 | Val: 10.2464\n",
            "Epoch 004 | Train: 9.6934 | Val: 9.8237\n",
            "Epoch 005 | Train: 9.4978 | Val: 9.3059\n",
            "Epoch 006 | Train: 9.2113 | Val: 10.7846\n",
            "Epoch 007 | Train: 9.1188 | Val: 9.8632\n",
            "Epoch 008 | Train: 9.0411 | Val: 9.7842\n",
            "Epoch 009 | Train: 8.9019 | Val: 9.7615\n",
            "Epoch 010 | Train: 8.9905 | Val: 9.2966\n",
            "Epoch 011 | Train: 8.7849 | Val: 9.4094\n",
            "Epoch 012 | Train: 8.7596 | Val: 9.4472\n",
            "Epoch 013 | Train: 8.6237 | Val: 9.2720\n",
            "Epoch 014 | Train: 8.6133 | Val: 9.2321\n",
            "Epoch 015 | Train: 8.6374 | Val: 9.6035\n",
            "Epoch 016 | Train: 8.6078 | Val: 9.6955\n",
            "Epoch 017 | Train: 8.4588 | Val: 9.3886\n",
            "Epoch 018 | Train: 8.5227 | Val: 9.4481\n",
            "Epoch 019 | Train: 8.4025 | Val: 9.8465\n",
            "Epoch 020 | Train: 8.4550 | Val: 9.5264\n",
            "Epoch 021 | Train: 8.4191 | Val: 9.9121\n",
            "Epoch 022 | Train: 8.3903 | Val: 9.3661\n",
            "Epoch 023 | Train: 8.3474 | Val: 9.5843\n",
            "Epoch 024 | Train: 8.3616 | Val: 9.3461\n",
            "Early stopping.\n",
            "Holdout MSE: 7.5221\n",
            "\n",
            "Trying hidden=48, dropout=0.15, lr=0.0007, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 42.2496 | Val: 12.1053\n",
            "Epoch 002 | Train: 10.6838 | Val: 10.7069\n",
            "Epoch 003 | Train: 9.9494 | Val: 10.4117\n",
            "Epoch 004 | Train: 9.6989 | Val: 9.8030\n",
            "Epoch 005 | Train: 9.4684 | Val: 9.2160\n",
            "Epoch 006 | Train: 9.5114 | Val: 9.5834\n",
            "Epoch 007 | Train: 9.2989 | Val: 9.9478\n",
            "Epoch 008 | Train: 9.1990 | Val: 9.7042\n",
            "Epoch 009 | Train: 9.2146 | Val: 9.5234\n",
            "Epoch 010 | Train: 9.1670 | Val: 9.7381\n",
            "Epoch 011 | Train: 9.0182 | Val: 9.6825\n",
            "Epoch 012 | Train: 8.9865 | Val: 9.4748\n",
            "Epoch 013 | Train: 8.8715 | Val: 9.9906\n",
            "Epoch 014 | Train: 8.8217 | Val: 9.3741\n",
            "Epoch 015 | Train: 8.6482 | Val: 9.2615\n",
            "Early stopping.\n",
            "Holdout MSE: 7.4017\n",
            "\n",
            "Trying hidden=64, dropout=0.2, lr=0.0007, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 34.6543 | Val: 10.6296\n",
            "Epoch 002 | Train: 10.4819 | Val: 10.0811\n",
            "Epoch 003 | Train: 10.0478 | Val: 9.5251\n",
            "Epoch 004 | Train: 9.8687 | Val: 9.4643\n",
            "Epoch 005 | Train: 9.6795 | Val: 9.7920\n",
            "Epoch 006 | Train: 9.4369 | Val: 9.7291\n",
            "Epoch 007 | Train: 9.3115 | Val: 9.3908\n",
            "Epoch 008 | Train: 9.3405 | Val: 9.4486\n",
            "Epoch 009 | Train: 9.0939 | Val: 9.9461\n",
            "Epoch 010 | Train: 9.0976 | Val: 9.5250\n",
            "Epoch 011 | Train: 9.1361 | Val: 9.6200\n",
            "Epoch 012 | Train: 8.9159 | Val: 9.9311\n",
            "Epoch 013 | Train: 8.9357 | Val: 9.6117\n",
            "Epoch 014 | Train: 8.8239 | Val: 9.2050\n",
            "Epoch 015 | Train: 8.6609 | Val: 9.5031\n",
            "Epoch 016 | Train: 8.6160 | Val: 9.3827\n",
            "Epoch 017 | Train: 8.6705 | Val: 9.4620\n",
            "Epoch 018 | Train: 8.4942 | Val: 9.3744\n",
            "Epoch 019 | Train: 8.4951 | Val: 9.5605\n",
            "Epoch 020 | Train: 8.6170 | Val: 9.4337\n",
            "Epoch 021 | Train: 8.4947 | Val: 9.3957\n",
            "Epoch 022 | Train: 8.5089 | Val: 9.2846\n",
            "Epoch 023 | Train: 8.5332 | Val: 9.3895\n",
            "Epoch 024 | Train: 8.4191 | Val: 9.4006\n",
            "Early stopping.\n",
            "Holdout MSE: 8.3573\n",
            "\n",
            "Trying hidden=64, dropout=0.25, lr=0.0007, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 38.2143 | Val: 11.8320\n",
            "Epoch 002 | Train: 11.2054 | Val: 9.9663\n",
            "Epoch 003 | Train: 10.5738 | Val: 9.7654\n",
            "Epoch 004 | Train: 10.0465 | Val: 10.4786\n",
            "Epoch 005 | Train: 9.8909 | Val: 10.1752\n",
            "Epoch 006 | Train: 9.7185 | Val: 9.5474\n",
            "Epoch 007 | Train: 9.8049 | Val: 9.9006\n",
            "Epoch 008 | Train: 9.6554 | Val: 9.5928\n",
            "Epoch 009 | Train: 9.4707 | Val: 9.1855\n",
            "Epoch 010 | Train: 9.4782 | Val: 9.4095\n",
            "Epoch 011 | Train: 9.3413 | Val: 9.2559\n",
            "Epoch 012 | Train: 9.3243 | Val: 9.5469\n",
            "Epoch 013 | Train: 9.2751 | Val: 9.6294\n",
            "Epoch 014 | Train: 9.3220 | Val: 9.3347\n",
            "Epoch 015 | Train: 9.1417 | Val: 9.4723\n",
            "Epoch 016 | Train: 9.0748 | Val: 9.4951\n",
            "Epoch 017 | Train: 9.0972 | Val: 9.3425\n",
            "Epoch 018 | Train: 9.1379 | Val: 9.3558\n",
            "Epoch 019 | Train: 8.7882 | Val: 9.1320\n",
            "Epoch 020 | Train: 8.8740 | Val: 9.2551\n",
            "Epoch 021 | Train: 8.8771 | Val: 9.5362\n",
            "Epoch 022 | Train: 8.8278 | Val: 9.1965\n",
            "Epoch 023 | Train: 8.8493 | Val: 9.7079\n",
            "Epoch 024 | Train: 8.7281 | Val: 9.5887\n",
            "Epoch 025 | Train: 8.8042 | Val: 9.4895\n",
            "Epoch 026 | Train: 8.6197 | Val: 9.4792\n",
            "Epoch 027 | Train: 8.7507 | Val: 9.9860\n",
            "Epoch 028 | Train: 8.6388 | Val: 9.5083\n",
            "Epoch 029 | Train: 8.6127 | Val: 9.6158\n",
            "Early stopping.\n",
            "Holdout MSE: 8.0303\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.0007, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 51.2760 | Val: 19.7309\n",
            "Epoch 002 | Train: 13.6531 | Val: 10.1987\n",
            "Epoch 003 | Train: 11.5908 | Val: 10.0542\n",
            "Epoch 004 | Train: 11.4114 | Val: 9.6670\n",
            "Epoch 005 | Train: 10.9953 | Val: 9.6083\n",
            "Epoch 006 | Train: 10.8014 | Val: 9.8301\n",
            "Epoch 007 | Train: 10.5733 | Val: 9.4586\n",
            "Epoch 008 | Train: 10.4886 | Val: 9.3918\n",
            "Epoch 009 | Train: 10.4842 | Val: 9.3541\n",
            "Epoch 010 | Train: 10.3104 | Val: 9.3739\n",
            "Epoch 011 | Train: 10.3308 | Val: 9.7739\n",
            "Epoch 012 | Train: 10.3005 | Val: 9.2697\n",
            "Epoch 013 | Train: 10.0325 | Val: 9.3495\n",
            "Epoch 014 | Train: 9.9977 | Val: 9.6126\n",
            "Epoch 015 | Train: 9.9786 | Val: 9.3724\n",
            "Epoch 016 | Train: 10.1432 | Val: 9.1525\n",
            "Epoch 017 | Train: 9.8896 | Val: 9.3248\n",
            "Epoch 018 | Train: 9.8652 | Val: 9.3264\n",
            "Epoch 019 | Train: 9.7693 | Val: 9.1632\n",
            "Epoch 020 | Train: 9.6993 | Val: 9.0778\n",
            "Epoch 021 | Train: 9.6373 | Val: 9.1650\n",
            "Epoch 022 | Train: 9.7287 | Val: 9.1800\n",
            "Epoch 023 | Train: 9.5767 | Val: 9.1639\n",
            "Epoch 024 | Train: 9.5598 | Val: 9.1672\n",
            "Epoch 025 | Train: 9.5516 | Val: 9.1255\n",
            "Epoch 026 | Train: 9.5310 | Val: 9.1596\n",
            "Epoch 027 | Train: 9.5382 | Val: 9.1853\n",
            "Epoch 028 | Train: 9.3911 | Val: 9.1234\n",
            "Epoch 029 | Train: 9.3549 | Val: 9.5580\n",
            "Epoch 030 | Train: 9.4810 | Val: 9.2489\n",
            "Early stopping.\n",
            "Holdout MSE: 8.3280\n",
            "\n",
            "Trying hidden=48, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 33.4427 | Val: 10.3985\n",
            "Epoch 002 | Train: 10.9590 | Val: 9.7506\n",
            "Epoch 003 | Train: 10.4123 | Val: 9.7099\n",
            "Epoch 004 | Train: 10.0625 | Val: 9.2407\n",
            "Epoch 005 | Train: 9.8294 | Val: 9.5078\n",
            "Epoch 006 | Train: 9.7647 | Val: 9.3511\n",
            "Epoch 007 | Train: 9.5321 | Val: 9.4686\n",
            "Epoch 008 | Train: 9.5360 | Val: 9.7519\n",
            "Epoch 009 | Train: 9.4173 | Val: 9.1818\n",
            "Epoch 010 | Train: 9.2969 | Val: 9.6217\n",
            "Epoch 011 | Train: 9.2612 | Val: 9.5012\n",
            "Epoch 012 | Train: 9.1184 | Val: 9.8604\n",
            "Epoch 013 | Train: 9.1408 | Val: 9.2222\n",
            "Epoch 014 | Train: 9.1390 | Val: 9.6456\n",
            "Epoch 015 | Train: 9.0702 | Val: 9.2925\n",
            "Epoch 016 | Train: 8.9544 | Val: 9.6078\n",
            "Epoch 017 | Train: 8.9219 | Val: 9.5490\n",
            "Epoch 018 | Train: 8.8958 | Val: 9.6490\n",
            "Epoch 019 | Train: 8.7666 | Val: 9.2337\n",
            "Early stopping.\n",
            "Holdout MSE: 7.7339\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.001, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 40.3749 | Val: 12.0383\n",
            "Epoch 002 | Train: 11.6802 | Val: 9.7395\n",
            "Epoch 003 | Train: 11.1660 | Val: 10.7709\n",
            "Epoch 004 | Train: 10.7628 | Val: 10.5851\n",
            "Epoch 005 | Train: 10.5380 | Val: 9.6093\n",
            "Epoch 006 | Train: 10.2801 | Val: 10.5178\n",
            "Epoch 007 | Train: 10.1761 | Val: 9.1433\n",
            "Epoch 008 | Train: 10.0552 | Val: 9.2224\n",
            "Epoch 009 | Train: 9.9158 | Val: 9.3165\n",
            "Epoch 010 | Train: 9.8343 | Val: 9.3610\n",
            "Epoch 011 | Train: 9.8406 | Val: 9.1781\n",
            "Epoch 012 | Train: 9.6661 | Val: 9.7594\n",
            "Epoch 013 | Train: 9.5235 | Val: 9.0803\n",
            "Epoch 014 | Train: 9.6090 | Val: 9.1647\n",
            "Epoch 015 | Train: 9.6400 | Val: 9.1070\n",
            "Epoch 016 | Train: 9.5917 | Val: 9.0289\n",
            "Epoch 017 | Train: 9.4664 | Val: 9.4854\n",
            "Epoch 018 | Train: 9.3985 | Val: 9.0469\n",
            "Epoch 019 | Train: 9.4347 | Val: 9.4122\n",
            "Epoch 020 | Train: 9.1495 | Val: 9.2172\n",
            "Epoch 021 | Train: 9.4133 | Val: 9.3385\n",
            "Epoch 022 | Train: 9.3043 | Val: 9.2884\n",
            "Epoch 023 | Train: 9.3018 | Val: 9.5061\n",
            "Epoch 024 | Train: 9.2428 | Val: 9.2487\n",
            "Epoch 025 | Train: 9.1724 | Val: 9.3773\n",
            "Epoch 026 | Train: 9.1340 | Val: 9.3744\n",
            "Early stopping.\n",
            "Holdout MSE: 7.4401\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.0007, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 54.8312 | Val: 16.1581\n",
            "Epoch 002 | Train: 13.8091 | Val: 11.2393\n",
            "Epoch 003 | Train: 11.7865 | Val: 10.3841\n",
            "Epoch 004 | Train: 11.3860 | Val: 10.3183\n",
            "Epoch 005 | Train: 10.9606 | Val: 10.0167\n",
            "Epoch 006 | Train: 10.8375 | Val: 10.7877\n",
            "Epoch 007 | Train: 10.4698 | Val: 10.7433\n",
            "Epoch 008 | Train: 10.6133 | Val: 9.9182\n",
            "Epoch 009 | Train: 10.3441 | Val: 9.3494\n",
            "Epoch 010 | Train: 10.2923 | Val: 9.6237\n",
            "Epoch 011 | Train: 10.2960 | Val: 9.3300\n",
            "Epoch 012 | Train: 10.1686 | Val: 9.6039\n",
            "Epoch 013 | Train: 9.9996 | Val: 9.7686\n",
            "Epoch 014 | Train: 10.0516 | Val: 9.6630\n",
            "Epoch 015 | Train: 9.9238 | Val: 9.1130\n",
            "Epoch 016 | Train: 9.9465 | Val: 9.4244\n",
            "Epoch 017 | Train: 9.8929 | Val: 9.3526\n",
            "Epoch 018 | Train: 9.6673 | Val: 9.1803\n",
            "Epoch 019 | Train: 9.7487 | Val: 9.6596\n",
            "Epoch 020 | Train: 9.7868 | Val: 9.3372\n",
            "Epoch 021 | Train: 9.7044 | Val: 9.0120\n",
            "Epoch 022 | Train: 9.6611 | Val: 9.4845\n",
            "Epoch 023 | Train: 9.6545 | Val: 8.9668\n",
            "Epoch 024 | Train: 9.5541 | Val: 9.0897\n",
            "Epoch 025 | Train: 9.5811 | Val: 9.3182\n",
            "Epoch 026 | Train: 9.6646 | Val: 9.1003\n",
            "Epoch 027 | Train: 9.5900 | Val: 9.0816\n",
            "Epoch 028 | Train: 9.4159 | Val: 9.2274\n",
            "Epoch 029 | Train: 9.5020 | Val: 9.1159\n",
            "Epoch 030 | Train: 9.4683 | Val: 9.0426\n",
            "Epoch 031 | Train: 9.3599 | Val: 9.1715\n",
            "Epoch 032 | Train: 9.3805 | Val: 9.1346\n",
            "Epoch 033 | Train: 9.2531 | Val: 9.1646\n",
            "Early stopping.\n",
            "Holdout MSE: 7.5053\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 41.7984 | Val: 12.7516\n",
            "Epoch 002 | Train: 12.3902 | Val: 10.2358\n",
            "Epoch 003 | Train: 11.6357 | Val: 9.5480\n",
            "Epoch 004 | Train: 11.3089 | Val: 9.5425\n",
            "Epoch 005 | Train: 10.9955 | Val: 9.3818\n",
            "Epoch 006 | Train: 10.5960 | Val: 9.5857\n",
            "Epoch 007 | Train: 10.6138 | Val: 10.1059\n",
            "Epoch 008 | Train: 10.4470 | Val: 9.2337\n",
            "Epoch 009 | Train: 10.3648 | Val: 9.4660\n",
            "Epoch 010 | Train: 10.2643 | Val: 9.3948\n",
            "Epoch 011 | Train: 10.1226 | Val: 9.2786\n",
            "Epoch 012 | Train: 10.0790 | Val: 9.3487\n",
            "Epoch 013 | Train: 9.9947 | Val: 9.1569\n",
            "Epoch 014 | Train: 9.8503 | Val: 9.0952\n",
            "Epoch 015 | Train: 9.8207 | Val: 8.9983\n",
            "Epoch 016 | Train: 9.7972 | Val: 9.2730\n",
            "Epoch 017 | Train: 9.7477 | Val: 9.0999\n",
            "Epoch 018 | Train: 9.7505 | Val: 9.2061\n",
            "Epoch 019 | Train: 9.6022 | Val: 9.2509\n",
            "Epoch 020 | Train: 9.5568 | Val: 9.2208\n",
            "Epoch 021 | Train: 9.6109 | Val: 9.0811\n",
            "Epoch 022 | Train: 9.4908 | Val: 8.9558\n",
            "Epoch 023 | Train: 9.5880 | Val: 9.0566\n",
            "Epoch 024 | Train: 9.3830 | Val: 9.5714\n",
            "Epoch 025 | Train: 9.4895 | Val: 9.2180\n",
            "Epoch 026 | Train: 9.3370 | Val: 9.1538\n",
            "Epoch 027 | Train: 9.3789 | Val: 9.2398\n",
            "Epoch 028 | Train: 9.3552 | Val: 9.3509\n",
            "Epoch 029 | Train: 9.4502 | Val: 9.2260\n",
            "Epoch 030 | Train: 9.3596 | Val: 9.3922\n",
            "Epoch 031 | Train: 9.4034 | Val: 9.1940\n",
            "Epoch 032 | Train: 9.3981 | Val: 9.3681\n",
            "Early stopping.\n",
            "Holdout MSE: 8.7501\n",
            "\n",
            "Trying hidden=64, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 29.4022 | Val: 10.9053\n",
            "Epoch 002 | Train: 10.5919 | Val: 9.9271\n",
            "Epoch 003 | Train: 10.0186 | Val: 9.9353\n",
            "Epoch 004 | Train: 9.7098 | Val: 9.1712\n",
            "Epoch 005 | Train: 9.6604 | Val: 9.4143\n",
            "Epoch 006 | Train: 9.4458 | Val: 9.3886\n",
            "Epoch 007 | Train: 9.4289 | Val: 9.2104\n",
            "Epoch 008 | Train: 9.3289 | Val: 9.7282\n",
            "Epoch 009 | Train: 9.1495 | Val: 9.0913\n",
            "Epoch 010 | Train: 9.0405 | Val: 9.4706\n",
            "Epoch 011 | Train: 8.9452 | Val: 9.1632\n",
            "Epoch 012 | Train: 8.8946 | Val: 9.4476\n",
            "Epoch 013 | Train: 8.8993 | Val: 9.3053\n",
            "Epoch 014 | Train: 8.7093 | Val: 9.3623\n",
            "Epoch 015 | Train: 8.7312 | Val: 9.4009\n",
            "Epoch 016 | Train: 8.6329 | Val: 9.3704\n",
            "Epoch 017 | Train: 8.7224 | Val: 9.3434\n",
            "Epoch 018 | Train: 8.7235 | Val: 9.3338\n",
            "Epoch 019 | Train: 8.7581 | Val: 9.4446\n",
            "Early stopping.\n",
            "Holdout MSE: 9.7286\n",
            "\n",
            "Trying hidden=32, dropout=0.15, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 38.4177 | Val: 11.7111\n",
            "Epoch 002 | Train: 11.1628 | Val: 10.7367\n",
            "Epoch 003 | Train: 10.4722 | Val: 9.3493\n",
            "Epoch 004 | Train: 10.1420 | Val: 9.5668\n",
            "Epoch 005 | Train: 9.6729 | Val: 9.6826\n",
            "Epoch 006 | Train: 9.7920 | Val: 9.3255\n",
            "Epoch 007 | Train: 9.5411 | Val: 9.1194\n",
            "Epoch 008 | Train: 9.6209 | Val: 9.9195\n",
            "Epoch 009 | Train: 9.5748 | Val: 9.5125\n",
            "Epoch 010 | Train: 9.3912 | Val: 9.4207\n",
            "Epoch 011 | Train: 9.2917 | Val: 9.3860\n",
            "Epoch 012 | Train: 9.2441 | Val: 9.4165\n",
            "Epoch 013 | Train: 9.2342 | Val: 9.5152\n",
            "Epoch 014 | Train: 9.1824 | Val: 9.1287\n",
            "Epoch 015 | Train: 9.0375 | Val: 9.5463\n",
            "Epoch 016 | Train: 9.1034 | Val: 9.5191\n",
            "Epoch 017 | Train: 9.1203 | Val: 9.1048\n",
            "Epoch 018 | Train: 9.0187 | Val: 9.3135\n",
            "Epoch 019 | Train: 8.9547 | Val: 9.3377\n",
            "Epoch 020 | Train: 8.9607 | Val: 9.2035\n",
            "Epoch 021 | Train: 9.0270 | Val: 9.5557\n",
            "Epoch 022 | Train: 8.8483 | Val: 9.6050\n",
            "Epoch 023 | Train: 8.8440 | Val: 9.3093\n",
            "Epoch 024 | Train: 8.7487 | Val: 9.8096\n",
            "Epoch 025 | Train: 8.7816 | Val: 9.4833\n",
            "Epoch 026 | Train: 8.6390 | Val: 9.4000\n",
            "Epoch 027 | Train: 8.7282 | Val: 9.4384\n",
            "Early stopping.\n",
            "Holdout MSE: 7.6020\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.001, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 39.5806 | Val: 11.4552\n",
            "Epoch 002 | Train: 12.3448 | Val: 9.9178\n",
            "Epoch 003 | Train: 11.5982 | Val: 10.0587\n",
            "Epoch 004 | Train: 11.2329 | Val: 9.4248\n",
            "Epoch 005 | Train: 10.9011 | Val: 9.7407\n",
            "Epoch 006 | Train: 10.6954 | Val: 9.5273\n",
            "Epoch 007 | Train: 10.4840 | Val: 9.8535\n",
            "Epoch 008 | Train: 10.4209 | Val: 9.1623\n",
            "Epoch 009 | Train: 10.3093 | Val: 9.5040\n",
            "Epoch 010 | Train: 10.1968 | Val: 9.6362\n",
            "Epoch 011 | Train: 10.0654 | Val: 9.4718\n",
            "Epoch 012 | Train: 9.9944 | Val: 9.2291\n",
            "Epoch 013 | Train: 10.0827 | Val: 9.0168\n",
            "Epoch 014 | Train: 10.1102 | Val: 9.6163\n",
            "Epoch 015 | Train: 9.8257 | Val: 9.8750\n",
            "Epoch 016 | Train: 9.7582 | Val: 9.3211\n",
            "Epoch 017 | Train: 9.6324 | Val: 9.2150\n",
            "Epoch 018 | Train: 9.6822 | Val: 9.7467\n",
            "Epoch 019 | Train: 9.5218 | Val: 9.3759\n",
            "Epoch 020 | Train: 9.5652 | Val: 9.2142\n",
            "Epoch 021 | Train: 9.5922 | Val: 9.7169\n",
            "Epoch 022 | Train: 9.4870 | Val: 9.2253\n",
            "Epoch 023 | Train: 9.5177 | Val: 9.1848\n",
            "Early stopping.\n",
            "Holdout MSE: 8.4139\n",
            "\n",
            "Trying hidden=64, dropout=0.2, lr=0.0007, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 35.2648 | Val: 10.9738\n",
            "Epoch 002 | Train: 10.7601 | Val: 10.2415\n",
            "Epoch 003 | Train: 10.0372 | Val: 10.3420\n",
            "Epoch 004 | Train: 9.7891 | Val: 9.4392\n",
            "Epoch 005 | Train: 9.6353 | Val: 10.3411\n",
            "Epoch 006 | Train: 9.4764 | Val: 9.5215\n",
            "Epoch 007 | Train: 9.3194 | Val: 9.7025\n",
            "Epoch 008 | Train: 9.3323 | Val: 9.8104\n",
            "Epoch 009 | Train: 8.9898 | Val: 9.3901\n",
            "Epoch 010 | Train: 9.0244 | Val: 9.5177\n",
            "Epoch 011 | Train: 8.8659 | Val: 9.5482\n",
            "Epoch 012 | Train: 8.9234 | Val: 9.3189\n",
            "Epoch 013 | Train: 8.7321 | Val: 9.4011\n",
            "Epoch 014 | Train: 8.7105 | Val: 9.5615\n",
            "Epoch 015 | Train: 8.7977 | Val: 9.6376\n",
            "Epoch 016 | Train: 8.6487 | Val: 9.5418\n",
            "Epoch 017 | Train: 8.7027 | Val: 9.3983\n",
            "Epoch 018 | Train: 8.6135 | Val: 9.2653\n",
            "Epoch 019 | Train: 8.6081 | Val: 9.5275\n",
            "Epoch 020 | Train: 8.4630 | Val: 9.7393\n",
            "Epoch 021 | Train: 8.5018 | Val: 9.2359\n",
            "Epoch 022 | Train: 8.4359 | Val: 9.6413\n",
            "Epoch 023 | Train: 8.5279 | Val: 9.4200\n",
            "Epoch 024 | Train: 8.3640 | Val: 9.5365\n",
            "Epoch 025 | Train: 8.4132 | Val: 9.3605\n",
            "Epoch 026 | Train: 8.2905 | Val: 9.1868\n",
            "Epoch 027 | Train: 8.2787 | Val: 9.2661\n",
            "Epoch 028 | Train: 8.3770 | Val: 9.5670\n",
            "Epoch 029 | Train: 8.2344 | Val: 9.9653\n",
            "Epoch 030 | Train: 8.1939 | Val: 9.3969\n",
            "Epoch 031 | Train: 8.2733 | Val: 9.2143\n",
            "Epoch 032 | Train: 8.2429 | Val: 9.4646\n",
            "Epoch 033 | Train: 8.1325 | Val: 9.6244\n",
            "Epoch 034 | Train: 8.1189 | Val: 9.4810\n",
            "Epoch 035 | Train: 8.0632 | Val: 9.6493\n",
            "Epoch 036 | Train: 8.0351 | Val: 10.2360\n",
            "Early stopping.\n",
            "Holdout MSE: 10.3689\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Epoch 001 | Train: 39.0506 | Val: 11.6645\n",
            "Epoch 002 | Train: 11.6227 | Val: 10.1900\n",
            "Epoch 003 | Train: 10.9239 | Val: 10.2384\n",
            "Epoch 004 | Train: 10.8999 | Val: 9.5336\n",
            "Epoch 005 | Train: 10.4992 | Val: 10.5125\n",
            "Epoch 006 | Train: 10.4344 | Val: 10.2207\n",
            "Epoch 007 | Train: 10.2012 | Val: 9.4721\n",
            "Epoch 008 | Train: 10.0572 | Val: 9.6110\n",
            "Epoch 009 | Train: 9.9085 | Val: 9.3507\n",
            "Epoch 010 | Train: 9.8811 | Val: 9.5985\n",
            "Epoch 011 | Train: 9.8161 | Val: 9.3510\n",
            "Epoch 012 | Train: 9.7472 | Val: 9.0548\n",
            "Epoch 013 | Train: 9.6863 | Val: 9.2541\n",
            "Epoch 014 | Train: 9.5104 | Val: 9.0212\n",
            "Epoch 015 | Train: 9.4547 | Val: 9.3559\n",
            "Epoch 016 | Train: 9.5689 | Val: 9.5869\n",
            "Epoch 017 | Train: 9.4853 | Val: 9.1277\n",
            "Epoch 018 | Train: 9.3883 | Val: 9.0996\n",
            "Epoch 019 | Train: 9.2944 | Val: 9.1012\n",
            "Epoch 020 | Train: 9.1780 | Val: 9.4894\n",
            "Epoch 021 | Train: 9.3723 | Val: 9.2428\n",
            "Epoch 022 | Train: 9.3519 | Val: 9.2093\n",
            "Epoch 023 | Train: 9.2536 | Val: 9.2203\n",
            "Epoch 024 | Train: 9.2322 | Val: 9.3340\n",
            "Early stopping.\n",
            "Holdout MSE: 9.2890\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.0007, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 48.0225 | Val: 17.7122\n",
            "Epoch 002 | Train: 12.4473 | Val: 9.6646\n",
            "Epoch 003 | Train: 11.1745 | Val: 9.7718\n",
            "Epoch 004 | Train: 10.8468 | Val: 10.0883\n",
            "Epoch 005 | Train: 10.3607 | Val: 9.7861\n",
            "Epoch 006 | Train: 10.2182 | Val: 9.4804\n",
            "Epoch 007 | Train: 10.1233 | Val: 9.3157\n",
            "Epoch 008 | Train: 9.9922 | Val: 9.4390\n",
            "Epoch 009 | Train: 9.9352 | Val: 9.5683\n",
            "Epoch 010 | Train: 9.8483 | Val: 9.2079\n",
            "Epoch 011 | Train: 9.6661 | Val: 9.4530\n",
            "Epoch 012 | Train: 9.7308 | Val: 9.6397\n",
            "Epoch 013 | Train: 9.7007 | Val: 9.2747\n",
            "Epoch 014 | Train: 9.6796 | Val: 9.3829\n",
            "Epoch 015 | Train: 9.5204 | Val: 9.0266\n",
            "Epoch 016 | Train: 9.3593 | Val: 9.3049\n",
            "Epoch 017 | Train: 9.5506 | Val: 9.2644\n",
            "Epoch 018 | Train: 9.3767 | Val: 9.2491\n",
            "Epoch 019 | Train: 9.2607 | Val: 9.3221\n",
            "Epoch 020 | Train: 9.2905 | Val: 9.2166\n",
            "Epoch 021 | Train: 9.2575 | Val: 9.1158\n",
            "Epoch 022 | Train: 9.3818 | Val: 9.1520\n",
            "Epoch 023 | Train: 9.2573 | Val: 9.4983\n",
            "Epoch 024 | Train: 9.0950 | Val: 9.4363\n",
            "Epoch 025 | Train: 9.1088 | Val: 9.4521\n",
            "Early stopping.\n",
            "Holdout MSE: 8.4089\n",
            "\n",
            "Trying hidden=48, dropout=0.15, lr=0.0007, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 43.8054 | Val: 11.0354\n",
            "Epoch 002 | Train: 10.8088 | Val: 9.9520\n",
            "Epoch 003 | Train: 10.0456 | Val: 9.6610\n",
            "Epoch 004 | Train: 9.8117 | Val: 10.1268\n",
            "Epoch 005 | Train: 9.6077 | Val: 9.4927\n",
            "Epoch 006 | Train: 9.3970 | Val: 9.7407\n",
            "Epoch 007 | Train: 9.2677 | Val: 9.4731\n",
            "Epoch 008 | Train: 9.1324 | Val: 9.5759\n",
            "Epoch 009 | Train: 9.1337 | Val: 9.5331\n",
            "Epoch 010 | Train: 9.0107 | Val: 9.3485\n",
            "Epoch 011 | Train: 8.8410 | Val: 9.3193\n",
            "Epoch 012 | Train: 8.8510 | Val: 9.2165\n",
            "Epoch 013 | Train: 8.8970 | Val: 9.5367\n",
            "Epoch 014 | Train: 8.6595 | Val: 9.8835\n",
            "Epoch 015 | Train: 8.6927 | Val: 9.4201\n",
            "Epoch 016 | Train: 8.7226 | Val: 9.2949\n",
            "Epoch 017 | Train: 8.7436 | Val: 9.3312\n",
            "Epoch 018 | Train: 8.5294 | Val: 10.0020\n",
            "Epoch 019 | Train: 8.6455 | Val: 9.7425\n",
            "Epoch 020 | Train: 8.4815 | Val: 9.3351\n",
            "Epoch 021 | Train: 8.5871 | Val: 9.4610\n",
            "Epoch 022 | Train: 8.4758 | Val: 9.2905\n",
            "Early stopping.\n",
            "Holdout MSE: 7.1629\n",
            "\n",
            "Trying hidden=32, dropout=0.25, lr=0.001, weight_decay=5e-05, batch_size=32\n",
            "Epoch 001 | Train: 41.9420 | Val: 11.5512\n",
            "Epoch 002 | Train: 12.4806 | Val: 10.1198\n",
            "Epoch 003 | Train: 11.7324 | Val: 9.9523\n",
            "Epoch 004 | Train: 11.2638 | Val: 9.5760\n",
            "Epoch 005 | Train: 10.8114 | Val: 9.3357\n",
            "Epoch 006 | Train: 10.7678 | Val: 9.5736\n",
            "Epoch 007 | Train: 10.6393 | Val: 9.5145\n",
            "Epoch 008 | Train: 10.5360 | Val: 9.4392\n",
            "Epoch 009 | Train: 10.4202 | Val: 9.3050\n",
            "Epoch 010 | Train: 10.3197 | Val: 9.9051\n",
            "Epoch 011 | Train: 10.2231 | Val: 9.7691\n",
            "Epoch 012 | Train: 10.1814 | Val: 9.2716\n",
            "Epoch 013 | Train: 9.9713 | Val: 9.3032\n",
            "Epoch 014 | Train: 9.9130 | Val: 9.5026\n",
            "Epoch 015 | Train: 9.7432 | Val: 9.2440\n",
            "Epoch 016 | Train: 9.8785 | Val: 9.2696\n",
            "Epoch 017 | Train: 9.6382 | Val: 9.1303\n",
            "Epoch 018 | Train: 9.7365 | Val: 9.4593\n",
            "Epoch 019 | Train: 9.8181 | Val: 9.3683\n",
            "Epoch 020 | Train: 9.6635 | Val: 9.4363\n",
            "Epoch 021 | Train: 9.7227 | Val: 9.3752\n",
            "Epoch 022 | Train: 9.4510 | Val: 9.1698\n",
            "Epoch 023 | Train: 9.6781 | Val: 9.1868\n",
            "Epoch 024 | Train: 9.4852 | Val: 9.4590\n",
            "Epoch 025 | Train: 9.3778 | Val: 9.1470\n",
            "Epoch 026 | Train: 9.5258 | Val: 9.0740\n",
            "Epoch 027 | Train: 9.3893 | Val: 9.2816\n",
            "Epoch 028 | Train: 9.4109 | Val: 9.0614\n",
            "Epoch 029 | Train: 9.3311 | Val: 9.1359\n",
            "Epoch 030 | Train: 9.3374 | Val: 9.4017\n",
            "Epoch 031 | Train: 9.3414 | Val: 9.3818\n",
            "Epoch 032 | Train: 9.1888 | Val: 9.2973\n",
            "Epoch 033 | Train: 9.2746 | Val: 9.2985\n",
            "Epoch 034 | Train: 9.2403 | Val: 9.2718\n",
            "Epoch 035 | Train: 9.1979 | Val: 9.2459\n",
            "Epoch 036 | Train: 9.3290 | Val: 9.2598\n",
            "Epoch 037 | Train: 9.2306 | Val: 9.2724\n",
            "Epoch 038 | Train: 9.2911 | Val: 9.2415\n",
            "Early stopping.\n",
            "Holdout MSE: 7.6039\n",
            "\n",
            "Trying hidden=32, dropout=0.15, lr=0.001, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 39.0870 | Val: 11.0117\n",
            "Epoch 002 | Train: 11.0561 | Val: 9.7388\n",
            "Epoch 003 | Train: 10.4866 | Val: 9.8135\n",
            "Epoch 004 | Train: 10.1373 | Val: 9.5158\n",
            "Epoch 005 | Train: 9.9210 | Val: 9.7487\n",
            "Epoch 006 | Train: 9.7673 | Val: 9.3446\n",
            "Epoch 007 | Train: 9.6160 | Val: 9.4676\n",
            "Epoch 008 | Train: 9.5657 | Val: 9.2837\n",
            "Epoch 009 | Train: 9.5699 | Val: 9.3158\n",
            "Epoch 010 | Train: 9.3862 | Val: 9.1591\n",
            "Epoch 011 | Train: 9.2792 | Val: 9.1146\n",
            "Epoch 012 | Train: 9.2960 | Val: 9.3934\n",
            "Epoch 013 | Train: 9.2371 | Val: 9.3353\n",
            "Epoch 014 | Train: 9.2553 | Val: 9.6376\n",
            "Epoch 015 | Train: 9.1727 | Val: 9.3292\n",
            "Epoch 016 | Train: 9.1530 | Val: 9.3726\n",
            "Epoch 017 | Train: 9.1107 | Val: 9.3676\n",
            "Epoch 018 | Train: 8.9326 | Val: 9.3761\n",
            "Epoch 019 | Train: 8.8454 | Val: 9.6012\n",
            "Epoch 020 | Train: 8.9182 | Val: 9.5802\n",
            "Epoch 021 | Train: 9.0418 | Val: 9.3589\n",
            "Early stopping.\n",
            "Holdout MSE: 7.9272\n",
            "\n",
            "Trying hidden=48, dropout=0.2, lr=0.001, weight_decay=0.0, batch_size=32\n",
            "Epoch 001 | Train: 34.2628 | Val: 11.1150\n",
            "Epoch 002 | Train: 10.7860 | Val: 9.6793\n",
            "Epoch 003 | Train: 10.2903 | Val: 9.5224\n",
            "Epoch 004 | Train: 10.0661 | Val: 9.5159\n",
            "Epoch 005 | Train: 9.7731 | Val: 9.7772\n",
            "Epoch 006 | Train: 9.6445 | Val: 9.5018\n",
            "Epoch 007 | Train: 9.6284 | Val: 9.7024\n",
            "Epoch 008 | Train: 9.4232 | Val: 9.1430\n",
            "Epoch 009 | Train: 9.2868 | Val: 9.2609\n",
            "Epoch 010 | Train: 9.2918 | Val: 9.4149\n",
            "Epoch 011 | Train: 9.0855 | Val: 9.3588\n",
            "Epoch 012 | Train: 9.1154 | Val: 9.3249\n",
            "Epoch 013 | Train: 9.1454 | Val: 9.3981\n",
            "Epoch 014 | Train: 9.0119 | Val: 9.1149\n",
            "Epoch 015 | Train: 9.0596 | Val: 9.2027\n",
            "Epoch 016 | Train: 8.9628 | Val: 9.0955\n",
            "Epoch 017 | Train: 8.9089 | Val: 9.5768\n",
            "Epoch 018 | Train: 8.9255 | Val: 9.2334\n",
            "Epoch 019 | Train: 8.8322 | Val: 9.2850\n",
            "Epoch 020 | Train: 8.8459 | Val: 9.5724\n",
            "Epoch 021 | Train: 8.8119 | Val: 9.4155\n",
            "Epoch 022 | Train: 8.7076 | Val: 9.5474\n",
            "Epoch 023 | Train: 8.6560 | Val: 9.3030\n",
            "Epoch 024 | Train: 8.6083 | Val: 9.1537\n",
            "Epoch 025 | Train: 8.6812 | Val: 9.8868\n",
            "Epoch 026 | Train: 8.6912 | Val: 9.5654\n",
            "Early stopping.\n",
            "Holdout MSE: 7.2684\n",
            "\n",
            "✅ Best config for CITYPARK: {'hidden_dim': 48, 'dropout': 0.15, 'learning_rate': 0.0007, 'weight_decay': 5e-05, 'batch_size': 32}\n",
            "📉 Best holdout MSE: 7.1629\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# --- Define parameter grid ---\n",
        "hidden_dims = [32, 48, 64]\n",
        "dropouts = [0.15, 0.2, 0.25]\n",
        "lrs = [0.001, 0.0007]\n",
        "weight_decays = [0.0, 1e-5, 5e-5]\n",
        "batch_sizes = [32]\n",
        "\n",
        "grid = list(product(hidden_dims, dropouts, lrs, weight_decays, batch_sizes))\n",
        "grid = random.sample(grid, 20)\n",
        "selected_stations = ['LIDL BEŽIGRAD', 'CITYPARK']\n",
        "results = {}\n",
        "\n",
        "# --- Grid search per selected station ---\n",
        "for station in selected_stations:\n",
        "    print(f\"\\n### Grid search for {station} ###\")\n",
        "    feat = features_dict[station]\n",
        "    targ = targets_dict[station]\n",
        "    train_idx, val_idx, holdout_idx = splits[station][\"train\"], splits[station][\"val\"], splits[station][\"holdout\"]\n",
        "    X_train, y_train = feat[train_idx], targ[train_idx]\n",
        "    X_val, y_val = feat[val_idx], targ[val_idx]\n",
        "    X_hold, y_hold = feat[holdout_idx], targ[holdout_idx]\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_config = None\n",
        "    best_model = None\n",
        "\n",
        "    for hdim, drop, lr, wd, bs in grid:\n",
        "        print(f\"\\nTrying hidden={hdim}, dropout={drop}, lr={lr}, weight_decay={wd}, batch_size={bs}\")\n",
        "        model = train_mlp(\n",
        "            X_train, y_train, X_val, y_val,\n",
        "            in_features=X_train.shape[1], out_features=pred_horizon,\n",
        "            epochs=epochs, batch_size=bs,\n",
        "            hidden_dim=hdim, dropout=drop,\n",
        "            lr=lr, patience=patience, device=device,\n",
        "            weight_decay=wd\n",
        "        )\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_hold_tensor = torch.from_numpy(X_hold).float().to(device)\n",
        "            hold_pred = model(X_hold_tensor).cpu()\n",
        "            mse = ((hold_pred.numpy() - y_hold) ** 2).mean()\n",
        "\n",
        "        print(f\"Holdout MSE: {mse:.4f}\")\n",
        "        if mse < best_mse:\n",
        "            best_mse = mse\n",
        "            best_config = (hdim, drop, lr, wd, bs)\n",
        "            best_model = model.cpu()\n",
        "\n",
        "    results[station] = {\n",
        "        \"best_mse\": best_mse,\n",
        "        \"best_config\": {\n",
        "            \"hidden_dim\": best_config[0],\n",
        "            \"dropout\": best_config[1],\n",
        "            \"learning_rate\": best_config[2],\n",
        "            \"weight_decay\": best_config[3],\n",
        "            \"batch_size\": best_config[4]\n",
        "        },\n",
        "        \"model\": best_model\n",
        "    }\n",
        "\n",
        "    print(f\"\\n✅ Best config for {station}: {results[station]['best_config']}\")\n",
        "    print(f\"📉 Best holdout MSE: {best_mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zls17fDG2JKJ",
        "outputId": "397ac469-3f1c-4530-fce5-9197acf58546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to mlp_with_neighbors.csv\n"
          ]
        }
      ],
      "source": [
        "# --- PREDICT ON TEST SET ---\n",
        "test_df = pd.read_csv(\"bicikelj_test.csv\")\n",
        "test_pred = test_df.copy()\n",
        "test_pred[station_cols] = test_pred[station_cols].astype(str)  # to match LightGBM logic\n",
        "\n",
        "i = 0\n",
        "while i < len(test_df):\n",
        "    window = test_df.iloc[i:i+history_len]\n",
        "    pred_start = i + history_len\n",
        "    if pred_start + pred_horizon > len(test_df):\n",
        "        break\n",
        "    to_pred = test_df.iloc[pred_start:pred_start+pred_horizon]\n",
        "    mask = to_pred[station_cols].isnull() | (to_pred[station_cols] == '')\n",
        "\n",
        "    timestamps = pd.to_datetime(to_pred['timestamp'].iloc[0])\n",
        "    hour = timestamps.hour / 23.0\n",
        "    dow = timestamps.dayofweek / 6.0\n",
        "\n",
        "    for station in station_cols:\n",
        "        if station not in mlp_models:\n",
        "            continue\n",
        "        if not mask[station].any():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            own_hist = window[station].astype(float).values\n",
        "            nn_hist = []\n",
        "            for nn in neighbors[station]:\n",
        "                nn_hist.append(window[nn].astype(float).values)\n",
        "            nn_hist = np.concatenate(nn_hist) if nn_hist else np.zeros(0)\n",
        "            f = np.concatenate([own_hist, nn_hist, [hour, dow]])[None, :]\n",
        "        except:\n",
        "            continue  # Skip if any required input is missing\n",
        "\n",
        "        model = mlp_models[station]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(torch.from_numpy(f).float()).numpy()[0]\n",
        "        for h in range(pred_horizon):\n",
        "            if mask[station].iloc[h]:\n",
        "                test_pred.loc[pred_start + h, station] = pred[h]\n",
        "    i += history_len + pred_horizon\n",
        "\n",
        "# --- EXPORT PREDICTIONS ---\n",
        "rows_with_preds = test_df[station_cols].isnull() | (test_df[station_cols] == '')\n",
        "rows_with_preds = rows_with_preds.any(axis=1)\n",
        "header = pd.DataFrame([test_df.columns], columns=test_df.columns)\n",
        "final = pd.concat([header, test_pred[rows_with_preds]], ignore_index=True)\n",
        "final.to_csv(\"mlp_with_neighbors.csv\", index=False, header=False)\n",
        "print(\"Predictions saved to mlp_with_neighbors.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz53N_h_3Rbv",
        "outputId": "b8898543-ce0b-4b39-8df0-064e4330189b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved LIDL BEŽIGRAD model to mlp_models/LIDL_BEŽIGRAD.pt\n",
            "Saved ŠMARTINSKI PARK model to mlp_models/ŠMARTINSKI_PARK.pt\n",
            "Saved SAVSKO NASELJE 1-ŠMARTINSKA CESTA model to mlp_models/SAVSKO_NASELJE_1-ŠMARTINSKA_CESTA.pt\n",
            "Saved ČRNUČE model to mlp_models/ČRNUČE.pt\n",
            "Saved VILHARJEVA CESTA model to mlp_models/VILHARJEVA_CESTA.pt\n",
            "Saved MASARYKOVA DDC model to mlp_models/MASARYKOVA_DDC.pt\n",
            "Saved POGAČARJEV TRG-TRŽNICA model to mlp_models/POGAČARJEV_TRG-TRŽNICA.pt\n",
            "Saved CANKARJEVA UL.-NAMA model to mlp_models/CANKARJEVA_UL.-NAMA.pt\n",
            "Saved ANTONOV TRG model to mlp_models/ANTONOV_TRG.pt\n",
            "Saved PRUŠNIKOVA model to mlp_models/PRUŠNIKOVA.pt\n",
            "Saved TEHNOLOŠKI PARK model to mlp_models/TEHNOLOŠKI_PARK.pt\n",
            "Saved KOSEŠKI BAJER model to mlp_models/KOSEŠKI_BAJER.pt\n",
            "Saved TIVOLI model to mlp_models/TIVOLI.pt\n",
            "Saved TRŽNICA MOSTE model to mlp_models/TRŽNICA_MOSTE.pt\n",
            "Saved GRUDNOVO NABREŽJE-KARLOVŠKA C. model to mlp_models/GRUDNOVO_NABREŽJE-KARLOVŠKA_C..pt\n",
            "Saved LIDL-LITIJSKA CESTA model to mlp_models/LIDL-LITIJSKA_CESTA.pt\n",
            "Saved ŠPORTNI CENTER STOŽICE model to mlp_models/ŠPORTNI_CENTER_STOŽICE.pt\n",
            "Saved ŠPICA model to mlp_models/ŠPICA.pt\n",
            "Saved ROŠKA - STRELIŠKA model to mlp_models/ROŠKA_-_STRELIŠKA.pt\n",
            "Saved BAVARSKI DVOR model to mlp_models/BAVARSKI_DVOR.pt\n",
            "Saved STARA CERKEV model to mlp_models/STARA_CERKEV.pt\n",
            "Saved SITULA model to mlp_models/SITULA.pt\n",
            "Saved ILIRSKA ULICA model to mlp_models/ILIRSKA_ULICA.pt\n",
            "Saved LIDL - RUDNIK model to mlp_models/LIDL_-_RUDNIK.pt\n",
            "Saved KOPALIŠČE KOLEZIJA model to mlp_models/KOPALIŠČE_KOLEZIJA.pt\n",
            "Saved POVŠETOVA - KAJUHOVA model to mlp_models/POVŠETOVA_-_KAJUHOVA.pt\n",
            "Saved DUNAJSKA C.-PS MERCATOR model to mlp_models/DUNAJSKA_C.-PS_MERCATOR.pt\n",
            "Saved CITYPARK model to mlp_models/CITYPARK.pt\n",
            "Saved KOPRSKA ULICA model to mlp_models/KOPRSKA_ULICA.pt\n",
            "Saved LIDL - VOJKOVA CESTA model to mlp_models/LIDL_-_VOJKOVA_CESTA.pt\n",
            "Saved POLJANSKA-POTOČNIKOVA model to mlp_models/POLJANSKA-POTOČNIKOVA.pt\n",
            "Saved POVŠETOVA-GRABLOVIČEVA model to mlp_models/POVŠETOVA-GRABLOVIČEVA.pt\n",
            "Saved PARK NAVJE-ŽELEZNA CESTA model to mlp_models/PARK_NAVJE-ŽELEZNA_CESTA.pt\n",
            "Saved ZALOG model to mlp_models/ZALOG.pt\n",
            "Saved CESTA NA ROŽNIK model to mlp_models/CESTA_NA_ROŽNIK.pt\n",
            "Saved HOFER-KAJUHOVA model to mlp_models/HOFER-KAJUHOVA.pt\n",
            "Saved DUNAJSKA C.-PS PETROL model to mlp_models/DUNAJSKA_C.-PS_PETROL.pt\n",
            "Saved STUDENEC model to mlp_models/STUDENEC.pt\n",
            "Saved PARKIRIŠČE NUK 2-FF model to mlp_models/PARKIRIŠČE_NUK_2-FF.pt\n",
            "Saved BRATOVŠEVA PLOŠČAD model to mlp_models/BRATOVŠEVA_PLOŠČAD.pt\n",
            "Saved KONGRESNI TRG-ŠUBIČEVA ULICA model to mlp_models/KONGRESNI_TRG-ŠUBIČEVA_ULICA.pt\n",
            "Saved BS4-STOŽICE model to mlp_models/BS4-STOŽICE.pt\n",
            "Saved GERBIČEVA - ŠPORTNI PARK SVOBODA model to mlp_models/GERBIČEVA_-_ŠPORTNI_PARK_SVOBODA.pt\n",
            "Saved ŽIVALSKI VRT model to mlp_models/ŽIVALSKI_VRT.pt\n",
            "Saved VOKA - SLOVENČEVA model to mlp_models/VOKA_-_SLOVENČEVA.pt\n",
            "Saved BTC CITY/DVORANA A model to mlp_models/BTC_CITY_DVORANA_A.pt\n",
            "Saved TRNOVO model to mlp_models/TRNOVO.pt\n",
            "Saved P+R BARJE model to mlp_models/P_R_BARJE.pt\n",
            "Saved ROŽNA DOLINA-ŠKRABČEVA UL. model to mlp_models/ROŽNA_DOLINA-ŠKRABČEVA_UL..pt\n",
            "Saved KINO ŠIŠKA model to mlp_models/KINO_ŠIŠKA.pt\n",
            "Saved BRODARJEV TRG model to mlp_models/BRODARJEV_TRG.pt\n",
            "Saved ZALOŠKA C.-GRABLOVIČEVA C. model to mlp_models/ZALOŠKA_C.-GRABLOVIČEVA_C..pt\n",
            "Saved DOLENJSKA C. - STRELIŠČE model to mlp_models/DOLENJSKA_C._-_STRELIŠČE.pt\n",
            "Saved ŠTEPANJSKO NASELJE 1-JAKČEVA ULICA model to mlp_models/ŠTEPANJSKO_NASELJE_1-JAKČEVA_ULICA.pt\n",
            "Saved SOSESKA NOVO BRDO model to mlp_models/SOSESKA_NOVO_BRDO.pt\n",
            "Saved TRŽNICA KOSEZE model to mlp_models/TRŽNICA_KOSEZE.pt\n",
            "Saved ALEJA - CELOVŠKA CESTA model to mlp_models/ALEJA_-_CELOVŠKA_CESTA.pt\n",
            "Saved MERCATOR CENTER ŠIŠKA model to mlp_models/MERCATOR_CENTER_ŠIŠKA.pt\n",
            "Saved GH ŠENTPETER-NJEGOŠEVA C. model to mlp_models/GH_ŠENTPETER-NJEGOŠEVA_C..pt\n",
            "Saved HOFER - POLJE model to mlp_models/HOFER_-_POLJE.pt\n",
            "Saved VIŠKO POLJE model to mlp_models/VIŠKO_POLJE.pt\n",
            "Saved BONIFACIJA model to mlp_models/BONIFACIJA.pt\n",
            "Saved P + R DOLGI MOST model to mlp_models/P___R_DOLGI_MOST.pt\n",
            "Saved DRAVLJE model to mlp_models/DRAVLJE.pt\n",
            "Saved POLJE model to mlp_models/POLJE.pt\n",
            "Saved SUPERNOVA LJUBLJANA - RUDNIK model to mlp_models/SUPERNOVA_LJUBLJANA_-_RUDNIK.pt\n",
            "Saved SREDNJA FRIZERSKA ŠOLA model to mlp_models/SREDNJA_FRIZERSKA_ŠOLA.pt\n",
            "Saved TRG OF-KOLODVORSKA UL. model to mlp_models/TRG_OF-KOLODVORSKA_UL..pt\n",
            "Saved TRG MDB model to mlp_models/TRG_MDB.pt\n",
            "Saved TRŽAŠKA C.-ILIRIJA model to mlp_models/TRŽAŠKA_C.-ILIRIJA.pt\n",
            "Saved PREŠERNOV TRG-PETKOVŠKOVO NABREŽJE model to mlp_models/PREŠERNOV_TRG-PETKOVŠKOVO_NABREŽJE.pt\n",
            "Saved MERCATOR MARKET - CELOVŠKA C. 163 model to mlp_models/MERCATOR_MARKET_-_CELOVŠKA_C._163.pt\n",
            "Saved SAVSKO NASELJE 2-LINHARTOVA CESTA model to mlp_models/SAVSKO_NASELJE_2-LINHARTOVA_CESTA.pt\n",
            "Saved BREG model to mlp_models/BREG.pt\n",
            "Saved BTC CITY ATLANTIS model to mlp_models/BTC_CITY_ATLANTIS.pt\n",
            "Saved IKEA model to mlp_models/IKEA.pt\n",
            "Saved MIKLOŠIČEV PARK model to mlp_models/MIKLOŠIČEV_PARK.pt\n",
            "Saved BARJANSKA C.-CENTER STAREJŠIH TRNOVO model to mlp_models/BARJANSKA_C.-CENTER_STAREJŠIH_TRNOVO.pt\n",
            "Saved LEK - VEROVŠKOVA model to mlp_models/LEK_-_VEROVŠKOVA.pt\n",
            "Saved AMBROŽEV TRG model to mlp_models/AMBROŽEV_TRG.pt\n",
            "Saved VOJKOVA - GASILSKA BRIGADA model to mlp_models/VOJKOVA_-_GASILSKA_BRIGADA.pt\n",
            "Saved RAKOVNIK model to mlp_models/RAKOVNIK.pt\n",
            "Saved PREGLOV TRG model to mlp_models/PREGLOV_TRG.pt\n",
            "Saved PLEČNIKOV STADION model to mlp_models/PLEČNIKOV_STADION.pt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def safe_filename(name):\n",
        "    return re.sub(r'[^\\w\\-_.]', '_', name)\n",
        "\n",
        "for station, model in mlp_models.items():\n",
        "    safe_name = safe_filename(station)\n",
        "    path = f\"mlp_models/{safe_name}.pt\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Saved {station} model to {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XubJ5LFx3dk-",
        "outputId": "6fc59375-b737-46d0-e448-08569c3eac37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: mlp_models/ (stored 0%)\n",
            "  adding: mlp_models/KOPRSKA_ULICA.pt (deflated 12%)\n",
            "  adding: mlp_models/ANTONOV_TRG.pt (deflated 12%)\n",
            "  adding: mlp_models/BRATOVŠEVA_PLOŠČAD.pt (deflated 11%)\n",
            "  adding: mlp_models/HOFER_-_POLJE.pt (deflated 12%)\n",
            "  adding: mlp_models/BS4-STOŽICE.pt (deflated 11%)\n",
            "  adding: mlp_models/BAVARSKI_DVOR.pt (deflated 12%)\n",
            "  adding: mlp_models/RAKOVNIK.pt (deflated 12%)\n",
            "  adding: mlp_models/GH_ŠENTPETER-NJEGOŠEVA_C..pt (deflated 11%)\n",
            "  adding: mlp_models/KINO_ŠIŠKA.pt (deflated 11%)\n",
            "  adding: mlp_models/MIKLOŠIČEV_PARK.pt (deflated 11%)\n",
            "  adding: mlp_models/ČRNUČE.pt (deflated 11%)\n",
            "  adding: mlp_models/POVŠETOVA_-_KAJUHOVA.pt (deflated 11%)\n",
            "  adding: mlp_models/PREŠERNOV_TRG-PETKOVŠKOVO_NABREŽJE.pt (deflated 11%)\n",
            "  adding: mlp_models/KONGRESNI_TRG-ŠUBIČEVA_ULICA.pt (deflated 11%)\n",
            "  adding: mlp_models/SREDNJA_FRIZERSKA_ŠOLA.pt (deflated 11%)\n",
            "  adding: mlp_models/ALEJA_-_CELOVŠKA_CESTA.pt (deflated 11%)\n",
            "  adding: mlp_models/POLJANSKA-POTOČNIKOVA.pt (deflated 11%)\n",
            "  adding: mlp_models/KOPALIŠČE_KOLEZIJA.pt (deflated 11%)\n",
            "  adding: mlp_models/VILHARJEVA_CESTA.pt (deflated 12%)\n",
            "  adding: mlp_models/BTC_CITY_ATLANTIS.pt (deflated 12%)\n",
            "  adding: mlp_models/ZALOG.pt (deflated 10%)\n",
            "  adding: mlp_models/TRŽNICA_KOSEZE.pt (deflated 11%)\n",
            "  adding: mlp_models/PARK_NAVJE-ŽELEZNA_CESTA.pt (deflated 11%)\n",
            "  adding: mlp_models/POVŠETOVA-GRABLOVIČEVA.pt (deflated 11%)\n",
            "  adding: mlp_models/ŠPORTNI_CENTER_STOŽICE.pt (deflated 11%)\n",
            "  adding: mlp_models/BONIFACIJA.pt (deflated 12%)\n",
            "  adding: mlp_models/VOKA_-_SLOVENČEVA.pt (deflated 11%)\n",
            "  adding: mlp_models/MERCATOR_MARKET_-_CELOVŠKA_C._163.pt (deflated 11%)\n",
            "  adding: mlp_models/ŽIVALSKI_VRT.pt (deflated 11%)\n",
            "  adding: mlp_models/HOFER-KAJUHOVA.pt (deflated 12%)\n",
            "  adding: mlp_models/CESTA_NA_ROŽNIK.pt (deflated 11%)\n",
            "  adding: mlp_models/BRODARJEV_TRG.pt (deflated 12%)\n",
            "  adding: mlp_models/MERCATOR_CENTER_ŠIŠKA.pt (deflated 11%)\n",
            "  adding: mlp_models/VOJKOVA_-_GASILSKA_BRIGADA.pt (deflated 12%)\n",
            "  adding: mlp_models/GRUDNOVO_NABREŽJE-KARLOVŠKA_C..pt (deflated 11%)\n",
            "  adding: mlp_models/POLJE.pt (deflated 10%)\n",
            "  adding: mlp_models/IKEA.pt (deflated 10%)\n",
            "  adding: mlp_models/LIDL_BEŽIGRAD.pt (deflated 11%)\n",
            "  adding: mlp_models/CITYPARK.pt (deflated 12%)\n",
            "  adding: mlp_models/TEHNOLOŠKI_PARK.pt (deflated 11%)\n",
            "  adding: mlp_models/TRŽAŠKA_C.-ILIRIJA.pt (deflated 11%)\n",
            "  adding: mlp_models/ŠPICA.pt (deflated 11%)\n",
            "  adding: mlp_models/LEK_-_VEROVŠKOVA.pt (deflated 11%)\n",
            "  adding: mlp_models/DUNAJSKA_C.-PS_PETROL.pt (deflated 12%)\n",
            "  adding: mlp_models/P_R_BARJE.pt (deflated 12%)\n",
            "  adding: mlp_models/DRAVLJE.pt (deflated 11%)\n",
            "  adding: mlp_models/SOSESKA_NOVO_BRDO.pt (deflated 12%)\n",
            "  adding: mlp_models/ILIRSKA_ULICA.pt (deflated 12%)\n",
            "  adding: mlp_models/LIDL-LITIJSKA_CESTA.pt (deflated 12%)\n",
            "  adding: mlp_models/DUNAJSKA_C.-PS_MERCATOR.pt (deflated 12%)\n",
            "  adding: mlp_models/POGAČARJEV_TRG-TRŽNICA.pt (deflated 11%)\n",
            "  adding: mlp_models/TRG_MDB.pt (deflated 11%)\n",
            "  adding: mlp_models/LIDL_-_VOJKOVA_CESTA.pt (deflated 12%)\n",
            "  adding: mlp_models/STARA_CERKEV.pt (deflated 12%)\n",
            "  adding: mlp_models/ŠTEPANJSKO_NASELJE_1-JAKČEVA_ULICA.pt (deflated 11%)\n",
            "  adding: mlp_models/LIDL_-_RUDNIK.pt (deflated 12%)\n",
            "  adding: mlp_models/KOSEŠKI_BAJER.pt (deflated 11%)\n",
            "  adding: mlp_models/PLEČNIKOV_STADION.pt (deflated 11%)\n",
            "  adding: mlp_models/STUDENEC.pt (deflated 12%)\n",
            "  adding: mlp_models/PREGLOV_TRG.pt (deflated 12%)\n",
            "  adding: mlp_models/SAVSKO_NASELJE_2-LINHARTOVA_CESTA.pt (deflated 13%)\n",
            "  adding: mlp_models/P___R_DOLGI_MOST.pt (deflated 12%)\n",
            "  adding: mlp_models/PARKIRIŠČE_NUK_2-FF.pt (deflated 11%)\n",
            "  adding: mlp_models/ROŠKA_-_STRELIŠKA.pt (deflated 11%)\n",
            "  adding: mlp_models/DOLENJSKA_C._-_STRELIŠČE.pt (deflated 11%)\n",
            "  adding: mlp_models/BARJANSKA_C.-CENTER_STAREJŠIH_TRNOVO.pt (deflated 11%)\n",
            "  adding: mlp_models/SUPERNOVA_LJUBLJANA_-_RUDNIK.pt (deflated 12%)\n",
            "  adding: mlp_models/TRNOVO.pt (deflated 10%)\n",
            "  adding: mlp_models/ROŽNA_DOLINA-ŠKRABČEVA_UL..pt (deflated 11%)\n",
            "  adding: mlp_models/GERBIČEVA_-_ŠPORTNI_PARK_SVOBODA.pt (deflated 11%)\n",
            "  adding: mlp_models/PRUŠNIKOVA.pt (deflated 11%)\n",
            "  adding: mlp_models/ŠMARTINSKI_PARK.pt (deflated 11%)\n",
            "  adding: mlp_models/TIVOLI.pt (deflated 11%)\n",
            "  adding: mlp_models/AMBROŽEV_TRG.pt (deflated 11%)\n",
            "  adding: mlp_models/VIŠKO_POLJE.pt (deflated 11%)\n",
            "  adding: mlp_models/TRŽNICA_MOSTE.pt (deflated 11%)\n",
            "  adding: mlp_models/BREG.pt (deflated 10%)\n",
            "  adding: mlp_models/BTC_CITY_DVORANA_A.pt (deflated 12%)\n",
            "  adding: mlp_models/SITULA.pt (deflated 10%)\n",
            "  adding: mlp_models/TRG_OF-KOLODVORSKA_UL..pt (deflated 12%)\n",
            "  adding: mlp_models/MASARYKOVA_DDC.pt (deflated 12%)\n",
            "  adding: mlp_models/CANKARJEVA_UL.-NAMA.pt (deflated 12%)\n",
            "  adding: mlp_models/SAVSKO_NASELJE_1-ŠMARTINSKA_CESTA.pt (deflated 11%)\n",
            "  adding: mlp_models/ZALOŠKA_C.-GRABLOVIČEVA_C..pt (deflated 11%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r mlp_models.zip mlp_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzD1L9Dz3iqg"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gfkj3Tl3kCQ",
        "outputId": "b21232f7-7536-47e9-b714-632845ebfd8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-8da767c59338>:42: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trial 1/10: hidden=128, dropout=0.1, lr=0.001, weight_decay=0.0, batch_size=32, heads=4, layers=1\n",
            "Holdout MSE: 27.3257\n",
            "\n",
            "Trial 2/10: hidden=128, dropout=0.2, lr=0.001, weight_decay=1e-05, batch_size=64, heads=2, layers=2\n",
            "Holdout MSE: 23.8305\n",
            "\n",
            "Trial 3/10: hidden=32, dropout=0.1, lr=0.0005, weight_decay=1e-05, batch_size=32, heads=4, layers=2\n",
            "Holdout MSE: 28.1930\n",
            "\n",
            "Trial 4/10: hidden=128, dropout=0.2, lr=0.001, weight_decay=0.0, batch_size=64, heads=4, layers=2\n",
            "Holdout MSE: 25.2420\n",
            "\n",
            "Trial 5/10: hidden=128, dropout=0.1, lr=0.0005, weight_decay=1e-05, batch_size=32, heads=4, layers=2\n",
            "Holdout MSE: 25.6478\n",
            "\n",
            "Trial 6/10: hidden=64, dropout=0.1, lr=0.0005, weight_decay=1e-05, batch_size=32, heads=4, layers=2\n",
            "Holdout MSE: 29.4097\n",
            "\n",
            "Trial 7/10: hidden=64, dropout=0.1, lr=0.001, weight_decay=0.0, batch_size=32, heads=2, layers=2\n",
            "Holdout MSE: 27.7303\n",
            "\n",
            "Trial 8/10: hidden=64, dropout=0.2, lr=0.0005, weight_decay=0.0, batch_size=64, heads=4, layers=1\n",
            "Holdout MSE: 25.4549\n",
            "\n",
            "Trial 9/10: hidden=128, dropout=0.2, lr=0.0005, weight_decay=1e-05, batch_size=32, heads=2, layers=2\n",
            "Holdout MSE: 24.5071\n",
            "\n",
            "Trial 10/10: hidden=32, dropout=0.1, lr=0.0005, weight_decay=1e-05, batch_size=64, heads=4, layers=2\n",
            "Holdout MSE: 27.7644\n",
            "\n",
            "Best Config:\n",
            "{'hidden_dim': 128, 'dropout': 0.2, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 64, 'n_heads': 2, 'num_layers': 2, 'holdout_mse': np.float64(23.830517108241718)}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# --- PARAMETERS ---\n",
        "history_len = 48\n",
        "pred_horizon = 4\n",
        "k_neighbors = 2\n",
        "val_ratio = 0.15\n",
        "holdout_ratio = 0.15\n",
        "epochs = 50\n",
        "patience = 8\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "selected_stations = ['LIDL BEŽIGRAD', 'CITYPARK']  # use any subset of station names\n",
        "\n",
        "# --- HYPERPARAMETER GRID ---\n",
        "hidden_dims = [32, 64, 128]\n",
        "dropouts = [0.1, 0.2]\n",
        "lrs = [1e-3, 5e-4]\n",
        "weight_decays = [0.0, 1e-5]\n",
        "batch_sizes = [32, 64]\n",
        "n_heads_list = [2, 4]\n",
        "num_layers_list = [1, 2]\n",
        "\n",
        "grid = list(product(hidden_dims, dropouts, lrs, weight_decays, batch_sizes, n_heads_list, num_layers_list))\n",
        "random.shuffle(grid)\n",
        "num_trials = 10  # limit to 10 random configs\n",
        "grid = grid[:num_trials]\n",
        "\n",
        "# --- LOAD DATA ---\n",
        "train_df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = train_df.columns[1:]\n",
        "for col in station_cols:\n",
        "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "train_df = train_df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:k_neighbors]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- DATASET ---\n",
        "class BicikeljDataset(Dataset):\n",
        "    def __init__(self, df, station_cols, neighbors, history_len, pred_horizon, station_filter=None):\n",
        "        self.samples = []\n",
        "        timestamps = pd.to_datetime(df['timestamp'])\n",
        "        hours = (timestamps.dt.hour / 23.0).values\n",
        "        dows = (timestamps.dt.dayofweek / 6.0).values\n",
        "        bikes = df[station_cols].values.astype(float)\n",
        "        name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "\n",
        "        for s_name in station_filter:\n",
        "            s_idx = name_to_idx[s_name]\n",
        "            nn_idx = [name_to_idx[nn] for nn in neighbors[s_name]]\n",
        "            for i in range(history_len, len(df) - pred_horizon + 1):\n",
        "                seq = []\n",
        "                for t in range(i - history_len, i):\n",
        "                    row = [bikes[t, s_idx]]\n",
        "                    row += [bikes[t, j] for j in nn_idx]\n",
        "                    row += [hours[t], dows[t]]\n",
        "                    seq.append(row)\n",
        "                seq = np.stack(seq)\n",
        "                target = bikes[i:i + pred_horizon, s_idx]\n",
        "                self.samples.append((seq, target))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# --- MODEL ---\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_heads, num_layers, output_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads,\n",
        "                                                   dim_feedforward=hidden_dim*4,\n",
        "                                                   dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# --- TRAIN FUNCTION ---\n",
        "def train_transformer(train_loader, val_loader, input_dim, output_dim, hidden_dim, n_heads, num_layers,\n",
        "                      dropout, lr, weight_decay, device):\n",
        "    model = TimeSeriesTransformer(input_dim, hidden_dim, n_heads, num_layers, output_dim, dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = loss_fn(preds, yb)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        avg_val = np.mean(val_losses)\n",
        "        if avg_val < best_val_loss:\n",
        "            best_val_loss = avg_val\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# --- BUILD DATASET ---\n",
        "datasets = [BicikeljDataset(train_df, station_cols, neighbors, history_len, pred_horizon, station_filter=[s]) for s in selected_stations]\n",
        "full_dataset = ConcatDataset(datasets)\n",
        "total = len(full_dataset)\n",
        "train_end = int(total * (1 - val_ratio - holdout_ratio))\n",
        "val_end = int(total * (1 - holdout_ratio))\n",
        "train_set = Subset(full_dataset, range(0, train_end))\n",
        "val_set = Subset(full_dataset, range(train_end, val_end))\n",
        "holdout_set = Subset(full_dataset, range(val_end, total))\n",
        "\n",
        "# --- DIMENSIONS ---\n",
        "x_sample, y_sample = full_dataset[0]\n",
        "input_dim = x_sample.shape[1]\n",
        "output_dim = y_sample.shape[0]\n",
        "\n",
        "# --- GRID SEARCH ---\n",
        "results = []\n",
        "for i, (hd, dp, lr, wd, bs, nh, nl) in enumerate(grid):\n",
        "    print(f\"\\nTrial {i+1}/{len(grid)}: hidden={hd}, dropout={dp}, lr={lr}, weight_decay={wd}, batch_size={bs}, heads={nh}, layers={nl}\")\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=bs)\n",
        "    holdout_loader = DataLoader(holdout_set, batch_size=bs)\n",
        "\n",
        "    model = train_transformer(train_loader, val_loader, input_dim, output_dim, hd, nh, nl, dp, lr, wd, device)\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    holdout_losses = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in holdout_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            loss = nn.functional.mse_loss(preds, yb)\n",
        "            holdout_losses.append(loss.item())\n",
        "    mse = np.mean(holdout_losses)\n",
        "    print(f\"Holdout MSE: {mse:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        \"hidden_dim\": hd,\n",
        "        \"dropout\": dp,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": wd,\n",
        "        \"batch_size\": bs,\n",
        "        \"n_heads\": nh,\n",
        "        \"num_layers\": nl,\n",
        "        \"holdout_mse\": mse\n",
        "    })\n",
        "\n",
        "# --- RESULTS ---\n",
        "results = sorted(results, key=lambda r: r['holdout_mse'])\n",
        "print(\"\\nBest Config:\")\n",
        "print(results[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CD5HiSGBrfu"
      },
      "source": [
        "# LSTM Per station"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkqQeUZxBstW",
        "outputId": "49258e10-45d7-4a3f-df5b-1395b1caefa1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-769bd482b1aa>:39: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Grid search for LIDL BEŽIGRAD ###\n",
            "\n",
            "Trying hidden=64, dropout=0.3, lr=0.001, wd=1e-05, bs=32\n",
            "Holdout MSE: 11.0719\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.0005, wd=1e-05, bs=64\n",
            "Holdout MSE: 10.5347\n",
            "\n",
            "Trying hidden=64, dropout=0.3, lr=0.001, wd=0.0, bs=32\n",
            "Holdout MSE: 10.9735\n",
            "\n",
            "Trying hidden=32, dropout=0.3, lr=0.0005, wd=0.0, bs=64\n",
            "Holdout MSE: 10.3531\n",
            "\n",
            "Trying hidden=128, dropout=0.3, lr=0.001, wd=0.0, bs=32\n",
            "Holdout MSE: 11.4727\n",
            "\n",
            "Trying hidden=128, dropout=0.2, lr=0.001, wd=1e-05, bs=64\n",
            "Holdout MSE: 11.7817\n",
            "\n",
            "Trying hidden=32, dropout=0.3, lr=0.001, wd=0.0, bs=64\n",
            "Holdout MSE: 10.5880\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.001, wd=0.0, bs=64\n",
            "Holdout MSE: 10.6158\n",
            "\n",
            "Trying hidden=128, dropout=0.3, lr=0.001, wd=1e-05, bs=32\n",
            "Holdout MSE: 11.1964\n",
            "\n",
            "Trying hidden=32, dropout=0.3, lr=0.001, wd=1e-05, bs=64\n",
            "Holdout MSE: 10.3310\n",
            "\n",
            "Best config for LIDL BEŽIGRAD: {'hidden_dim': 32, 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 64}\n",
            "Best holdout MSE: 10.3310\n",
            "\n",
            "### Grid search for CITYPARK ###\n",
            "\n",
            "Trying hidden=64, dropout=0.3, lr=0.001, wd=1e-05, bs=32\n",
            "Holdout MSE: 7.9313\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.0005, wd=1e-05, bs=64\n",
            "Holdout MSE: 7.8228\n",
            "\n",
            "Trying hidden=64, dropout=0.3, lr=0.001, wd=0.0, bs=32\n",
            "Holdout MSE: 7.4676\n",
            "\n",
            "Trying hidden=32, dropout=0.3, lr=0.0005, wd=0.0, bs=64\n",
            "Holdout MSE: 7.6311\n",
            "\n",
            "Trying hidden=128, dropout=0.3, lr=0.001, wd=0.0, bs=32\n",
            "Holdout MSE: 8.5815\n",
            "\n",
            "Trying hidden=128, dropout=0.2, lr=0.001, wd=1e-05, bs=64\n",
            "Holdout MSE: 7.7833\n",
            "\n",
            "Trying hidden=32, dropout=0.3, lr=0.001, wd=0.0, bs=64\n",
            "Holdout MSE: 7.5835\n",
            "\n",
            "Trying hidden=32, dropout=0.2, lr=0.001, wd=0.0, bs=64\n",
            "Holdout MSE: 7.4837\n",
            "\n",
            "Trying hidden=128, dropout=0.3, lr=0.001, wd=1e-05, bs=32\n",
            "Holdout MSE: 7.7180\n",
            "\n",
            "Trying hidden=32, dropout=0.3, lr=0.001, wd=1e-05, bs=64\n",
            "Holdout MSE: 7.1318\n",
            "\n",
            "Best config for CITYPARK: {'hidden_dim': 32, 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 64}\n",
            "Best holdout MSE: 7.1318\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# --- PARAMETERS ---\n",
        "history_len = 48\n",
        "pred_horizon = 4\n",
        "k_neighbors = 2\n",
        "val_ratio = 0.15\n",
        "holdout_ratio = 0.15\n",
        "epochs = 50\n",
        "patience = 8\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "selected_stations = ['LIDL BEŽIGRAD', 'CITYPARK']\n",
        "\n",
        "# --- HYPERPARAMETER GRID ---\n",
        "hidden_dims = [32, 64, 128]\n",
        "dropouts = [0.2, 0.3]\n",
        "lrs = [1e-3, 5e-4]\n",
        "weight_decays = [0.0, 1e-5]\n",
        "batch_sizes = [32, 64]\n",
        "\n",
        "grid = list(product(hidden_dims, dropouts, lrs, weight_decays, batch_sizes))\n",
        "random.shuffle(grid)\n",
        "grid = grid[:10]  # Try only 10 configurations\n",
        "\n",
        "# --- LOAD DATA ---\n",
        "train_df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = train_df.columns[1:]\n",
        "for col in station_cols:\n",
        "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "train_df[station_cols] = train_df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "train_df = train_df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- NEIGHBOR DETECTION ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:k_neighbors]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- DATASET CLASS ---\n",
        "class BicikeljDataset(Dataset):\n",
        "    def __init__(self, df, station_name, station_cols, neighbors, history_len, pred_horizon):\n",
        "        self.samples = []\n",
        "        timestamps = pd.to_datetime(df['timestamp'])\n",
        "        hours = (timestamps.dt.hour / 23.0).values\n",
        "        dows = (timestamps.dt.dayofweek / 6.0).values\n",
        "        bikes = df[station_cols].values.astype(float)\n",
        "        name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "\n",
        "        s_idx = name_to_idx[station_name]\n",
        "        nn_idx = [name_to_idx[nn] for nn in neighbors[station_name]]\n",
        "\n",
        "        for i in range(history_len, len(df) - pred_horizon + 1):\n",
        "            seq = []\n",
        "            for t in range(i - history_len, i):\n",
        "                row = [bikes[t, s_idx]]\n",
        "                row += [bikes[t, j] for j in nn_idx]\n",
        "                row += [hours[t], dows[t]]\n",
        "                seq.append(row)\n",
        "            seq = np.stack(seq)\n",
        "            target = bikes[i:i + pred_horizon, s_idx]\n",
        "            self.samples.append((seq, target))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# --- LSTM MODEL ---\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = self.dropout(h_n[-1])\n",
        "        return self.fc(h)\n",
        "\n",
        "# --- TRAINING FUNCTION ---\n",
        "def train_lstm(train_loader, val_loader, input_dim, output_dim, hidden_dim, dropout, lr, weight_decay):\n",
        "    model = LSTMForecast(input_dim, hidden_dim, output_dim, dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = loss_fn(preds, yb)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        val_loss = np.mean(val_losses)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# --- PER STATION LOOP ---\n",
        "results = {}\n",
        "for station in selected_stations:\n",
        "    print(f\"\\n### Grid search for {station} ###\")\n",
        "    dataset = BicikeljDataset(train_df, station, station_cols, neighbors, history_len, pred_horizon)\n",
        "    total = len(dataset)\n",
        "    train_end = int(total * (1 - val_ratio - holdout_ratio))\n",
        "    val_end = int(total * (1 - holdout_ratio))\n",
        "    train_set = torch.utils.data.Subset(dataset, range(0, train_end))\n",
        "    val_set = torch.utils.data.Subset(dataset, range(train_end, val_end))\n",
        "    holdout_set = torch.utils.data.Subset(dataset, range(val_end, total))\n",
        "\n",
        "    sample_x, sample_y = dataset[0]\n",
        "    input_dim = sample_x.shape[1]\n",
        "    output_dim = sample_y.shape[0]\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_config = None\n",
        "    best_model = None\n",
        "\n",
        "    for hdim, drop, lr, wd, bs in grid:\n",
        "        print(f\"\\nTrying hidden={hdim}, dropout={drop}, lr={lr}, wd={wd}, bs={bs}\")\n",
        "        train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=bs)\n",
        "        holdout_loader = DataLoader(holdout_set, batch_size=bs)\n",
        "\n",
        "        model = train_lstm(train_loader, val_loader, input_dim, output_dim, hdim, drop, lr, wd)\n",
        "\n",
        "        # Evaluate on holdout\n",
        "        model.eval()\n",
        "        holdout_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in holdout_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = nn.functional.mse_loss(preds, yb)\n",
        "                holdout_losses.append(loss.item())\n",
        "        mse = np.mean(holdout_losses)\n",
        "        print(f\"Holdout MSE: {mse:.4f}\")\n",
        "\n",
        "        if mse < best_mse:\n",
        "            best_mse = mse\n",
        "            best_config = (hdim, drop, lr, wd, bs)\n",
        "            best_model = model.cpu()\n",
        "\n",
        "    results[station] = {\n",
        "        \"best_mse\": best_mse,\n",
        "        \"best_config\": {\n",
        "            \"hidden_dim\": best_config[0],\n",
        "            \"dropout\": best_config[1],\n",
        "            \"lr\": best_config[2],\n",
        "            \"weight_decay\": best_config[3],\n",
        "            \"batch_size\": best_config[4],\n",
        "        },\n",
        "        \"model\": best_model\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBest config for {station}: {results[station]['best_config']}\")\n",
        "    print(f\"Best holdout MSE: {best_mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTn-v1soGMXV"
      },
      "source": [
        "# LSTM Shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIEGzP1KGNlM",
        "outputId": "71efc3a3-6666-43cd-cdaa-6c3dad5a2739"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-74cd962ef921>:29: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[station_cols] = df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Val Loss = 11.8076\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Val Loss = 11.5019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Val Loss = 11.6014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Val Loss = 11.3264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Val Loss = 11.1160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Val Loss = 11.4553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Val Loss = 11.0925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Val Loss = 11.1123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Val Loss = 11.1308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Val Loss = 11.0821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Val Loss = 11.2118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Val Loss = 11.2074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Val Loss = 11.0327\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Val Loss = 11.0280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Val Loss = 10.9593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Val Loss = 10.9804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Val Loss = 11.0089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Val Loss = 10.9491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Val Loss = 11.0127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Val Loss = 11.0551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Val Loss = 10.9530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Val Loss = 11.0295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Val Loss = 11.0662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Val Loss = 11.0872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Val Loss = 10.8617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Val Loss = 10.8690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Val Loss = 10.9643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Val Loss = 10.9061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Val Loss = 10.8802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Val Loss = 10.9756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Val Loss = 10.7911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32: Val Loss = 10.9400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33: Val Loss = 10.9721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: Val Loss = 10.7299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: Val Loss = 10.9597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36: Val Loss = 10.9000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Val Loss = 10.9011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38: Val Loss = 10.9743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39: Val Loss = 10.9573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Val Loss = 11.0736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41: Val Loss = 11.0157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42: Val Loss = 10.9561\n",
            "Early stopping at epoch 42\n",
            "Saved shared model predictions to 'shared_lstm_holdout_preds.npy'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "HISTORY_LEN = 48\n",
        "PRED_HORIZON = 4\n",
        "K_NEIGHBORS = 2\n",
        "HIDDEN_DIM = 32\n",
        "DROPOUT = 0.3\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "PATIENCE = 8\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Load data ---\n",
        "df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = df.columns[1:]\n",
        "\n",
        "for col in station_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df[station_cols] = df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "df = df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- Neighbors ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- Dataset ---\n",
        "class SharedLSTMDataset(Dataset):\n",
        "    def __init__(self, df, station_cols, neighbors, history_len, pred_horizon):\n",
        "        self.samples = []\n",
        "        self.station_indices = []\n",
        "\n",
        "        timestamps = pd.to_datetime(df['timestamp'])\n",
        "        hours = (timestamps.dt.hour / 23.0).values\n",
        "        dows = (timestamps.dt.dayofweek / 6.0).values\n",
        "        bikes = df[station_cols].values.astype(np.float32)\n",
        "        name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "        N = len(df)\n",
        "\n",
        "        # Precompute extra features\n",
        "        time_feats = np.stack([hours, dows], axis=1)\n",
        "\n",
        "        for s_name in station_cols:\n",
        "            s_idx = name_to_idx[s_name]\n",
        "            nn_idx = [name_to_idx[nn] for nn in neighbors[s_name]]\n",
        "\n",
        "            series = bikes[:, [s_idx] + nn_idx]  # [T, 1 + k_neighbors]\n",
        "            all_feats = np.concatenate([series, time_feats], axis=1)  # [T, D]\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(history_len, N - pred_horizon + 1):\n",
        "                seq = all_feats[i - history_len:i]\n",
        "                target = bikes[i:i + pred_horizon, s_idx]\n",
        "                self.samples.append((seq, target))\n",
        "                self.station_indices.append(s_name)\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# --- Model ---\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.dropout(h_n[-1])\n",
        "        return self.fc(out)\n",
        "\n",
        "# --- Train ---\n",
        "def train_lstm(model, train_loader, val_loader, device):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\", leave=False)\n",
        "        for xb, yb in batch_bar:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            batch_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                val_losses.append(criterion(pred, yb).item())\n",
        "\n",
        "        val_loss = np.mean(val_losses)\n",
        "        print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Data splitting ---\n",
        "dataset = SharedLSTMDataset(df, station_cols, neighbors, HISTORY_LEN, PRED_HORIZON)\n",
        "N = len(dataset)\n",
        "val_size = int(0.15 * N)\n",
        "holdout_size = int(0.15 * N)\n",
        "train_size = N - val_size - holdout_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(dataset, range(0, train_size))\n",
        "val_set = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n",
        "holdout_set = torch.utils.data.Subset(dataset, range(train_size + val_size, N))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
        "holdout_loader = DataLoader(holdout_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "# --- Train model ---\n",
        "input_dim = 1 + K_NEIGHBORS + 2  # station + neighbors + time features\n",
        "output_dim = PRED_HORIZON\n",
        "\n",
        "model = LSTMForecast(input_dim, HIDDEN_DIM, output_dim, DROPOUT)\n",
        "model = train_lstm(model, train_loader, val_loader, DEVICE)\n",
        "\n",
        "# --- Evaluate on holdout ---\n",
        "model.eval()\n",
        "holdout_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in holdout_loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        pred = model(xb).cpu().numpy()\n",
        "        holdout_preds.append(pred)\n",
        "\n",
        "holdout_preds = np.vstack(holdout_preds)\n",
        "np.save(\"shared_lstm_holdout_preds.npy\", holdout_preds)\n",
        "print(\"Saved shared model predictions to 'shared_lstm_holdout_preds.npy'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX9YPFumZENY"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"lstm_model_added_features.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB-6nsdGdWz_",
        "outputId": "9fc3d2b5-b02b-403b-cbc8-63ef24ca06bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMForecast(\n",
              "  (lstm): LSTM(5, 32, batch_first=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Recreate the same architecture\n",
        "model = LSTMForecast(input_dim=5, hidden_dim=32, output_dim=4, dropout=0.3)\n",
        "model.load_state_dict(torch.load(\"lstm_model_added_features.pt\"))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LI64EF0_djwo",
        "outputId": "6a320526-161e-423f-beb4-ec09c521ee23"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-95b675e4fa1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Prepare helper maps and features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstation_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtimestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mhours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m23.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Predict only the unknown rows in bicikelj_test.csv using a single shared model ---\n",
        "\n",
        "# Recreate model and load the saved weights\n",
        "input_dim = 1 + K_NEIGHBORS + 2  # own station + neighbors + hour + dow\n",
        "output_dim = PRED_HORIZON\n",
        "\n",
        "model = LSTMForecast(input_dim, 32, output_dim, 0.3).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"lstm_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Prepare helper maps and features\n",
        "test_feats = test_df[station_cols].values.astype(np.float32)\n",
        "timestamps = pd.to_datetime(test_df[\"timestamp\"])\n",
        "hours = (timestamps.dt.hour / 23.0).values\n",
        "dows = (timestamps.dt.dayofweek / 6.0).values\n",
        "name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "\n",
        "# Collect predictions\n",
        "pred_matrix = np.full_like(test_feats, np.nan)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(HISTORY_LEN, len(test_df) - PRED_HORIZON + 1):\n",
        "        # Predict only if all stations are missing at the target range\n",
        "        if np.isnan(test_feats[i:i + PRED_HORIZON]).all(axis=0).all():\n",
        "            for station in station_cols:\n",
        "                s_idx = name_to_idx[station]\n",
        "                nn_idx = [name_to_idx[nn] for nn in neighbors[station]]\n",
        "\n",
        "                # Prepare input sequence\n",
        "                seq = []\n",
        "                for t in range(i - HISTORY_LEN, i):\n",
        "                    row = [test_feats[t, s_idx]]\n",
        "                    row += [test_feats[t, j] for j in nn_idx]\n",
        "                    row += [hours[t], dows[t]]\n",
        "                    seq.append(row)\n",
        "                seq = torch.tensor([seq], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "                # Predict and insert into matrix\n",
        "                pred = model(seq).cpu().numpy().flatten()\n",
        "                for j in range(PRED_HORIZON):\n",
        "                    pred_matrix[i + j, s_idx] = pred[j]\n",
        "\n",
        "# Build output DataFrame\n",
        "pred_df = pd.DataFrame(pred_matrix, columns=station_cols)\n",
        "pred_df.insert(0, \"timestamp\", test_df[\"timestamp\"])\n",
        "\n",
        "# Only include rows where predictions were made (i.e. they were originally all NaN)\n",
        "rows_to_output = test_df[station_cols].isna().all(axis=1)\n",
        "pred_df_filtered = pred_df[rows_to_output].copy()\n",
        "\n",
        "# Save final predictions\n",
        "pred_df_filtered.to_csv(\"bicikelj_test_predictions_128.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB4_GbpZ4Lud"
      },
      "source": [
        "# Added holidays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "iWXY2eM04N_S",
        "outputId": "ef734deb-36bb-48d5-9f98-89c7ee336cc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-51eb92a0719c>:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[station_cols] = df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-51eb92a0719c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMForecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m# --- Evaluate on holdout ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-51eb92a0719c>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(model, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mbatch_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS} [Training]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-51eb92a0719c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# --- Model ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from tqdm import tqdm\n",
        "import holidays\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "HISTORY_LEN = 48\n",
        "PRED_HORIZON = 4\n",
        "K_NEIGHBORS = 2\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.1\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "PATIENCE = 8\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Load data ---\n",
        "df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = df.columns[1:]\n",
        "\n",
        "# Clean and fill NaNs\n",
        "for col in station_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df[station_cols] = df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "df = df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- Neighbors ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- Dataset with time features ---\n",
        "class SharedLSTMDataset(Dataset):\n",
        "    def __init__(self, df, station_cols, neighbors, history_len, pred_horizon):\n",
        "        self.samples = []\n",
        "        self.station_indices = []\n",
        "\n",
        "        timestamps = pd.to_datetime(df['timestamp'])\n",
        "        hour_sin = np.sin(2 * np.pi * timestamps.dt.hour / 24)\n",
        "        hour_cos = np.cos(2 * np.pi * timestamps.dt.hour / 24)\n",
        "        is_weekend = (timestamps.dt.dayofweek >= 5).astype(float)\n",
        "        slo_holidays = holidays.Slovenia()\n",
        "        is_holiday = timestamps.dt.date.astype(str).isin([str(d) for d in slo_holidays]).astype(float)\n",
        "\n",
        "        bikes = df[station_cols].values.astype(np.float32)\n",
        "        name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "        N = len(df)\n",
        "\n",
        "        # Precompute extra features\n",
        "        time_feats = np.stack([hour_sin, hour_cos, is_weekend, is_holiday], axis=1)\n",
        "\n",
        "        for s_name in station_cols:\n",
        "            s_idx = name_to_idx[s_name]\n",
        "            nn_idx = [name_to_idx[nn] for nn in neighbors[s_name]]\n",
        "\n",
        "            series = bikes[:, [s_idx] + nn_idx]  # [T, 1 + k_neighbors]\n",
        "            all_feats = np.concatenate([series, time_feats], axis=1)  # [T, D]\n",
        "\n",
        "            for i in range(history_len, N - pred_horizon + 1):\n",
        "                seq = all_feats[i - history_len:i]\n",
        "                target = bikes[i:i + pred_horizon, s_idx]\n",
        "                self.samples.append((seq, target))\n",
        "                self.station_indices.append(s_name)\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# --- Model ---\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.dropout(h_n[-1])\n",
        "        return self.fc(out)\n",
        "\n",
        "# --- Train ---\n",
        "def train_lstm(model, train_loader, val_loader, device):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\", leave=False)\n",
        "        for xb, yb in batch_bar:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            batch_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                val_losses.append(criterion(pred, yb).item())\n",
        "\n",
        "        val_loss = np.mean(val_losses)\n",
        "        print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# --- Data splitting ---\n",
        "dataset = SharedLSTMDataset(df, station_cols, neighbors, HISTORY_LEN, PRED_HORIZON)\n",
        "N = len(dataset)\n",
        "val_size = int(0.15 * N)\n",
        "holdout_size = int(0.15 * N)\n",
        "train_size = N - val_size - holdout_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(dataset, range(0, train_size))\n",
        "val_set = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n",
        "holdout_set = torch.utils.data.Subset(dataset, range(train_size + val_size, N))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
        "holdout_loader = DataLoader(holdout_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "# --- Train model ---\n",
        "input_dim = 1 + K_NEIGHBORS + 4  # station + neighbors + 4 time features\n",
        "output_dim = PRED_HORIZON\n",
        "\n",
        "model = LSTMForecast(input_dim, HIDDEN_DIM, output_dim, DROPOUT)\n",
        "model = train_lstm(model, train_loader, val_loader, DEVICE)\n",
        "\n",
        "# --- Evaluate on holdout ---\n",
        "model.eval()\n",
        "holdout_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in holdout_loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        pred = model(xb).cpu().numpy()\n",
        "        holdout_preds.append(pred)\n",
        "\n",
        "holdout_preds = np.vstack(holdout_preds)\n",
        "np.save(\"shared_lstm_holdout_preds.npy\", holdout_preds)\n",
        "print(\"✅ Saved shared model predictions to 'shared_lstm_holdout_preds.npy'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgHMMJz09AGl"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"lstm_model_added_features.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG3jtW-h9DH_",
        "outputId": "af79f9b4-fc85-42a0-8f94-57a0c3375ffd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMForecast(\n",
              "  (lstm): LSTM(7, 128, batch_first=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Recreate the same architecture\n",
        "modell = LSTMForecast(input_dim=7, hidden_dim=128, output_dim=4, dropout=0.1)\n",
        "modell.load_state_dict(torch.load(\"lstm_model_added_features.pt\"))\n",
        "modell.to(DEVICE)\n",
        "modell.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-_dnpSXCT03",
        "outputId": "88f1e773-9248-4d4c-c21c-43ad652b494e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved predictions to 'bicikelj_test_predictions_128.csv'\n"
          ]
        }
      ],
      "source": [
        "# --- Predict only the unknown rows in bicikelj_test.csv using a single shared model ---\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import holidays\n",
        "\n",
        "# --- Constants ---\n",
        "HISTORY_LEN = 48\n",
        "PRED_HORIZON = 4\n",
        "K_NEIGHBORS = 2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Load data ---\n",
        "test_df = pd.read_csv(\"bicikelj_test.csv\")\n",
        "meta = pd.read_csv(\"bicikelj_metadata.csv\")\n",
        "station_cols = test_df.columns[1:]\n",
        "\n",
        "# --- Neighbors ---\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- Load model ---\n",
        "input_dim = 1 + K_NEIGHBORS + 4  # 1 station + 2 neighbors + 4 time features\n",
        "output_dim = PRED_HORIZON\n",
        "\n",
        "class LSTMForecast(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.dropout(h_n[-1])\n",
        "        return self.fc(out)\n",
        "\n",
        "model = LSTMForecast(input_dim, 128, output_dim, 0.1).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"lstm_model_added_features.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# --- Prepare features ---\n",
        "test_feats = test_df[station_cols].values.astype(np.float32)\n",
        "timestamps = pd.to_datetime(test_df[\"timestamp\"])\n",
        "\n",
        "hour_sin = np.sin(2 * np.pi * timestamps.dt.hour / 24)\n",
        "hour_cos = np.cos(2 * np.pi * timestamps.dt.hour / 24)\n",
        "is_weekend = (timestamps.dt.dayofweek >= 5).astype(float)\n",
        "slo_holidays = holidays.Slovenia()\n",
        "is_holiday = timestamps.dt.date.astype(str).isin([str(d) for d in slo_holidays]).astype(float)\n",
        "\n",
        "name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "time_feats = np.stack([hour_sin, hour_cos, is_weekend, is_holiday], axis=1)\n",
        "\n",
        "# --- Predict ---\n",
        "pred_matrix = np.full_like(test_feats, np.nan)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(HISTORY_LEN, len(test_df) - PRED_HORIZON + 1):\n",
        "        if np.isnan(test_feats[i:i + PRED_HORIZON]).all(axis=0).all():\n",
        "            for station in station_cols:\n",
        "                s_idx = name_to_idx[station]\n",
        "                nn_idx = [name_to_idx[nn] for nn in neighbors[station]]\n",
        "\n",
        "                seq = []\n",
        "                for t in range(i - HISTORY_LEN, i):\n",
        "                    row = [test_feats[t, s_idx]]\n",
        "                    row += [test_feats[t, j] for j in nn_idx]\n",
        "                    row += list(time_feats[t])\n",
        "                    seq.append(row)\n",
        "                seq = torch.tensor([seq], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "                pred = model(seq).cpu().numpy().flatten()\n",
        "                for j in range(PRED_HORIZON):\n",
        "                    pred_matrix[i + j, s_idx] = pred[j]\n",
        "\n",
        "# --- Save output ---\n",
        "pred_df = pd.DataFrame(pred_matrix, columns=station_cols)\n",
        "pred_df.insert(0, \"timestamp\", test_df[\"timestamp\"])\n",
        "\n",
        "rows_to_output = test_df[station_cols].isna().all(axis=1)\n",
        "pred_df_filtered = pred_df[rows_to_output].copy()\n",
        "\n",
        "pred_df_filtered.to_csv(\"bicikelj_test_predictions_128.csv\", index=False)\n",
        "print(\"✅ Saved predictions to 'bicikelj_test_predictions_128.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vttHXdKrZEO9",
        "outputId": "b26eb3ef-809f-4167-fa9a-eba083695dac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-7d9d46934415>:37: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[station_cols] = df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
            "Evaluating on holdout: 100%|██████████| 4019/4019 [00:07<00:00, 537.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Final Holdout MSE = 9.457019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "import holidays\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "HISTORY_LEN = 48\n",
        "PRED_HORIZON = 4\n",
        "K_NEIGHBORS = 2\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Model ---\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.dropout(h_n[-1])\n",
        "        return self.fc(out)\n",
        "\n",
        "# --- Load data ---\n",
        "df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = df.columns[1:]\n",
        "\n",
        "# Clean and fill NaNs\n",
        "for col in station_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df[station_cols] = df[station_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "df = df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- Neighbors ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- Dataset with time features ---\n",
        "class SharedLSTMDataset(Dataset):\n",
        "    def __init__(self, df, station_cols, neighbors, history_len, pred_horizon):\n",
        "        self.samples = []\n",
        "        self.station_indices = []\n",
        "\n",
        "        timestamps = pd.to_datetime(df['timestamp'])\n",
        "        hour_sin = np.sin(2 * np.pi * timestamps.dt.hour / 24)\n",
        "        hour_cos = np.cos(2 * np.pi * timestamps.dt.hour / 24)\n",
        "        is_weekend = (timestamps.dt.dayofweek >= 5).astype(float)\n",
        "        slo_holidays = holidays.Slovenia()\n",
        "        is_holiday = timestamps.dt.date.astype(str).isin([str(d) for d in slo_holidays]).astype(float)\n",
        "\n",
        "        bikes = df[station_cols].values.astype(np.float32)\n",
        "        name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "        N = len(df)\n",
        "\n",
        "        time_feats = np.stack([hour_sin, hour_cos, is_weekend, is_holiday], axis=1)\n",
        "\n",
        "        for s_name in station_cols:\n",
        "            s_idx = name_to_idx[s_name]\n",
        "            nn_idx = [name_to_idx[nn] for nn in neighbors[s_name]]\n",
        "\n",
        "            series = bikes[:, [s_idx] + nn_idx]\n",
        "            all_feats = np.concatenate([series, time_feats], axis=1)\n",
        "\n",
        "            for i in range(history_len, N - pred_horizon + 1):\n",
        "                seq = all_feats[i - history_len:i]\n",
        "                target = bikes[i:i + pred_horizon, s_idx]\n",
        "                self.samples.append((seq, target))\n",
        "                self.station_indices.append(s_name)\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# --- Prepare holdout set ---\n",
        "dataset = SharedLSTMDataset(df, station_cols, neighbors, HISTORY_LEN, PRED_HORIZON)\n",
        "N = len(dataset)\n",
        "val_size = int(0.15 * N)\n",
        "holdout_size = int(0.15 * N)\n",
        "train_size = N - val_size - holdout_size\n",
        "\n",
        "holdout_set = torch.utils.data.Subset(dataset, range(train_size + val_size, N))\n",
        "holdout_loader = DataLoader(holdout_set, batch_size=64)\n",
        "\n",
        "# --- Load model ---\n",
        "input_dim = 1 + K_NEIGHBORS + 4\n",
        "output_dim = PRED_HORIZON\n",
        "hidden_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "model = LSTMForecast(input_dim, hidden_dim, output_dim, dropout).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"lstm_model_added_features.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# --- Evaluate MSE ---\n",
        "mse_loss = nn.MSELoss()\n",
        "all_losses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in tqdm(holdout_loader, desc=\"Evaluating on holdout\"):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        preds = model(xb)\n",
        "        loss = mse_loss(preds, yb)\n",
        "        all_losses.append(loss.item())\n",
        "\n",
        "final_mse = np.mean(all_losses)\n",
        "print(f\"✅ Final Holdout MSE = {final_mse:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UvHcj2U42lR"
      },
      "source": [
        "## Grid search LSTM Shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H65ox8co42FI",
        "outputId": "511056a7-aa27-4119-880e-7ad1aea7d792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Running grid search over 2 combinations...\n",
            "\n",
            "🔍 Combo 1: hidden_dim=128, dropout=0.0, lr=0.001, weight_decay=1e-05\n",
            "✅ Val Loss: 10.7869, Holdout Loss: 10.7490\n",
            "\n",
            "🔍 Combo 2: hidden_dim=128, dropout=0.1, lr=0.001, weight_decay=1e-05\n",
            "✅ Val Loss: 10.6357, Holdout Loss: 10.9593\n",
            "\n",
            "📊 Top 5 Results:\n",
            "   hidden_dim  dropout     lr  weight_decay   val_loss  holdout_loss\n",
            "0         128      0.0  0.001       0.00001  10.786934     10.748992\n",
            "1         128      0.1  0.001       0.00001  10.635716     10.959280\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from tqdm import tqdm\n",
        "import holidays\n",
        "import random\n",
        "\n",
        "# --- Static Params ---\n",
        "HISTORY_LEN = 48\n",
        "PRED_HORIZON = 4\n",
        "K_NEIGHBORS = 2\n",
        "EPOCHS = 20\n",
        "PATIENCE = 5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_COMBINATIONS = 20\n",
        "TRAIN_FRACTION = 0.01\n",
        "\n",
        "# --- Load data ---\n",
        "df = pd.read_csv('bicikelj_train.csv')\n",
        "meta = pd.read_csv('bicikelj_metadata.csv')\n",
        "station_cols = df.columns[1:]\n",
        "\n",
        "# Clean and fill\n",
        "for col in station_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df[station_cols] = df[station_cols].ffill().bfill()\n",
        "df = df.dropna(subset=station_cols, how='all').reset_index(drop=True)\n",
        "\n",
        "# --- Neighbors ---\n",
        "coords = np.deg2rad(meta[['latitude', 'longitude']].values)\n",
        "station_names = meta['name'].tolist()\n",
        "dists = haversine_distances(coords, coords) * 6371\n",
        "neighbors = {}\n",
        "for i, name in enumerate(station_names):\n",
        "    order = np.argsort(dists[i])\n",
        "    nn_idx = [j for j in order if j != i][:K_NEIGHBORS]\n",
        "    neighbors[name] = [station_names[j] for j in nn_idx]\n",
        "\n",
        "# --- Dataset ---\n",
        "class SharedLSTMDataset(Dataset):\n",
        "    def __init__(self, df, station_cols, neighbors, history_len, pred_horizon):\n",
        "        self.samples = []\n",
        "        timestamps = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "        hour_sin = np.sin(2 * np.pi * timestamps.dt.hour / 24)\n",
        "        hour_cos = np.cos(2 * np.pi * timestamps.dt.hour / 24)\n",
        "        is_weekend = (timestamps.dt.dayofweek >= 5).astype(float)\n",
        "        slo_holidays = holidays.Slovenia()\n",
        "        is_holiday = timestamps.dt.date.astype(str).isin([str(d) for d in slo_holidays]).astype(float)\n",
        "        time_feats = np.stack([hour_sin, hour_cos, is_weekend, is_holiday], axis=1)\n",
        "\n",
        "        bikes = df[station_cols].values.astype(np.float32)\n",
        "        name_to_idx = {name: i for i, name in enumerate(station_cols)}\n",
        "        N = len(df)\n",
        "\n",
        "        for s_name in station_cols:\n",
        "            s_idx = name_to_idx[s_name]\n",
        "            nn_idx = [name_to_idx[nn] for nn in neighbors[s_name]]\n",
        "            series = bikes[:, [s_idx] + nn_idx]\n",
        "            full_feats = np.concatenate([series, time_feats], axis=1)\n",
        "\n",
        "            for i in range(history_len, N - pred_horizon + 1):\n",
        "                seq = full_feats[i - history_len:i]\n",
        "                target = bikes[i:i + pred_horizon, s_idx]\n",
        "                self.samples.append((seq, target))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# --- Model ---\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        return self.fc(self.dropout(h_n[-1]))\n",
        "\n",
        "# --- Train function ---\n",
        "def train_lstm(model, train_loader, val_loader, lr, weight_decay):\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_loss = float('inf')\n",
        "    best_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                val_loss += criterion(model(xb), yb).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model, best_loss\n",
        "\n",
        "# --- Grid search space ---\n",
        "param_grid = {\n",
        "    # 'hidden_dim': [16, 32, 64],\n",
        "    # 'dropout': [0.1, 0.3, 0.5],\n",
        "    # 'lr': [1e-3, 5e-4],\n",
        "    # 'weight_decay': [1e-5, 1e-4]\n",
        "    'hidden_dim': [128],\n",
        "    'dropout': [0.0, 0.1],\n",
        "    'lr': [1e-3],\n",
        "    'weight_decay': [1e-5]\n",
        "}\n",
        "param_combos = list(itertools.product(*param_grid.values()))\n",
        "random.shuffle(param_combos)\n",
        "param_combos = param_combos[:MAX_COMBINATIONS]\n",
        "\n",
        "# --- Prepare dataset ---\n",
        "dataset = SharedLSTMDataset(df, station_cols, neighbors, HISTORY_LEN, PRED_HORIZON)\n",
        "N = len(dataset)\n",
        "reduced_N = int(N * TRAIN_FRACTION)\n",
        "indices = list(range(N))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_size = int(reduced_N * 0.7)\n",
        "val_size = int(reduced_N * 0.15)\n",
        "holdout_size = reduced_N - train_size - val_size\n",
        "\n",
        "train_set = Subset(dataset, indices[:train_size])\n",
        "val_set = Subset(dataset, indices[train_size:train_size + val_size])\n",
        "holdout_set = Subset(dataset, indices[train_size + val_size:train_size + val_size + holdout_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=64)\n",
        "holdout_loader = DataLoader(holdout_set, batch_size=64)\n",
        "\n",
        "# --- Run grid search ---\n",
        "input_dim = 1 + K_NEIGHBORS + 4\n",
        "output_dim = PRED_HORIZON\n",
        "\n",
        "results = []\n",
        "print(f\"⏳ Running grid search over {len(param_combos)} combinations...\")\n",
        "for i, (hdim, dr, lr, wd) in enumerate(param_combos):\n",
        "    print(f\"\\n🔍 Combo {i+1}: hidden_dim={hdim}, dropout={dr}, lr={lr}, weight_decay={wd}\")\n",
        "    model = LSTMForecast(input_dim, hdim, output_dim, dr)\n",
        "    model, val_loss = train_lstm(model, train_loader, val_loader, lr, wd)\n",
        "\n",
        "    # Evaluate on holdout\n",
        "    model.eval()\n",
        "    holdout_loss = 0.0\n",
        "    criterion = nn.MSELoss()\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in holdout_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            holdout_loss += criterion(model(xb), yb).item()\n",
        "    holdout_loss /= len(holdout_loader)\n",
        "\n",
        "    print(f\"✅ Val Loss: {val_loss:.4f}, Holdout Loss: {holdout_loss:.4f}\")\n",
        "    results.append({\n",
        "        \"hidden_dim\": hdim,\n",
        "        \"dropout\": dr,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": wd,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"holdout_loss\": holdout_loss\n",
        "    })\n",
        "\n",
        "# --- Save results ---\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"holdout_loss\")\n",
        "results_df.to_csv(\"grid_search_results.csv\", index=False)\n",
        "print(\"\\n📊 Top 5 Results:\")\n",
        "print(results_df.head())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
